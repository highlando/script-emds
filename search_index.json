[["index.html", "Einführung in die mathematische Datenanalyse Vorwort", " Einführung in die mathematische Datenanalyse Jan Heiland FAU Erlangen-Nürnberg – Sommersemester 2022 Vorwort Das ist ein Aufschrieb. Korrekturen und Wünsche immer gerne als issues oder pull requests ans github-repo. "],["was-ist-data-science.html", "1 Was ist Data Science? 1.1 Wie passiert die Datenanalyse? 1.2 Was sind Daten? 1.3 Beispiele 1.4 Python 1.5 Aufgaben", " 1 Was ist Data Science? Data Science umfasst unter anderem folgende Aufgaben: Strukturieren/Aufbereiten (Umgehen mit falschen, korrumpierten, fehlenden, unformatierten Daten) Data Exploration (Daten “verstehen”) Data Analysis (quantitative Analysen, Hypothesen aufstellen) Data Visualization (Hypothesen graphisch kommunizieren) Modelle erzeugen/validieren (Regeln/Muster erkennen, Vorhersagen treffen) – das ist Machine Learning aber es gibt auch viele andere Ansätze. Daten Reduktion 1.1 Wie passiert die Datenanalyse? Mit mathematischen Methoden aus den Bereichen der linearen Algebra (z.B. Matrizen, Basen, lineare Gleichungssysteme) Statistik (z.B. Mittelwerte, Korrelationen, Verteilungen) Analysis (Grenzwerte, Abschätzungen) … Dabei hilft Software, z.B., Excel Python Matlab R bei der Berechnung, Automatisierung, Visualisierung. Python ist ein de-facto Standard in Data Science und Machine Learning. 1.2 Was sind Daten? Wie sehen Daten aus? Numerisch reell, z.B. Temperatur Numerisch diskret, z.B. Anzahl Ordinal: Element einer festen Menge mit expliziter Ordnung, z.B. {neuwertig, mit Gebrauchsspuren, defekt} Binär: Eine von zwei Möglichkeiten, z.B. Wahr/Falsch oder aktiv/inaktiv Kategoriell: Element einer festen Menge ohne klare Ordnung, z.B. {Säugetier, Vogel, Fisch} sonstige strukturierte Daten, z.B. geographische Daten, Graphen reiner Text, z.B. Freitext in Restaurantbewertung Außerdem können wir noch allgemeine Eigenschaften (Qualitätsmerkmale) von Daten unterscheiden strukturiert lückenhaft fehlerbehaftet (verrauscht) interpretierbar geordnet (oder nicht zu ordnen) 1.3 Beispiele 1.3.1 Tabellendaten – Mietpreise Abbildung: Tabelle von Wohnungsangeboten Hier wären die Aufgaben von Data Science: Daten “verstehen”, Zusammenhänge zwischen Variablen aufdecken, visualisieren. Gegebenenfalls fehlende Einträge bei (z.B.) kaltmiete vorhersagen Abbildung: Eine Spalte der Tabelle Datenexploration und -analyse für einzelne Variablen 1/3 Wir betrachten eine numerische Variable in einem rechteckigen Datensatz, also eine Spalte (z.B. kaltmiete). Wir bezeichnen den \\(i\\)-ten Eintrag in dieser Spalte mit \\(x_i\\), wobei \\(i=1,\\ldots,N\\) (\\(N\\) Anzahl der Zeilen). Folgende Schätzer/Metriken können dabei helfen, diese Spalte besser zu verstehen: Mittelwert \\(\\overline x= \\frac1 N\\sum_{i=1}^N x_i\\) gewichteter Mittelwert \\(\\overline x_w = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{j=1}^N w_j}\\), wobei \\(w_i\\) das Gewicht des \\(i\\)-ten Eintrages ist (z.B. eine andere Variable). Varianz: \\(s_x^2 = \\frac{1}{N-1} \\sum_{i=1}^N (x_i-\\overline x)^2\\) Standardabweichung \\(s = \\sqrt{s_x^2}\\). Median = \\(\\frac{315 + 400}{2} = 357.5\\). Datenexploration und -analyse für mehrere Variablen Wir betrachten zwei Spalten \\(x = (x_1,\\ldots,x_N)\\) und \\(y = (y_1,\\ldots, y_N)\\). Das Verteilung von zwei Variablen läßt sich im sogenannte Scatter Plot visualisieren. Datenexploration und -analyse für mehrere Variablen Wir betrachten zwei Spalten \\(x = (x_1,\\ldots,x_N)\\) und \\(y = (y_1,\\ldots, y_N)\\). Kovarianz \\(s_{xy} = \\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\overline x)(y_i - \\overline y)\\) Korrelation \\(\\rho_{xy} = \\frac{s_{xy}}{s_x\\cdot s_y} \\in [-1,1]\\). \\(\\rho \\approx 1\\): Starke positive Korrelation, wenn \\(x\\) groß ist, ist \\(y\\) auch groß. \\(\\rho \\approx -1\\): Starke negative Korrelation, wenn \\(x\\) groß ist, ist \\(y\\) klein \\(\\rho \\approx 0\\): Wenig/keine Korrelation. Von Kiatdd - Eigenes Werk, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=37108966 1.3.2 COVID-19 Daten Vergleiche die Einführung in Mathematik für Data Science 1 vom letzten Semester. 1.3.3 Netflix Prize Hierbei geht es darum, ob aus bekannten Bewertungen von vielen verschiedenen Benutzern für viele verschiedene Filme abgeleitet werden kann, ob ein bestimmter Nutzer einen bestimmten Film mag (also positiv bewerten würde). Vergleiche auch Wikipedia:Netflix_Prize Das (Trainings-)Daten bestehen über 480189 Benutzer, die für 17770 Filme insgesamt 100480507 Bewertungen als ganze Zahlen zwischen 1 und 5 verteilten. Ziel der Datenanalyse war es, für 2817131 “Paare” von Benutzern und Filmen, die Bewertung vorauszusagen. Neben der schieren Masse an Daten kamen noch Einschränkungen hinzu, die ein Mindestmaß an Qualität der Vorhersage sicherstellen sollten. Das Problem ließe sich wie folgt darstellen. Benutzer \\ Film F1 F2 ... Fn ... B1 – 3 ... 5 ... B2 3 4 ... 2 ... B3 1 2 ... ? ... ... 3 4 ... – ... Gegeben viele (aber bei weitem nicht alle) Einträge in einer riesigen Tabelle. Können wir aus den Zusammenhängen bestimmte fehlende Einträge (z.B. wie findet Nutzer B3 den Film Fn) herleiten? Die besten Lösungen für dieses Problem basieren durchweg auf Machine Learning Ansätzen. 1.4 Python Die Programmiersprache python wird uns durchs Semester begleiten. Einfach weil sie so wichtig ist für Data Science aber auch weil sie (meiner Meinung nach) einfach zu erlernen und zu benutzen ist. 1.5 Aufgaben 1.5.1 Python Bringen sie ihr python zum Laufen, installieren sie numpy, scipy und matplotlib und führen sie das folgende script aus. import numpy as np import matplotlib.pyplot as plt N = 20 xmax = 2 xmin = 0 xdata = np.linspace(xmin, xmax, N) ydata = np.exp(xdata) plt.figure(1) plt.plot(xdata, ydata, &#39;.&#39;) plt.figure(2) plt.semilogy(xdata, ydata, &#39;.&#39;) plt.show() 1.5.2 Einheitsmatrix Schreiben sie ein script, dass die 5x5 Einheitsmatrix auf 3 verschiedene Arten erzeugt. (Eine Art könnte die eingebaute numpy Funktion eye sein). import numpy as np idfive = np.eye(5) print(idfive) Hinweis: schauen sie sich mal an wie numpy’s arrays funktionieren. 1.5.3 Matrizen Multiplikation und Potenz Schreiben sie ein script, das die Übungsaufgabe aus der Vorlesung (potenzieren der Matrizen \\(M_i\\), \\(i=1,2,3,4\\)) löst. Zum Beispiel mit import numpy as np mone = np.array([[0.9, 0.9], [0.9, 0.9]]) mone_ptwo = mone @ mone print(mone_ptwo) mone_pfour = mone_ptwo @ mone_ptwo print(mone_pfour) Oder so: import numpy as np mone = np.array([[0.9, 0.9], [0.9, 0.9]]) mone_p = np.eye(2) for k in range(16): mone_p = mone_p @ mone if k == 1 or k == 3 or k == 15: print(&#39;k=&#39;, k+1) print(mone_p) Achtung: bei Matrizen kann auch * benutzt werden – das ist aber nicht die richtige Matrizenmultiplikation (sondern die Multiplikation eintragsweise) Moegliche Realisierung der Matrizenmultiplikation np.dot(A, B) – die klassische Methode A.dot(B) – das selbe (manchmal besser, wenn A etwas allgemeiner ist (zum Beispiel eine scipy.sparse matrix) A @ B – convenience Notation "],["lineare-regression.html", "2 Lineare Regression 2.1 Rauschen und Fitting 2.2 Ansätze für lineare Regression 2.3 Fehlerfunktional und Minimierung 2.4 Berechnung der Bestlösung 2.5 Beispiel", " 2 Lineare Regression Auch bekannt als lineare Ausgleichsrechnung oder Methode der kleinsten Quadrate. Ein wesentlicher Aspekt von Data Science ist die Analyse oder das Verstehen von Daten. Allgemein gesagt, es wird versucht, aus den Daten heraus Aussagen über Trends oder Eigenschaften des Phänomens zu treffen, mit welchem die Daten im Zusammenhang stehen. Wir kommen nochmal auf das Beispiel aus der Einführungswoche zurück, werfen eine bereits geschärften Blick darauf und gehen das mit verbesserten mathematischen Methoden an. Gegeben seien die Fallzahlen aus der CoVID Pandemie 2020 für Bayern für den Oktober 2020. Anzahl der SARS-CoV-2 Neuinfektionen in Bayern im Oktober 2020. Tag 1 2 3 4 5 6 7 8 9 10 11 Fälle 352 347 308 151 360 498 664 686 740 418 320 Tag 12 13 14 15 16 17 18 19 20 21 Fälle 681 691 1154 1284 127 984 573 1078 1462 2239 Tag 22 23 24 25 26 27 28 29 30 31 Fälle 2236 2119 1663 1413 2283 2717 3113 2972 3136 2615 Fallzahlen von Sars-CoV-2 in Bayern im Oktober 2020 Wieder stellen wir uns die Frage ob wir in den Daten einen funktionalen Zusammenhang feststellen können. Also ob wir die Datenpaare (Tag \\(x\\), Infektionen am Tag \\(x\\)) die wir als (\\(x_i\\), \\(y_i\\)) über eine Funktion \\(f\\) und die Paare (\\(x\\), \\(f(x)\\)) beschreiben (im Sinne von gut darstellen oder approximieren) können. 2.1 Rauschen und Fitting Beim obigen Beispiel (und ganz generell bei Daten) ist davon auszugehen, dass die Daten verrauscht sind, also einem Trend folgen oder in einem funktionalen Zusammenhang stehen aber zufällige Abweichungen oder Fehler enthalten. Unter diesem Gesichtspunkt ist eine Funktion, die \\(f(x_i)=y_i\\) erzwingt nicht zielführend. (Wir wollen Trends und größere Zusammenhänge erkennen und nicht kleine Fehler nachzeichnen.) Das zu strenge Anpassen an möglicherweise verrauschte Daten wird overfitting genannt. Vielmehr werden wir nach einer Funktion \\(f\\) suchen, die die Daten näherungsweise nachstellt: \\(f(x_i)\\approx y_i\\) Hierbei passen jetzt allerdings auch Funktionen, die vielleicht einfach zu handhaben sind aber die Daten kaum noch repräsentieren. Jan spricht von underfitting. Eine gute Approximation besteht im Kompromiss von nah an den Daten aber mit wenig overfitting. 2.2 Ansätze für lineare Regression Um eine solche Funktion \\(f\\) zu finden, trifft Jan als erstes ein paar Modellannahmen. Modellannahmen legen fest, wie das \\(f\\) im Allgemeinen aussehen soll und versuchen dabei die Bestimmung von \\(f\\) zu ermöglichen zu garantieren, dass \\(f\\) auch die gewollten Aussagen liefert und sicherzustellen, dass \\(f\\) zum Problem passt. Jan bemerke, dass die ersten beiden Annahmen im Spannungsverhältnis zur dritten stehen. Lineare Regression besteht darin, dass die Funktion als Linearkombination \\[\\begin{equation*} f_w(x) = \\sum_{j=1}^n w_j b_j(x) \\end{equation*}\\] von Basisfunktionen geschrieben wird und dann die Koeffizienten \\(w_i\\) so bestimmt werden, dass \\(f\\) die Daten bestmöglich annähert. Jan bemerke, dass bestmöglich wieder overfitting bedeuten kann aber auch, bei schlechter Wahl der Basis, wenig aussagekräftig sein kann. Der gute Kompromiss liegt also jetzt in der Wahl der passenden Basisfunktionen und deren Anzahl. (Mehr Basisfunktionen bedeutet möglicherweise bessere Approximation aber auch die Gefahr von overfitting.) Typische Wahlen für die Basis \\(\\{b_1, b_2, \\dotsc, b_n\\}\\) sind Polynome: \\(\\{1, x, x^2, \\dotsc, x^{n-1}\\}\\) – für \\(n=2\\) ist der Ansatz eine Gerade Trigonometrische Funktionen: \\(\\{1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x), \\dotsc\\}\\) Splines – Polynome, die abschnittsweise definiert werden Wavelets – Verallgemeinerungen von trigonometrischen Funktionen 2.3 Fehlerfunktional und Minimierung Wir setzen nun also an \\[\\begin{equation*} f_w(x) = \\sum_{j=1}^nw_j b_j (x) \\end{equation*}\\] und wollen damit \\(y_i \\approx f_w(x_i)\\) bestmöglich erreichen (indem wir die Koeffizienten \\((w_1, \\dotsc, w_n)\\) optimal wählen. Bestmöglich und optimal spezifizieren wir über den Mittelwert der quadratischen Abweichungen in der Approximation über alle Datenpunkte \\[\\begin{equation*} \\frac{1}{N}\\sum_{i=1}^N (y_i - f_w(x_i))^2 \\end{equation*}\\] Ein paar Bemerkungen jetzt müssen wir die \\(w_i\\)’s bestimmen so dass dieser Fehlerterm minimal wird das optimale \\(w\\) ist unabhängig von einer Skalierung des Fehlerterms des wegen schreiben wir gerne einfach \\(\\frac 12 \\sum_{i=1}^N (y_i - f_w(x_i))^2\\) als das Zielfunktional, das es zu minimieren gilt. Wie finden wir jetzt die \\(w_i\\)’s? Zunächst gilt, dass \\[\\begin{equation*} f_w(x_i) = \\sum_{i=j}^n w_j b_j(x_i) = \\begin{bmatrix} b_1(x_i) &amp; b_2(x_i) &amp; \\dots &amp; b_n(x_i) \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\end{equation*}\\] und wenn wir alle \\(f_w(x_i)\\), \\(i=1,\\dotsc,N\\) übereinander in einen Vektor schreiben, dass \\[\\begin{equation*} f_w(\\mathbf x) := \\begin{bmatrix} f_w(x_1) \\\\ \\vdots \\\\ f_w(x_N) \\end{bmatrix} = \\begin{bmatrix} b_1(x_1) &amp; \\dots &amp; b_n(x_1) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_1(x_N) &amp; \\dots &amp; b_n(x_N) \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} =: \\Phi(\\mathbf x) w \\end{equation*}\\] Damit, mit \\(\\mathbf y\\) als den Vektor aller \\(y_i\\)’s, und mit der Definition der Vektornorm, können wir unser Minimierungsproblem schreiben als \\[\\begin{equation*} \\frac 12 \\sum_{i=1}^N (y_i - f_w(x_i))^2 = \\frac 12 \\| \\mathbf y - \\Phi (\\mathbf x) w\\|^2 \\to \\min. \\end{equation*}\\] Wir bemerken, dass das Fehlerfunktional immer größer und bestenfalls gleich 0 ist falls das lineare Gleichungssystem \\(\\Phi (\\mathbf x)w = \\mathbf y\\) eine Lösung \\(w\\) hat, ist das auch eine Lösung unserer Minimierung im typischen Falle aber ist allerdings \\(N\\gg n\\) und das System überbestimmt (\\(n=N\\) würde ein overfitting bedeuten…) sodass wir keine Lösung des linearen Gleichungssystems erwarten können. Das Minimierungsproblems selbst hat allerdings immer eine Lösung. 2.4 Berechnung der Bestlösung Wir suchen also ein Minimum der Funktion (mit \\(\\Phi\\), \\(\\mathbf x\\), \\(\\mathbf y\\) gegeben) \\[\\begin{equation*} \\begin{split} w \\mapsto \\frac 12 \\|\\mathbf y - \\Phi(\\mathbf x)w \\|^2 &amp;= \\frac 12 (\\mathbf y - \\Phi(\\mathbf x)w)^T(\\mathbf y - \\Phi(\\mathbf x)w) \\\\ &amp;= \\frac 12 [\\mathbf y^T\\mathbf y - \\mathbf y^T \\Phi(\\mathbf x)w - w^T \\Phi(\\mathbf x)^T\\mathbf y + w^T \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w] \\\\ &amp;= \\frac 12 [\\mathbf y^T\\mathbf y -2 w^T \\Phi(\\mathbf x)^T\\mathbf y + w^T \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w] \\end{split} \\end{equation*}\\] wobei wir die Definition der Norm \\(\\|v\\|^2 = v^Tv\\) und die Eigenschaft, dass für die skalare Größe \\(w^T \\Phi(\\mathbf x)^T\\mathbf y = [w^T \\Phi(\\mathbf x)^T\\mathbf y]^T = \\mathbf y^T \\Phi(\\mathbf x)w\\) gilt, ausgenutzt haben. Wären \\(w\\) und \\(\\mathbf y\\) keine Vektoren sondern einfach reelle Zahlen, wäre das hier eine Parabelgleichung \\(aw^2 + bw + c\\) mit \\(a&gt;0\\), die immer eine Minimalstelle hat. Tatsächlich gilt hier alles ganz analog. Insbesondere ist \\(\\Phi(\\mathbf x)^T\\Phi(\\mathbf x)\\) in der Regel “größer 0” (was heißt das wohl bei quadratischen Matrizen?). Und mittels “Nullsetzen” der ersten Ableitung können wir das Minimum bestimmen. In diesem Fall ist die erste Ableitung (nach \\(w\\)) \\[\\begin{equation*} \\nabla_w (\\frac 12 \\|\\mathbf y - \\Phi(\\mathbf x) \\|^2) = \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w - \\Phi(\\mathbf x)^T\\mathbf y, \\end{equation*}\\] (den Gradienten \\(\\nabla_w\\) als Ableitung von Funktionen mit mehreren Veränderlichen werden wir noch genauer behandeln) was uns als Lösung, die Lösung des linearen Gleichungssystems \\[\\begin{equation*} \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w = \\Phi(\\mathbf x)^T\\mathbf y \\end{equation*}\\] definiert. Letzte Frage: Wann hat dieses Gleichungssystems eine eindeutige Lösung? Mit \\(N&gt;n\\) (also \\(\\Phi(\\mathbf x)\\) hat mehr Zeilen als Spalten) gelten die Äquivalenzen: \\(\\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w = \\Phi(\\mathbf x)^T\\mathbf y\\) hat eine eindeutige Lösung die Matrix \\(\\Phi(\\mathbf x)^T\\Phi(\\mathbf x)\\) ist regulär die Spalten von \\(\\Phi(\\mathbf x)\\) sind linear unabhängig die Vektoren \\(b_i(\\mathbf x)\\) sind linear unabhängig. Praktischerweise tritt genau diese Situation im Allgemeinen ein. \\(N&gt;n\\) (mehr Datenpunkte als Parameter) \\(b_i\\)’s werden als linear unabhängig (im Sinne ihres Funktionenraums) gewählt, was die lineare unabhängigket der \\(b_i(\\mathbf x)\\) impliziert. 2.5 Beispiel Unsere Covid-Zahlen “mit einer Geraden angenähert”: \\(f_w(x) = w_1 + w_2 x\\) – das heißt \\(n=2\\) und Basisfunktionen \\(b_1(x)\\equiv 1\\) und \\(b_2(x) = x\\) \\(\\mathbf x = (1,2,3, \\dots, 31)\\) – die Tage im Februar, das heißt \\(N=31\\) \\(\\mathbf y = (352, 347, \\dots, 2615)\\) – die Fallzahlen Wir bekommen \\[\\begin{equation*} \\Phi(\\mathbf x) = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; 31 \\end{bmatrix} \\end{equation*}\\] (die Spalten sind linear unabhängig) und müssen “nur” das 2x2 System \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ 1 &amp; 2 &amp; 3 &amp; \\dots &amp; 31 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ \\vdots \\\\ 1 &amp; 31 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ 1 &amp; 2 &amp; 3 &amp; \\dots &amp; 31 \\end{bmatrix} \\begin{bmatrix} 352 \\\\ 347 \\\\ 308 \\\\ \\vdots \\\\ 2615 \\end{bmatrix} \\] lösen um die Approximation \\(f_w\\) zu bestimmen. Und noch als letzte Bemerkung. Egal wie die Basisfunktionen \\(b_i\\) gewählt werden, die Parameterabhängigkeit von \\(w\\) ist immer linear. Deswegen der Name lineare Ausgleichsrechnung. "],["matrix-zerlegungen.html", "3 Matrix-Zerlegungen 3.1 QR Zerlegung 3.2 Aufgaben", " 3 Matrix-Zerlegungen Die Lösung \\(w\\) des Problems der linearen Ausgleichsrechnung war entweder als Lösung eines Optimierungsproblems \\[\\begin{equation*} \\min_{w} \\| Aw - y \\|^2 \\end{equation*}\\] oder als Lösung des linearen Gleichungssystems \\[\\begin{equation*} A^TAw=y \\end{equation*}\\] gegeben. Hierbei steht nun \\(A\\in \\mathbb R^{N\\times n}\\) für die Matrix \\(\\Phi(\\mathbf x)\\) der Daten und Basisfunktionen. Wir hatten uns überlegt, dass in den meisten Fällen die Matrix mehr Zeilen als Spalten hat (\\(N&gt;n\\)) und die Spalten linear unabhängig sind. 3.1 QR Zerlegung Wir betrachten nochmal das Optimierungsproblem \\(\\min_{w} \\| Aw - y \\|^2\\). Gäbe es eine Lösung des Systems \\(Aw=y\\), wäre das sofort eine Lösung des Optimierungsproblems. Da \\(A\\) aber mehr Zeilen als Spalten hat, ist das System \\(Aw=y\\) überbestimmt und eine Lösung in der Regel nicht gegeben. Die Überlegung ist nun, die Gleichung \\(Aw=y\\) so gut wie möglich zu erfüllen, indem wir die relevanten Gleichungen indentifizieren und wenigstens diese lösen. Ein systematischer (und wie wir später sehen werden auch zum Optimierungsproblem passender) Zugang bietet die QR Zerlegung. Theorem 3.1 (QR Zerlegung) Sei \\(A\\mathbb R^{m\\times n}\\), \\(m&gt;n\\). Dann existiert eine orthonormale Matrix \\(Q\\in \\mathbb R^{m\\times m}\\) und eine obere Dreiecksmatrix \\(\\hat R\\in \\mathbb R^{n\\times n}\\) derart dass \\[\\begin{equation*} A = QR =: Q \\begin{bmatrix} \\hat R \\\\ 0 \\end{bmatrix}. \\end{equation*}\\] Hat \\(A\\) vollen (Spalten)Rang, dann ist \\(\\hat R\\) invertierbar. Hier heißt orthonormale Matrix \\(Q\\), dass die Spalten von \\(Q\\) paarweise orthogonal sind. Insbesondere gilt \\[\\begin{equation*} Q^T Q = I. \\end{equation*}\\] Für unser zu lösendes Problem ergibt sich dadurch die Umformung \\[\\begin{equation*} Aw = y \\quad \\Leftrightarrow \\quad QRw=y \\quad \\Leftrightarrow \\quad Q^TQRw=Q^T y \\quad \\Leftrightarrow \\quad Rw = Q^Ty \\end{equation*}\\] oder auch \\[\\begin{equation*} \\begin{bmatrix} \\hat R \\\\ 0 \\end{bmatrix}w = Q^Ty = \\begin{bmatrix} Q_1^T \\\\ Q_2^T \\end{bmatrix} y \\end{equation*}\\] (wobei wir die \\(Q_1\\in \\mathbb R^{m\\times n}\\) die Matrix der ersten \\(n\\) Spalten von \\(Q\\) ist) und als Kompromiss der Vorschlag, das Teilsystem \\[\\begin{equation*} \\hat R w = Q_1^Ty \\end{equation*}\\] nach \\(w\\) zu lösen und in Kauf zu nehmen, dass der Rest, nämlich das \\(Q_2^Ty\\), nicht notwendigerweise gleich null ist. Wir halten zunächst mal fest, dass Obwohl \\(Q\\) eine reguläre Matrix ist, bedarf der Übergang von \\(Aw=y\\) zu \\(Q^TAw=Q^Ty\\) einer genaueren Analyse. Wir bemerken, dass für eine hypothetische komplette Lösung \\(Aw-y\\), diese Transformation keine Rolle spielt. Für die Kompromisslösung jedoch schon, weil beispielsweise verschiedene Konstruktionen eines invertierbaren Teils, verschiedene Residuen bedeuten und somit Optimalität im Sinne von \\(\\min_w \\|Aw-y\\|^2\\) nicht garantiert ist. Allerdings, wie Sie als Übungsaufgabe nachweisen werden, löst dieser Ansatz tatsächlich das Optimierungsproblem. 3.2 Aufgaben 3.2.1 (T) Norm und Orthogonale Transformation Sei \\(Q\\in \\mathbb R^{n\\times n}\\) eine orthogonale Matrix und sei \\(y\\in \\mathbb R^{n}\\). Zeigen Sie, dass \\[\\begin{equation*} \\|y\\|^2 = \\|Qy \\|^2 \\end{equation*}\\] gilt. 3.2.2 (T) QR Zerlegung und Kleinstes Quadrate Problem Sei \\(A\\in \\mathbb R^{m,n}\\), \\(m&gt;n\\), \\(A\\) hat vollen Rank und sei \\[\\begin{equation*} \\begin{bmatrix} Q_1 &amp; Q_2 \\end{bmatrix} \\begin{bmatrix} \\hat R \\\\ 0 \\end{bmatrix} = A \\end{equation*}\\] eine QR-Zerlegung von \\(A\\). Zeigen sie, dass die Lösung "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
