[["index.html", "Einführung in die mathematische Datenanalyse Vorwort", " Einführung in die mathematische Datenanalyse Jan Heiland FAU Erlangen-Nürnberg – Sommersemester 2022 Vorwort Das ist ein Aufschrieb. Korrekturen und Wünsche immer gerne als issues oder pull requests ans github-repo. "],["was-ist-data-science.html", "1 Was ist Data Science? 1.1 Wie passiert die Datenanalyse? 1.2 Was sind Daten? 1.3 Beispiele 1.4 Python 1.5 Aufgaben", " 1 Was ist Data Science? Data Science umfasst unter anderem folgende Aufgaben: Strukturieren/Aufbereiten (Umgehen mit falschen, korrumpierten, fehlenden, unformatierten Daten) Data Exploration (Daten “verstehen”) Data Analysis (quantitative Analysen, Hypothesen aufstellen) Data Visualization (Hypothesen graphisch kommunizieren) Modelle erzeugen/validieren (Regeln/Muster erkennen, Vorhersagen treffen) – das ist Machine Learning aber es gibt auch viele andere Ansätze. Daten Reduktion 1.1 Wie passiert die Datenanalyse? Mit mathematischen Methoden aus den Bereichen der linearen Algebra (z.B. Matrizen, Basen, lineare Gleichungssysteme) Statistik (z.B. Mittelwerte, Korrelationen, Verteilungen) Analysis (Grenzwerte, Abschätzungen) … Dabei hilft Software, z.B., Excel Python Matlab R bei der Berechnung, Automatisierung, Visualisierung. Python ist ein de-facto Standard in Data Science und Machine Learning. 1.2 Was sind Daten? Wie sehen Daten aus? Numerisch reell, z.B. Temperatur Numerisch diskret, z.B. Anzahl Ordinal: Element einer festen Menge mit expliziter Ordnung, z.B. {neuwertig, mit Gebrauchsspuren, defekt} Binär: Eine von zwei Möglichkeiten, z.B. Wahr/Falsch oder aktiv/inaktiv Kategoriell: Element einer festen Menge ohne klare Ordnung, z.B. {Säugetier, Vogel, Fisch} sonstige strukturierte Daten, z.B. geographische Daten, Graphen reiner Text, z.B. Freitext in Restaurantbewertung Außerdem können wir noch allgemeine Eigenschaften (Qualitätsmerkmale) von Daten unterscheiden strukturiert lückenhaft fehlerbehaftet (verrauscht) interpretierbar geordnet (oder nicht zu ordnen) 1.3 Beispiele 1.3.1 Tabellendaten – Mietpreise Abbildung: Tabelle von Wohnungsangeboten Hier wären die Aufgaben von Data Science: Daten “verstehen”, Zusammenhänge zwischen Variablen aufdecken, visualisieren. Gegebenenfalls fehlende Einträge bei (z.B.) kaltmiete vorhersagen Abbildung: Eine Spalte der Tabelle Datenexploration und -analyse für einzelne Variablen 1/3 Wir betrachten eine numerische Variable in einem rechteckigen Datensatz, also eine Spalte (z.B. kaltmiete). Wir bezeichnen den \\(i\\)-ten Eintrag in dieser Spalte mit \\(x_i\\), wobei \\(i=1,\\ldots,N\\) (\\(N\\) Anzahl der Zeilen). Folgende Schätzer/Metriken können dabei helfen, diese Spalte besser zu verstehen: Mittelwert \\(\\overline x= \\frac1 N\\sum_{i=1}^N x_i\\) gewichteter Mittelwert \\(\\overline x_w = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{j=1}^N w_j}\\), wobei \\(w_i\\) das Gewicht des \\(i\\)-ten Eintrages ist (z.B. eine andere Variable). Varianz: \\(s_x^2 = \\frac{1}{N-1} \\sum_{i=1}^N (x_i-\\overline x)^2\\) Standardabweichung \\(s = \\sqrt{s_x^2}\\). Median = \\(\\frac{315 + 400}{2} = 357.5\\). Datenexploration und -analyse für mehrere Variablen Wir betrachten zwei Spalten \\(x = (x_1,\\ldots,x_N)\\) und \\(y = (y_1,\\ldots, y_N)\\). Das Verteilung von zwei Variablen läßt sich im sogenannte Scatter Plot visualisieren. Datenexploration und -analyse für mehrere Variablen Wir betrachten zwei Spalten \\(x = (x_1,\\ldots,x_N)\\) und \\(y = (y_1,\\ldots, y_N)\\). Kovarianz \\(s_{xy} = \\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\overline x)(y_i - \\overline y)\\) Korrelation \\(\\rho_{xy} = \\frac{s_{xy}}{s_x\\cdot s_y} \\in [-1,1]\\). \\(\\rho \\approx 1\\): Starke positive Korrelation, wenn \\(x\\) groß ist, ist \\(y\\) auch groß. \\(\\rho \\approx -1\\): Starke negative Korrelation, wenn \\(x\\) groß ist, ist \\(y\\) klein \\(\\rho \\approx 0\\): Wenig/keine Korrelation. Von Kiatdd - Eigenes Werk, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=37108966 1.3.2 COVID-19 Daten Vergleiche die Einführung in Mathematik für Data Science 1 vom letzten Semester. 1.3.3 Netflix Prize Hierbei geht es darum, ob aus bekannten Bewertungen von vielen verschiedenen Benutzern für viele verschiedene Filme abgeleitet werden kann, ob ein bestimmter Nutzer einen bestimmten Film mag (also positiv bewerten würde). Vergleiche auch Wikipedia:Netflix_Prize Das (Trainings-)Daten bestehen über 480189 Benutzer, die für 17770 Filme insgesamt 100480507 Bewertungen als ganze Zahlen zwischen 1 und 5 verteilten. Ziel der Datenanalyse war es, für 2817131 “Paare” von Benutzern und Filmen, die Bewertung vorauszusagen. Neben der schieren Masse an Daten kamen noch Einschränkungen hinzu, die ein Mindestmaß an Qualität der Vorhersage sicherstellen sollten. Das Problem ließe sich wie folgt darstellen. Benutzer \\ Film F1 F2 ... Fn ... B1 – 3 ... 5 ... B2 3 4 ... 2 ... B3 1 2 ... ? ... ... 3 4 ... – ... Gegeben viele (aber bei weitem nicht alle) Einträge in einer riesigen Tabelle. Können wir aus den Zusammenhängen bestimmte fehlende Einträge (z.B. wie findet Nutzer B3 den Film Fn) herleiten? Die besten Lösungen für dieses Problem basieren durchweg auf Machine Learning Ansätzen. 1.4 Python Die Programmiersprache python wird uns durchs Semester begleiten. Einfach weil sie so wichtig ist für Data Science aber auch weil sie (meiner Meinung nach) einfach zu erlernen und zu benutzen ist. 1.5 Aufgaben 1.5.1 Python Bringen sie ihr python zum Laufen, installieren sie numpy, scipy und matplotlib und führen sie das folgende script aus. import numpy as np import matplotlib.pyplot as plt N = 20 xmax = 2 xmin = 0 xdata = np.linspace(xmin, xmax, N) ydata = np.exp(xdata) plt.figure(1) plt.plot(xdata, ydata, &#39;.&#39;) plt.figure(2) plt.semilogy(xdata, ydata, &#39;.&#39;) plt.show() 1.5.2 Einheitsmatrix Schreiben sie ein script, dass die 5x5 Einheitsmatrix auf 3 verschiedene Arten erzeugt. (Eine Art könnte die eingebaute numpy Funktion eye sein). import numpy as np idfive = np.eye(5) print(idfive) Hinweis: schauen sie sich mal an wie numpy’s arrays funktionieren. 1.5.3 Matrizen Multiplikation und Potenz Schreiben sie ein script, das die Übungsaufgabe aus der Vorlesung (potenzieren der Matrizen \\(M_i\\), \\(i=1,2,3,4\\)) löst. Zum Beispiel mit import numpy as np mone = np.array([[0.9, 0.9], [0.9, 0.9]]) mone_ptwo = mone @ mone print(mone_ptwo) mone_pfour = mone_ptwo @ mone_ptwo print(mone_pfour) Oder so: import numpy as np mone = np.array([[0.9, 0.9], [0.9, 0.9]]) mone_p = np.eye(2) for k in range(16): mone_p = mone_p @ mone if k == 1 or k == 3 or k == 15: print(&#39;k=&#39;, k+1) print(mone_p) Achtung: bei Matrizen kann auch * benutzt werden – das ist aber nicht die richtige Matrizenmultiplikation (sondern die Multiplikation eintragsweise) Moegliche Realisierung der Matrizenmultiplikation np.dot(A, B) – die klassische Methode A.dot(B) – das selbe (manchmal besser, wenn A etwas allgemeiner ist (zum Beispiel eine scipy.sparse matrix) A @ B – convenience Notation "],["lineare-regression.html", "2 Lineare Regression 2.1 Rauschen und Fitting 2.2 Ansätze für lineare Regression 2.3 Fehlerfunktional und Minimierung 2.4 Berechnung der Bestlösung 2.5 Beispiel", " 2 Lineare Regression Auch bekannt als lineare Ausgleichsrechnung oder Methode der kleinsten Quadrate. Ein wesentlicher Aspekt von Data Science ist die Analyse oder das Verstehen von Daten. Allgemein gesagt, es wird versucht, aus den Daten heraus Aussagen über Trends oder Eigenschaften des Phänomens zu treffen, mit welchem die Daten im Zusammenhang stehen. Wir kommen nochmal auf das Beispiel aus der Einführungswoche zurück, werfen eine bereits geschärften Blick darauf und gehen das mit verbesserten mathematischen Methoden an. Gegeben seien die Fallzahlen aus der CoVID Pandemie 2020 für Bayern für den Oktober 2020. Tag 12 13 14 15 16 17 18 19 20 21 Fälle 681 691 1154 1284 127 984 573 1078 1462 2239 Tag 22 23 24 25 26 27 28 29 30 31 Fälle 2236 2119 1663 1413 2283 2717 3113 2972 3136 2615 Fallzahlen von Sars-CoV-2 in Bayern im Oktober 2020 Wieder stellen wir uns die Frage ob wir in den Daten einen funktionalen Zusammenhang feststellen können. Also ob wir die Datenpaare (Tag \\(x\\), Infektionen am Tag \\(x\\)) die wir als (\\(x_i\\), \\(y_i\\)) über eine Funktion \\(f\\) und die Paare (\\(x\\), \\(f(x)\\)) beschreiben (im Sinne von gut darstellen oder approximieren) können. 2.1 Rauschen und Fitting Beim obigen Beispiel (und ganz generell bei Daten) ist davon auszugehen, dass die Daten verrauscht sind, also einem Trend folgen oder in einem funktionalen Zusammenhang stehen aber zufällige Abweichungen oder Fehler enthalten. Unter diesem Gesichtspunkt ist eine Funktion, die \\(f(x_i)=y_i\\) erzwingt nicht zielführend. (Wir wollen Trends und größere Zusammenhänge erkennen und nicht kleine Fehler nachzeichnen.) Das zu strenge Anpassen an möglicherweise verrauschte Daten wird overfitting genannt. Vielmehr werden wir nach einer Funktion \\(f\\) suchen, die die Daten näherungsweise nachstellt: \\(f(x_i)\\approx y_i\\) Hierbei passen jetzt allerdings auch Funktionen, die vielleicht einfach zu handhaben sind aber die Daten kaum noch repräsentieren. Jan spricht von underfitting. Eine gute Approximation besteht im Kompromiss von nah an den Daten aber mit wenig overfitting. 2.2 Ansätze für lineare Regression Um eine solche Funktion \\(f\\) zu finden, trifft Jan als erstes ein paar Modellannahmen. Modellannahmen legen fest, wie das \\(f\\) im Allgemeinen aussehen soll und versuchen dabei die Bestimmung von \\(f\\) zu ermöglichen zu garantieren, dass \\(f\\) auch die gewollten Aussagen liefert und sicherzustellen, dass \\(f\\) zum Problem passt. Jan bemerke, dass die ersten beiden Annahmen im Spannungsverhältnis zur dritten stehen. Lineare Regression besteht darin, dass die Funktion als Linearkombination \\[\\begin{equation*} f_w(x) = \\sum_{j=1}^n w_j b_j(x) \\end{equation*}\\] von Basisfunktionen geschrieben wird und dann die Koeffizienten \\(w_i\\) so bestimmt werden, dass \\(f\\) die Daten bestmöglich annähert. Jan bemerke, dass bestmöglich wieder overfitting bedeuten kann aber auch, bei schlechter Wahl der Basis, wenig aussagekräftig sein kann. Der gute Kompromiss liegt also jetzt in der Wahl der passenden Basisfunktionen und deren Anzahl. (Mehr Basisfunktionen bedeutet möglicherweise bessere Approximation aber auch die Gefahr von overfitting.) Typische Wahlen für die Basis \\(\\{b_1, b_2, \\dotsc, b_n\\}\\) sind Polynome: \\(\\{1, x, x^2, \\dotsc, x^{n-1}\\}\\) – für \\(n=2\\) ist der Ansatz eine Gerade Trigonometrische Funktionen: \\(\\{1, \\cos(x), \\sin(x), \\cos(2x), \\sin(2x), \\dotsc\\}\\) Splines – Polynome, die abschnittsweise definiert werden Wavelets – Verallgemeinerungen von trigonometrischen Funktionen 2.3 Fehlerfunktional und Minimierung Wir setzen nun also an \\[\\begin{equation*} f_w(x) = \\sum_{j=1}^nw_j b_j (x) \\end{equation*}\\] und wollen damit \\(y_i \\approx f_w(x_i)\\) bestmöglich erreichen (indem wir die Koeffizienten \\((w_1, \\dotsc, w_n)\\) optimal wählen. Bestmöglich und optimal spezifizieren wir über den Mittelwert der quadratischen Abweichungen in der Approximation über alle Datenpunkte \\[\\begin{equation*} \\frac{1}{N}\\sum_{i=1}^N (y_i - f_w(x_i))^2 \\end{equation*}\\] Ein paar Bemerkungen jetzt müssen wir die \\(w_i\\)’s bestimmen so dass dieser Fehlerterm minimal wird das optimale \\(w\\) ist unabhängig von einer Skalierung des Fehlerterms des wegen schreiben wir gerne einfach \\(\\frac 12 \\sum_{i=1}^N (y_i - f_w(x_i))^2\\) als das Zielfunktional, das es zu minimieren gilt. Wie finden wir jetzt die \\(w_i\\)’s? Zunächst gilt, dass \\[\\begin{equation*} f_w(x_i) = \\sum_{i=j}^n w_j b_j(x_i) = \\begin{bmatrix} b_1(x_i) &amp; b_2(x_i) &amp; \\dots &amp; b_n(x_i) \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\end{equation*}\\] und wenn wir alle \\(f_w(x_i)\\), \\(i=1,\\dotsc,N\\) übereinander in einen Vektor schreiben, dass \\[\\begin{equation*} f_w(\\mathbf x) := \\begin{bmatrix} f_w(x_1) \\\\ \\vdots \\\\ f_w(x_N) \\end{bmatrix} = \\begin{bmatrix} b_1(x_1) &amp; \\dots &amp; b_n(x_1) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_1(x_N) &amp; \\dots &amp; b_n(x_N) \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} =: \\Phi(\\mathbf x) w \\end{equation*}\\] Damit, mit \\(\\mathbf y\\) als den Vektor aller \\(y_i\\)’s, und mit der Definition der Vektornorm, können wir unser Minimierungsproblem schreiben als \\[\\begin{equation*} \\frac 12 \\sum_{i=1}^N (y_i - f_w(x_i))^2 = \\frac 12 \\| \\mathbf y - \\Phi (\\mathbf x) w\\|^2 \\to \\min. \\end{equation*}\\] Wir bemerken, dass das Fehlerfunktional immer größer und bestenfalls gleich 0 ist falls das lineare Gleichungssystem \\(\\Phi (\\mathbf x)w = \\mathbf y\\) eine Lösung \\(w\\) hat, ist das auch eine Lösung unserer Minimierung im typischen Falle aber ist allerdings \\(N\\gg n\\) und das System überbestimmt (\\(n=N\\) würde ein overfitting bedeuten…) sodass wir keine Lösung des linearen Gleichungssystems erwarten können. Das Minimierungsproblems selbst hat allerdings immer eine Lösung. 2.4 Berechnung der Bestlösung Wir suchen also ein Minimum der Funktion (mit \\(\\Phi\\), \\(\\mathbf x\\), \\(\\mathbf y\\) gegeben) \\[\\begin{equation*} \\begin{split} w \\mapsto \\frac 12 \\|\\mathbf y - \\Phi(\\mathbf x)w \\|^2 &amp;= \\frac 12 (\\mathbf y - \\Phi(\\mathbf x)w)^T(\\mathbf y - \\Phi(\\mathbf x)w) \\\\ &amp;= \\frac 12 [\\mathbf y^T\\mathbf y - \\mathbf y^T \\Phi(\\mathbf x)w - w^T \\Phi(\\mathbf x)^T\\mathbf y + w^T \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w] \\\\ &amp;= \\frac 12 [\\mathbf y^T\\mathbf y -2 w^T \\Phi(\\mathbf x)^T\\mathbf y + w^T \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w] \\end{split} \\end{equation*}\\] wobei wir die Definition der Norm \\(\\|v\\|^2 = v^Tv\\) und die Eigenschaft, dass für die skalare Größe \\(w^T \\Phi(\\mathbf x)^T\\mathbf y = [w^T \\Phi(\\mathbf x)^T\\mathbf y]^T = \\mathbf y^T \\Phi(\\mathbf x)w\\) gilt, ausgenutzt haben. Wären \\(w\\) und \\(\\mathbf y\\) keine Vektoren sondern einfach reelle Zahlen, wäre das hier eine Parabelgleichung \\(aw^2 + bw + c\\) mit \\(a&gt;0\\), die immer eine Minimalstelle hat. Tatsächlich gilt hier alles ganz analog. Insbesondere ist \\(\\Phi(\\mathbf x)^T\\Phi(\\mathbf x)\\) in der Regel “größer 0” (was heißt das wohl bei quadratischen Matrizen?). Und mittels “Nullsetzen” der ersten Ableitung können wir das Minimum bestimmen. In diesem Fall ist die erste Ableitung (nach \\(w\\)) \\[\\begin{equation*} \\nabla_w (\\frac 12 \\|\\mathbf y - \\Phi(\\mathbf x) \\|^2) = \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w - \\Phi(\\mathbf x)^T\\mathbf y, \\end{equation*}\\] (den Gradienten \\(\\nabla_w\\) als Ableitung von Funktionen mit mehreren Veränderlichen werden wir noch genauer behandeln) was uns als Lösung, die Lösung des linearen Gleichungssystems \\[\\begin{equation*} \\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w = \\Phi(\\mathbf x)^T\\mathbf y \\end{equation*}\\] definiert. Letzte Frage: Wann hat dieses Gleichungssystems eine eindeutige Lösung? Mit \\(N&gt;n\\) (also \\(\\Phi(\\mathbf x)\\) hat mehr Zeilen als Spalten) gelten die Äquivalenzen: \\(\\Phi(\\mathbf x)^T\\Phi(\\mathbf x)w = \\Phi(\\mathbf x)^T\\mathbf y\\) hat eine eindeutige Lösung die Matrix \\(\\Phi(\\mathbf x)^T\\Phi(\\mathbf x)\\) ist regulär die Spalten von \\(\\Phi(\\mathbf x)\\) sind linear unabhängig die Vektoren \\(b_i(\\mathbf x)\\) sind linear unabhängig. Praktischerweise tritt genau diese Situation im Allgemeinen ein. \\(N&gt;n\\) (mehr Datenpunkte als Parameter) \\(b_i\\)’s werden als linear unabhängig (im Sinne ihres Funktionenraums) gewählt, was die lineare unabhängigket der \\(b_i(\\mathbf x)\\) impliziert. 2.5 Beispiel Unsere Covid-Zahlen “mit einer Geraden angenähert”: \\(f_w(x) = w_1 + w_2 x\\) – das heißt \\(n=2\\) und Basisfunktionen \\(b_1(x)\\equiv 1\\) und \\(b_2(x) = x\\) \\(\\mathbf x = (1,2,3, \\dots, 31)\\) – die Tage im Februar, das heißt \\(N=31\\) \\(\\mathbf y = (352, 347, \\dots, 2615)\\) – die Fallzahlen Wir bekommen \\[\\begin{equation*} \\Phi(\\mathbf x) = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; 31 \\end{bmatrix} \\end{equation*}\\] (die Spalten sind linear unabhängig) und müssen “nur” das 2x2 System \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ 1 &amp; 2 &amp; 3 &amp; \\dots &amp; 31 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ \\vdots \\\\ 1 &amp; 31 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ 1 &amp; 2 &amp; 3 &amp; \\dots &amp; 31 \\end{bmatrix} \\begin{bmatrix} 352 \\\\ 347 \\\\ 308 \\\\ \\vdots \\\\ 2615 \\end{bmatrix} \\] lösen um die Approximation \\(f_w\\) zu bestimmen. Und noch als letzte Bemerkung. Egal wie die Basisfunktionen \\(b_i\\) gewählt werden, die Parameterabhängigkeit von \\(w\\) ist immer linear. Deswegen der Name lineare Ausgleichsrechnung. "],["matrix-zerlegungen.html", "3 Matrix-Zerlegungen 3.1 QR Zerlegung 3.2 Singulärwertzerlegung 3.3 Aufgaben", " 3 Matrix-Zerlegungen Die Lösung \\(w\\) des Problems der linearen Ausgleichsrechnung war entweder als Lösung eines Optimierungsproblems \\[\\begin{equation*} \\min_{w} \\| Aw - y \\|^2 \\end{equation*}\\] oder als Lösung des linearen Gleichungssystems \\[\\begin{equation*} A^TAw=y \\end{equation*}\\] gegeben. Hierbei steht nun \\(A\\in \\mathbb R^{N\\times n}\\) für die Matrix \\(\\Phi(\\mathbf x)\\) der Daten und Basisfunktionen. Wir hatten uns überlegt, dass in den meisten Fällen die Matrix mehr Zeilen als Spalten hat (\\(N&gt;n\\)) und die Spalten linear unabhängig sind. 3.1 QR Zerlegung Wir betrachten nochmal das Optimierungsproblem \\(\\min_{w} \\| Aw - y \\|^2\\). Gäbe es eine Lösung des Systems \\(Aw=y\\), wäre das sofort eine Lösung des Optimierungsproblems. Da \\(A\\) aber mehr Zeilen als Spalten hat, ist das System \\(Aw=y\\) überbestimmt und eine Lösung in der Regel nicht gegeben. Die Überlegung ist nun, die Gleichung \\(Aw=y\\) so gut wie möglich zu erfüllen, indem wir die relevanten Gleichungen indentifizieren und wenigstens diese lösen. Ein systematischer (und wie wir später sehen werden auch zum Optimierungsproblem passender) Zugang bietet die QR Zerlegung. Theorem 3.1 (QR Zerlegung) Sei \\(A\\in \\mathbb R^{m\\times n}\\), \\(m&gt;n\\). Dann existiert eine orthonormale Matrix \\(Q\\in \\mathbb R^{m\\times m}\\) und eine obere Dreiecksmatrix \\(\\hat R\\in \\mathbb R^{n\\times n}\\) derart dass \\[\\begin{equation*} A = QR =: Q \\begin{bmatrix} \\hat R \\\\ 0 \\end{bmatrix}. \\end{equation*}\\] Hat \\(A\\) vollen (Spalten)Rang, dann ist \\(\\hat R\\) invertierbar. Hier heißt orthonormale Matrix \\(Q\\), dass die Spalten von \\(Q\\) paarweise orthogonal sind. Insbesondere gilt \\[\\begin{equation*} Q^T Q = I. \\end{equation*}\\] Für unser zu lösendes Problem ergibt sich dadurch die Umformung \\[\\begin{equation*} Aw = y \\quad \\Leftrightarrow \\quad QRw=y \\quad \\Leftrightarrow \\quad Q^TQRw=Q^T y \\quad \\Leftrightarrow \\quad Rw = Q^Ty \\end{equation*}\\] oder auch \\[\\begin{equation*} \\begin{bmatrix} \\hat R \\\\ 0 \\end{bmatrix}w = Q^Ty = \\begin{bmatrix} Q_1^T \\\\ Q_2^T \\end{bmatrix} y \\end{equation*}\\] (wobei wir die \\(Q_1\\in \\mathbb R^{m\\times n}\\) die Matrix der ersten \\(n\\) Spalten von \\(Q\\) ist) und als Kompromiss der Vorschlag, das Teilsystem \\[\\begin{equation*} \\hat R w = Q_1^Ty \\end{equation*}\\] nach \\(w\\) zu lösen und in Kauf zu nehmen, dass der Rest, nämlich das \\(Q_2^Ty\\), nicht notwendigerweise gleich null ist. Wir halten zunächst mal fest, dass Obwohl \\(Q\\) eine reguläre Matrix ist, bedarf der Übergang von \\(Aw=y\\) zu \\(Q^TAw=Q^Ty\\) einer genaueren Analyse. Wir bemerken, dass für eine hypothetische komplette Lösung \\(Aw-y\\), diese Transformation keine Rolle spielt. Für die Kompromisslösung jedoch schon, weil beispielsweise verschiedene Konstruktionen eines invertierbaren Teils, verschiedene Residuen bedeuten und somit Optimalität im Sinne von \\(\\min_w \\|Aw-y\\|^2\\) nicht garantiert ist. Allerdings, wie Sie als Übungsaufgabe nachweisen werden, löst dieser Ansatz tatsächlich das Optimierungsproblem. 3.2 Singulärwertzerlegung Eine weitere Matrix Zerlegung, die eng mit der Lösung von Optimierungsproblemen oder überbestimmten Gleichungssystemen zusammenhängt ist die Singulärwertzerlegung (SVD – von english: Singular Value Decomposition). Theorem 3.2 (Singulärwertzerlegung) Sei \\(A\\in \\mathbb C^{m\\times n}\\), \\(m\\geq n\\). Dann existieren orthogonale Matrizen \\(U \\in \\mathbb C^{m\\times m}\\) und \\(V\\in \\mathbb C^{n\\times n}\\) und eine Matrix \\(\\Sigma \\in \\mathbb R^{m\\times n}\\) der Form \\[\\begin{equation*} \\Sigma = \\begin{bmatrix} \\sigma_1 &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; \\sigma_2 &amp;\\ddots &amp; \\vdots\\\\ 0 &amp; \\ddots &amp; \\ddots &amp;0\\\\ 0 &amp; \\dots&amp;0 &amp; \\sigma_n \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] mit reellen sogenannten Singulärwerten \\[\\begin{equation*} \\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0 \\end{equation*}\\] sodass gilt \\[\\begin{equation*} A = U \\Sigma V^* \\end{equation*}\\] wobei gilt \\(V^* = \\overline{V^T}\\) (transponiert und komplex konjugiert). Ein paar Bemerkungen. Ist \\(A\\) reell, können auch \\(U\\) und \\(V\\) reell gewählt werden. Die Annahme \\(m \\geq n\\) war nur nötig um für die Matrix \\(\\Sigma\\) keine Fallunterscheidung zu machen. (Für \\(m\\leq n\\) “steht der Nullblock rechts von den Singulärwerten”). Insbesondere gilt \\(A^* = V\\Sigma U^*\\) ist eine SVD von \\(A^*\\). Eine Illustration der Zerlegung ist in Abbildung ?? zu sehen. Wir machen einige Überlegungen im Hinblick auf große Matrizen. Sei dazu \\(m&gt;n\\), \\(A\\in \\mathbb C^{m\\times n}\\) und \\(A=U\\Sigma V^*\\) eine SVD wie in Theorem @ref{thm:SVD}. Sei nun \\[\\begin{equation*} U = \\begin{bmatrix} U_1 &amp; U_2 \\end{bmatrix} % = \\begin{bmatrix} V_1^* &amp; V_2^* \\end{equation*}\\] partitioniert sodass \\(U_1\\) die ersten \\(n\\) Spalten von \\(U\\) enthält. Dann gilt (nach der Matrix-Multiplikations Regel Zeile mal Spalte die Teile \\(U_2\\) und \\(V_2\\) immer mit dem Nullblock in \\(\\Sigma\\) multipliziert werden) dass \\[\\begin{equation*} A = U\\Sigma V = \\begin{bmatrix} U_1 &amp; U_2 \\end{bmatrix} \\begin{bmatrix} \\hat \\Sigma \\\\ 0 \\end{bmatrix} V^* % \\begin{bmatrix} V_1^* \\\\ V_2^* \\end{bmatrix} = U_1 \\hat \\Sigma V^* % \\begin{bmatrix} V_1^* \\\\ V_2^* \\end{bmatrix} \\end{equation*}\\] Es genügt also nur die ersten \\(m\\) Spalten von \\(U\\) zu berechnen. Das ist die sogenannte slim SVD. Hat, darüberhinaus, die Matrix \\(A\\) keinen vollen Rang, also \\(\\operatorname{Rg}(A) = r &lt; n\\), dann: ist \\(\\sigma_i=0\\), für alle \\(i=r+1, \\dotsc, n\\), (wir erinnern uns, dass die Singulärwerte nach Größe sortiert sind) die Matrix \\(\\hat \\Sigma\\) hat \\(n-r\\) Nullzeilen für die Zerlegung sind nur die ersten \\(r\\) Spalten von \\(U\\) und \\(V\\) relevant – die sogenannte Kompakte SVD. In der Datenapproximation ist außerdem die truncated SVD von Interesse. Dazu sei \\(\\hat r&lt;r\\) ein beliebig gewählter Index. Dann werden alle Singulärwerte, \\(\\sigma_i=0\\), für alle \\(i=\\hat r+1, \\dotsc, n\\), abgeschnitten – das heißt null gesetzt und die entsprechende kompakte SVD berechnet. Hier gilt nun nicht mehr die Gleichheit in der Zerlegung. Vielmehr gilt \\[\\begin{equation*} A \\approx A_{\\hat r} \\end{equation*}\\] wobei \\(A_{\\hat r}\\) aus der truncated SVD von \\(A\\) erzeugt wurde. Allerdings ist diese Approximation von \\(A\\) durch optimal in dem Sinne, dass es keine Matrix vom Rang \\(\\hat r \\geq r=\\operatorname{Rg}(A)\\) gibt, die \\(A\\) (in der induzierten euklidischen Norm1) besser approximiert. Es gilt \\[\\begin{equation*} \\min_{B\\in \\mathbb C^{m\\times n}, \\operatorname{Rg}(B)=\\hat r} \\|A-B\\|_2 = \\|A-A_{\\hat r}\\|_2 = \\sigma_{\\hat r + 1}; \\end{equation*}\\] vgl. Bollhoefer/Mehrmann Satz 14.15. Zum Abschluss noch der Zusammenhang zum Optimierungsproblem. Ist \\(A=U\\Sigma V^*\\) “SV-zerlegt”, dann gilt \\[\\begin{equation*} A^*Aw = V\\Sigma^*U^*U\\Sigma V^*w = V\\hat \\Sigma^2 V^* \\end{equation*}\\] und damit \\[\\begin{equation*} A^*Aw = A^*y \\quad \\Leftrightarrow \\quad V\\hat \\Sigma^2 V^* = V\\Sigma^*U^*y \\quad \\Leftrightarrow \\quad w = V(\\Sigma^+)^*U^*y \\end{equation*}\\] wobei \\[\\begin{equation*} \\Sigma^+ = \\begin{bmatrix} \\hat \\Sigma^{-1} \\\\ 0_{m-n \\times n} \\end{bmatrix} \\end{equation*}\\] aus \\(\\Sigma \\hat \\Sigma^{-1}\\hat \\Sigma^{-1}\\) herrührt. Bemerkung: \\(\\Sigma^+\\) kann auch definiert werden, wenn \\(\\hat \\Sigma\\) nicht invertierbar ist (weil manche Diagonaleinträge null sind). Dann wird \\(\\hat \\Sigma^+\\) betrachtet, bei welcher nur die \\(\\sigma_i&gt;0\\) invertiert werden und die anderen \\(\\sigma_i=0\\) belassen werden. Das definiert eine sogenannte verallgemeinerte Inverse und löst auch das Optimierungsproblem falls \\(A\\) keinen vollen Rang hat. Illustration der SVD. Bitte beachten der \\(*\\) bedeutet hier transponiert und komplex konjugiert. By Cmglee - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=67853297 3.3 Aufgaben Erklärung: (T) heißt theoretische Aufgabe, (P) heißt programmieren. 3.3.1 Norm und Orthogonale Transformation (T) Sei \\(Q\\in \\mathbb R^{n\\times n}\\) eine orthogonale Matrix und sei \\(y\\in \\mathbb R^{n}\\). Zeigen Sie, dass \\[\\begin{equation*} \\|y\\|^2 = \\|Qy \\|^2 \\end{equation*}\\] gilt. (2 Punkte) 3.3.2 Kleinste Quadrate und Mittelwert Zeigen sie, dass der kleinste Quadrate Ansatz zur Approximation einer Datenwolke \\[\\begin{equation*} (x_i, y_i), \\quad i=1,2,\\dotsc,N, \\end{equation*}\\] mittels einer konstanten Funktion \\(f(x)=w_1\\) auf \\(w_1\\) auf den Mittelwert der \\(y_i\\) führt. (6 Punkte) 3.3.3 QR Zerlegung und Kleinstes Quadrate Problem (T) Sei \\(A\\in \\mathbb R^{m,n}\\), \\(m&gt;n\\), \\(A\\) hat vollen Rank und sei \\[\\begin{equation*} \\begin{bmatrix} Q_1 &amp; Q_2 \\end{bmatrix} \\begin{bmatrix} \\hat R \\\\ 0 \\end{bmatrix} = A \\end{equation*}\\] eine QR-Zerlegung von \\(A\\). Zeigen sie, dass die Lösung von \\[\\begin{equation*} \\hat R w = Q_1^T y \\end{equation*}\\] ein kritischer Punkt (d.h. der Gradient \\(\\nabla_w\\) verschwindet) von \\[\\begin{equation*} w \\mapsto \\frac 12 \\| Aw - y \\|^2 \\end{equation*}\\] ist. Hinweis: Die Formel für den Gradienten wurde in der Vorlesung 02 hergeleitet. (6 Punkte) 3.3.4 Eigenwerte Symmetrischer Matrizen (T) Zeigen Sie, dass Eigenwerte symmetrischer reeller Matrizen \\(A\\in \\mathbb R^{n\\times n}\\) immer reell sind. (3 Punkte) 3.3.5 Singulärwertzerlegung und Eigenwerte (T) Zeigen Sie, dass die quadrierten Singulärwerte einer Matrix \\(A\\in \\mathbb R^{m\\times n}\\), \\(m&gt;n\\), genau die Eigenwerte der Matrix \\(A^TA\\) sind und in welcher Beziehung sie mit den Eigenwerten von \\(AA^T\\) stehen. Hinweis: hier ist “\\(m&gt;n\\)” wichtig. (6 Punkte) 3.3.6 Python – Laden und Speichern von Arrays Speichern sie die Covid-Daten aus obiger Tabelle zur späteren Verwendung als ein numpy.array. Beispielsweise so: import numpy as np import matplotlib.pyplot as plt days = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31] case = [352, 347, 308, 151, 360, 498, 664, 686, 740, 418, 320, 681, 691, 1154, 1284, 127, 984, 573, 1078, 1462, 2239, 2236, 2119, 1663, 1413, 2283, 2717, 3113, 2972, 3136, 2615] data = np.vstack([days, case]) print(&#39;Shape of data: &#39;, data.shape) datafilestr = &#39;coviddata.npy&#39; np.save(datafilestr, data) lddata = np.load(datafilestr) plt.figure(1) plt.plot(lddata[0, :], lddata[1, :], &#39;s&#39;, label=&#39;Cases/Day&#39;) plt.title(&#39;Covid Faelle in Bayern im Oktober 2020&#39;) plt.legend() plt.show() 3.3.7 Lineare Regression für Covid Daten (P) Führen sie auf den Covid-daten eine lineare Regression zum Fitten einer konstanten Funktion \\(f(x)=c\\) einer linearen Funktion \\(f(x) = ax+b\\) einer quadratischen Funktion \\(f(x) = w_1 + w_2 x + w_3x^2\\) durch. Berechnen Sie die mittlere quadratische Abweichung \\(\\frac{1}{N}\\sum_{i=1}^N\\|y_i - f(x_i)\\|^2\\) für alle drei Approximationen und plotten Sie die Covid-Daten zusammen mit der aus der Regression erhaltenen Funktion. Beispielsweise so: import numpy as np import matplotlib.pyplot as plt datafilestr = &#39;coviddata.npy&#39; cvdata = np.load(datafilestr) # ## Definieren der Basis Funktion(en) def b_zero(x): &#39;&#39;&#39; eine konstante Funktion &#39;&#39;&#39; return 1. ybf = cvdata[1, :] # die y-werte xbf = cvdata[0, :] # die x-werte N = xbf.size # Anzahl Datenpunkte ybf = ybf.reshape((N, 1)) # reshape als Spaltenvektor bzx = [b_zero(x) for x in xbf] # eine Liste mit Funktionswerten bzx = np.array(bzx).reshape((N, 1)) # ein Spalten Vektor Phix = bzx # hier nur eine Spalte # Das LGS AtA w = At y rhs = Phix.T @ ybf AtA = Phix.T @ Phix # ACHTUNG: das hier geht nur weil AtA keine Matrix ist in diesem Fall w = 1./AtA * rhs # ACHTUNG: das hier ging nur weil AtA keine Matrix ist in diesem Fall def get_regfunc(weights, basfunlist=[]): &#39;&#39;&#39; Eine Funktion, die eine Funktion erzeugt Eingang: die Gewichte, eine Liste von Basisfunktionen Ausgang: die entsprechende Funktion `f = w_1b_1 + ... ` &#39;&#39;&#39; def regfunc(x): fval = 0 for kkk, basfun in enumerate(basfunlist): fval = fval + weights[kkk]*basfun(x) return fval return regfunc const_regfunc = get_regfunc([w], [b_zero]) const_approx = [const_regfunc(x) for x in xbf] const_approx = np.array(const_approx).reshape((N, 1)) plt.figure(1) plt.plot(cvdata[0, :], cvdata[1, :], &#39;s&#39;, label=&#39;Cases/Day&#39;) plt.plot(cvdata[0, :], const_approx, &#39;-&#39;, label=&#39;constant fit&#39;) plt.legend() plt.show() Hinweise: liste = [f(x) for x in iterable] ist sehr bequem um Vektoren von Funktionswerten zu erzeugen aber nicht sehr pythonesque (und auch im Zweifel nicht effizient). Besser ist es Funktionen zu schreiben, die vektorisiert sind. Zum Beispiel können die meisten built-in Funktionen wie np.exp ein array als Eingang direkt in ein array der Funktionswerte umsetzen. Eine Funktion, die eine Funktion erzeugt finde ich sehr hilfreich für viele Anwendungen (ist aber manchmal nicht so gut nachvollziehbar). 3.3.8 Truncated SVD (P+T) (P) Berechnen und plotten sie die Singulärwerte einer \\(4000\\times 1000\\) Matrix mit zufälligen Einträgen und die einer Matrix mit “echten” Daten (hier Simulationsdaten einer Stroemungssimulation)2. Berechnen sie den Fehler der truncated SVD \\(\\|A-A_{\\hat r}\\|\\) für \\(\\hat r = 10, 20, 40\\) für beide Matrizen. (T) Was lässt sich bezüglich einer Kompression der Daten mittels SVD für die beiden Matrizen sagen. (Vergleichen sie die plots der Singulärwerte und beziehen sie sich auf die gegebene Formel für die Differenz). (P+T) Für die “echten” Daten: Speichern sie die Faktoren der bei \\(\\hat r=40\\) abgeschnittenen SVD und vergleichen Sie den Speicherbedarf der Faktoren und der eigentlichen Matrix. Beispielcode: import numpy as np import scipy.linalg as spla import matplotlib.pyplot as plt randmat = np.random.randn(4000, 1000) rndU, rndS, rndV = spla.svd(randmat) print(&#39;U-dims: &#39;, rndU.shape) print(&#39;V-dims: &#39;, rndV.shape) print(&#39;S-dims: &#39;, rndS.shape) plt.figure(1) plt.semilogy(rndS, &#39;.&#39;, label=&#39;Singulaerwerte (random Matrix)&#39;) realdatamat = np.load(&#39;velfielddata.npy&#39;) # # Das hier ist eine aufwaendige Operation rlU, rlS, rlV = spla.svd(realdatamat, full_matrices=False) # # auf keinen Fall `full_matrices=False` vergessen print(&#39;U-dims: &#39;, rlU.shape) print(&#39;V-dims: &#39;, rlV.shape) print(&#39;S-dims: &#39;, rlS.shape) plt.figure(1) plt.semilogy(rlS, &#39;.&#39;, label=&#39;Singulaerwerte (Daten Matrix)&#39;) plt.legend() plt.show() Hinweise: Es gibt viele verschiedene Normen für Vektoren und Matrizen. Sie dürfen einfach mit np.linalg.norm arbeiten. Gerne aber mal in die Dokumentation schauen welche Norm berechnet wird. Die (T) Abschnitte hier bitte mit den anderen (T) Aufgaben oder als Bildschirmausgabe im Programm. Auf Matrixnormen kommen wir noch in der Vorlesung zu sprechen.↩ Download bitte hier – Achtung das sind 370MB↩ "],["hauptkomponentenanalyse.html", "4 Hauptkomponentenanalyse 4.1 Variationskoeffizienten 4.2 Koordinatenwechsel", " 4 Hauptkomponentenanalyse Während in den vorherigen Kapiteln der Versuch war, einen funktionalen Zusammenhang in Daten zu bekommen, geht es jetzt darum, statistische Eigenschaften aus Daten zu extrahieren. Wir werden sehen, dass das auch nur ein anderer Blickwinkel auf das gleiche Problem Wie können wir die Daten verstehen? ist und auch die SVD wieder treffen. Wir nehmen noch einmal die Covid-Daten her, vergessen kurz, dass es sich um eine Zeitreihe handelt und betrachten sie als Datenpunkte \\((x_i, y_i)\\), \\(i=1,\\dotsc,N\\), im zweidimensionalen Raum mit Koordinaten \\(x\\) und \\(y\\). Als erstes werden die Daten zentriert indem in jeder Komponente der Mittelwert \\[\\begin{equation*} x_c = \\frac 1N \\sum_{i=1}^N x_i, \\quad y_c = \\frac 1N \\sum_{i=1}^N y_i. \\end{equation*}\\] abgezogen wird, also die Daten durch \\((x_i-\\bar x,\\, y_i-\\bar y)\\) ersetzt werden. Fallzahlen von Sars-CoV-2 in Bayern im Oktober 2020 – zentriert 4.1 Variationskoeffizienten Als nächstes kann Jan sich fragen, wie gut die Daten durch ihren Mittelwert beschrieben werden und die Varianzen berechnen, die für zentrierte Daten so aussehen \\[\\begin{equation*} s_x^2 = \\frac {1}{N-1} \\sum_{i=1}^N x_i^2, \\quad s_y^2 = \\frac {1}{N-1} \\sum_{i=1}^N y_i^2. \\end{equation*}\\] Im gegebenen Fall bekommen wir \\[\\begin{equation*} s_x^2 = 2480 \\quad s_y^2 \\approx 28029755. \\end{equation*}\\] Da der grosse Unterschied eventuell durch die verschiedene Skalierung der Daten herrührt berechnen wir besser die Variationskoeffizienten mittels \\[\\begin{equation*} \\operatorname {VarK}(x) = \\frac{\\sqrt{s_x^2} }{x_c} \\approx 3.11 \\quad \\operatorname {VarK}(y) = \\frac{\\sqrt{s_y^2} }{y_c} \\approx 4.16 \\end{equation*}\\] und schließen daraus, dass in \\(y\\) Richtung viel passiert und in \\(x\\) Richtung nicht ganz so viel. Das ist jeder Hinsicht nicht befriedigend, wir können weder Redundanzen ausmachen (eine Dimension der Daten vielleicht weniger wichtig?) noch dominierende Richtungen feststellen (obwohl dem Bild nach so eine offenbar existiert) und müssen konstatieren, dass die Repräsentation der Daten im \\((x,y)\\) Koordinatensystem nicht optimal ist. Die Frage ist also, ob es ein Koordinatensystem gibt, dass die Daten besser darstellt. Ein Koordinatensystem ist nichts anderes als eine Basis. Und die Koordinaten eines Datenpunktes sind die Komponenten des entsprechenden Vektors in dieser Basis. Typischerweise sind Koordinatensysteme orthogonal (das heißt eine Orthogonalbasis) und häufig noch orientiert (die Basisvektoren haben eine bestimmte Reihenfolge und eine bestimmte Richtung). 4.2 Koordinatenwechsel Sei nun also \\(\\{b_1,b_2\\}\\subset \\mathbb R^{2}\\) eine orthogonale Basis. Wie allgemein gebräuchlich, sagen wir orthogonal, meinen aber orthonormal. In jedem Falle soll gelten \\[\\begin{equation*} b_1^T b_1=1, \\quad b_2^Tb_2=1, \\quad b_1^Tb_2 = b_2^Tb_1 = 0. \\end{equation*}\\] Wir können also alle Datenpunkte \\(\\mathbf x_i = \\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix}\\) in der neuen Basis darstellen mit eindeutig bestimmten Koeffizienten \\(\\alpha_{i1}\\) und \\(\\alpha_{i2}\\) mittels \\[\\begin{equation*} \\mathbf x_i = \\alpha_{i1}b_1 + \\alpha_{i2}b_2. \\end{equation*}\\] Für orthogonale Basen sind die Koeffizienten durch testen mit dem Basisvektor einfach zu berechnen: \\[\\begin{align*} b_1^T\\mathbb x_i = b_1^T(\\alpha_{i1}b_1 + \\alpha_{i2}b_2) = \\alpha_{i1}b_1^Tb_1 + \\alpha_{i2}b_1^Tb_2 = \\alpha_{i1}\\cdot 1 + \\alpha_{i2} \\cdot 0 = \\alpha_{i1},\\\\ b_2^T\\mathbb x_i = b_2^T(\\alpha_{i1}b_1 + \\alpha_{i2}b_2) = \\alpha_{i1}b_1^Tb_2 + \\alpha_{i2}b_2^Tb_2 = \\alpha_{i1}\\cdot 0 + \\alpha_{i2}\\cdot 0 = \\alpha_{i2}. \\end{align*}\\] Es gilt also \\[\\begin{equation*} \\alpha_{i1} = b_1^T\\mathbb x = b_1^T\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix}, \\quad \\alpha_{i2} = b_2^T\\mathbb x = b_2^T\\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix}. \\end{equation*}\\] Damit, können wir jeden Datenpunkt \\(\\mathbf x_i=(x_i, y_i)\\) in den neuen Koordinaten \\((\\alpha_{i1}, \\alpha_{i2})\\) ausdrücken. Zunächst halten wir fest, dass auch in den neuen Koordinaten die Daten zentriert sind. Es gilt nämlich, dass \\[\\begin{equation*} \\frac 1N \\sum_{i=1}^N \\alpha_{ji}=\\frac 1N \\sum_{i=1}^N b_j^T\\mathbb x_i =\\frac 1N b_j^T \\sum_{i=1}^N \\begin{bmatrix} x_i \\\\ y_i \\end{bmatrix} =\\frac 1N b_j^T \\begin{bmatrix} \\sum_{i=1}^N x_i \\\\ \\sum_{i=1}^N y_i \\end{bmatrix} =b_j^T \\begin{bmatrix} \\frac 1N \\sum_{i=1}^N x_i \\\\ \\frac 1N \\sum_{i=1}^N y_i \\end{bmatrix} =b_j^T \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 0, \\end{equation*}\\] für \\(j=1,2\\). Desweiteren gilt wegen der Orthogonalität von \\(B=[b_1~b_2]\\in \\mathbb R^{2\\times 2}\\), dass \\[\\begin{equation*} x_{i}^2 + y_{i}^2 = \\|\\mathbb x_i\\|^2 = \\|B^T\\mathbb x_i\\|^2 = \\|\\begin{bmatrix} b_1^T \\\\ b_2^T \\end{bmatrix} \\mathbb x_i\\|^2 = \\|\\begin{bmatrix} b_1^T\\mathbb x \\\\ b_2^T\\mathbb x \\end{bmatrix}\\|^2 = \\|\\begin{bmatrix} \\alpha_{i1} \\\\ \\alpha_{i2} \\end{bmatrix}\\|^2 = \\alpha_{i1}^2 + \\alpha_{i2}^2 \\end{equation*}\\] woraus wir folgern, dass in jedem orthogonalen Koordinaten System, die Summe der beiden Varianzen die gleiche ist: \\[\\begin{equation*} s_x^2 + s_y^2 = \\frac{1}{N-1}\\sum_{i=1}^N(x_i^2 + y_i^2) = \\frac{1}{N-1}\\sum_{i=1}^N(\\alpha_{i1}^2 + \\alpha_{i2}^2) =: s_1^2 + s_2^2. \\end{equation*}\\] Auf Matrixnormen kommen wir noch in der Vorlesung zu sprechen.↩ Download bitte hier – Achtung das sind 370MB↩ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
