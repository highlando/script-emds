# Optimierung

\providecommand{\diva}[1]{\mathrm{d} #1}
<!-- \DeclareMathOperator\diva{d} -->

Einige Optimierungsprobleme haben wir schon kennengelernt und zwar

   * bei der linearen Regression (vgl. besonders Kapitel \@ref(sec-linreg-minimierung)) bei der die Parameter der Ansatzfunktion **bestm&ouml;glich** an die Daten gefittet wurden
   * bei der Hauptkomponentenanalyse (vgl. besonders Kapitel \@ref(sec-PCA-maximierung)) als wir die Hauptrichtungen so ermittelt, dass in ihrer Richtung die Varianz **maximal** wurde.

Hier lag in beiden F&auml;llen ein sogenanntes linear-quadratisches Optimierungsproblem vor, f&uuml;r welche die Existenz der L&ouml;sungen bewiesen werden kann und die mit Methoden aus der linearen Algebra direkt gel&ouml;st werden k&ouml;nnen.

Sogenannte *nichtlineare Optimierungsprobleme* liegen vor, wenn die zu erf&uuml;llenden Ziele wirklich nichtlineare Gleichungen enthalten. Zum Bespiel sind g&auml;ngige Elemente von *Neuronalen Netzen* als 
\begin{equation*}
f(x) = \tanh(a\, x + b)
\end{equation*}
gegeben, mit Parametern $a$, $b$ die aus der Minimierung eines *mean square error* Terms der Form
\begin{equation*}
\frac 1N \sum_{k=1}^N|y_k - \tanh(a\, x_k + b)|^2
\end{equation*}
f&uuml;r gegebene Trainingsdaten $(x_k, y_k)$ bestimmt werden.

Wir sehen, dass die Optimierung ein ganz wesentlicher Teil der *Data Science* ist (und ganz unabh&auml;ngig davon ein eigenes Forschungsgebiet ist und quasi &uuml;berall in der Praxis mittel-- oder unmittelbar benutzt wird).

Nachfolgend ein paar Begriffe aus der Optimierung

 * *glatte Optimierung* -- das Modell beinhaltet ausreichend stetige (bzw. differenzierbare) Funktionen. Ein Optimum k&ouml;nnte durch Bestimmung von Ableitungen berechnet werden.
 * *nichtglatte Optimierung* -- das Modell beinhaltet Teile, die nicht differenzierbar sind. Das kann am Problem selbst liegen oder, was bei *machine learning* im Speziellen und *Data Science* an sich eine Rolle spielt, dass die Daten verrauscht sind. Jetzt ist eine (klassische) Ableitung nicht mehr m&ouml;glich aber es existieren diverse Ans&auml;tze zur Abhilfe.
 * *diskrete Optimierung* -- in obigen F&auml;llen sind wir implizit davon ausgegangen, dass die L&ouml;sung und die Gleichungen auf den reellen oder komplexen Zahlen leben. Denkbar ist aber auch, dass diskrete L&ouml;sungen (z.B. Anzahlen oder bin&auml;re Zust&auml;nde) gesucht werden.
 * *kombinatorische Optimierung* -- hier ergeben sich die m&ouml;glichen L&ouml;sungen als Kombination von M&ouml;glichkeiten. Oft ist die Anzahl m&ouml;glicher L&ouml;sungen endlich aber sehr gro&szlig;.
 * *restringierte Optimierung* -- zus&auml;tzlich zu einem Optimalit&auml;tskriterium sind noch Nebenbedingungen zu erf&uuml;llen.

Hier werden wir uns erstmal mit glatter Optimierung ohne Nebenbedingungen besch&auml;ftigen.

## Multivariable Funktionen

Um unsere Optimierungsprobleme in allgemeiner Form zu formulieren und auch um allgemeine L&ouml;sungsans&auml;tze herzuleiten, brauchen wir erstmal ein paar Begriffe aus der Analysis von multivariablen Funktionen.

Es sei also eine Funktion
\begin{equation*}
f\colon \mathbb R^{n}\to \mathbb R^{}, \quad f \colon (x_1,x_2, \dots, x_n) \mapsto f(x_1, x_2, \dotsc, x_n)
\end{equation*}
gegeben, die ein $n$-tupel von Variablen auf einen reellen Wert abbildet. Ein Beispiel w&auml;re
\begin{equation*}
g(x_1, x_2, x_3) = \sin(x_1) + x_3\cos(x_2) + x_1^2 x_2^2 x_3^2 - 2x_2.
\end{equation*}

::: {#variablen-namen .JHSAYS data-latex=''}
Hier verwenden wir jetzt $x_i$ f&uuml;r die $i$-te Variable der multivariablen Funktion. Bitte nicht verwechseln mit der Bezeichnung f&uuml;r den $i$-ten Datenpunkt zum Beispiel in Kapitel \@ref(sec-lineare-regression).

Au&szlig;erdem w&auml;re es besser wir w&uuml;rden die Funktion mit einem Definitionsbereich $D(f)$ angeben, also schreiben
\begin{equation*}
f\colon D(f) \subset \mathbb R^{n}\to \mathbb R^{}
\end{equation*}
da eine Funktion eventuell nicht &uuml;berall definiert ist (bzw. definiert sein muss). Im Sinne der besseren Lesbarkeit, ersparen wir uns dieses Detail.
:::

## Partielle Ableitungen und der Gradient

Wenn wir alle Variablen bis auf die $i$-te f&uuml;r einen Augenblick einfrieren (also auf einen konstanten Wert $\bar x_j$ festlegen und nicht ver&auml;ndern), k&ouml;nnen wir die Teilfunktion 
\begin{equation*}
\bar f_{(i)}\colon \mathbb R^{}\to \mathbb R^{}, \quad \bar f_{(i)}\colon x_i \to f(\bar x_1, \dotsc, \bar x_{i-1}, x_i, \bar x_{i+1}, \dotsc, \bar x_n)
\end{equation*}
als Funktion mit einer Ver&auml;nderlichen (und $n-1$ Parametern) betrachten und wie gehabt ableiten. Die erhaltene, sogenannte *partielle Ableitung* h&auml;ngt von den Parametern $\bar x_j$ ab und wird mit
\begin{equation*}
\frac{\partial f}{\partial x_i}
\end{equation*}
bezeichnet. Alle $n$ m&ouml;glichen partiellen Ableitungen in einen Vektor geschrieben ergeben den sogenannten *Gradienten*
\begin{equation*}
\nabla f = 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
\colon \mathbb R^{n} \to \mathbb R^{n},
\end{equation*}
der wiederum eine (vektorwertige) Funktion von $n$ Variablen ist.
F&uuml;r unser obiges Beispiel bekommen wir
\begin{equation*}
\nabla g(x_1, x_2, x_3) =
\begin{bmatrix}
\cos(x_1)+2x_1x_2^2x_3^2\\
-x_3\sin(x_2) + x_1^2x_2x_3^2 - 2 \\
\cos(x_2) + 2x_1^2x_2^2x_3
\end{bmatrix}
\end{equation*}

## Richtungs-Ableitung

Die Betrachtung der Funktionen $\bar f_{(i)}$ erm&ouml;glicht die Analysis des Verhaltens der Funktion entlang der $i$-ten Koordinaten Richtung. Um die Funktion in einer beliebigen Richtung um einen festen Punkt 
\begin{equation*}
\bar x = (\bar x_1, \bar x_2, \dotsc, \bar x_n)
\end{equation*}
zu untersuchen, kann die Funktion entlang einer Richtung $v \in \mathbb R^{n}$ parametrisiert werden:
\begin{equation*}
\bar f_{(v)} \colon \mathbb R^{} \to \mathbb R^{}, \quad t \mapsto \bar f_{(v)}(t) = f(\bar x + tv)
\end{equation*}

::: {#richtung-is-partiell .JHSAYS data-latex=''}
F&uuml;r $v=e_i$ bekommen wir direkt, dass $\bar f_{(v)}=\bar f_{(i)}$.
:::

Wiederum handelt es sich um eine reelle Funktion in einer Variablen (hier der Parameter $t$), die wir schulbuchm&auml;&szlig;ig ableiten k&ouml;nnten. F&uuml;r diese sogenannte *Richtungsableitung* gilt der folgende Zusammenhang^[Das folgt aus der Kettenregel f&uuml;r multivariable Funktionen, die wir in der Vorlesung *Mathematik f&uuml;r Data Science 2* noch beweisen werden]
\begin{equation*}
\frac{\diva \bar f_{(v)}}{\diva t}(0) = \sum_i^n \frac{\partial f}{\partial x_i}(\bar x_i) v_i = \langle \nabla f(\bar x), v \rangle
\end{equation*}

::: {#richtung-ableitung .JHSAYS data-latex=''}
Die Ver&auml;nderungsrate der Funktion $f$ im Punkt $\bar x$ in der Richtung von $v$ ist gleich dem Skalarprodukt aus Gradient im Punkt $\bar x$ und der gew&auml;hlten Richtung.
:::

Sei nun $v$ normiert, also $\|v\|=1$, dann gilt nach der Winkelformel, dass 
\begin{equation*}
\langle \nabla f(\bar x), v \rangle = \|\nabla f(\bar x) \| \cos (\alpha)
\end{equation*}
wobei $\alpha$ der von $\nabla f(\bar x)$ und $v$ eingeschlossene Winkel ist. Wir bemerken, dass die Ver&auml;nderung maximal wird, wenn $\alpha = 0$ oder 
\begin{equation*}
v = \frac{1}{ \| \nabla f(\bar x) \| } \nabla f(\bar x)
\end{equation*}
als ein Vielfaches des Gradientenvektors gew&auml;hlt wird.
Andersherum k&ouml;nnen wir einige ganz wesentliche Aspekte folgern:

 * der Gradient selbst in die Richtung des st&auml;rksten Anstiegs
 * der negative Gradient $-\nabla f(\bar x)$ zeigt in die Richtung des st&auml;rksten Abfalls
 * ist der Gradient der Nullvektor, dann findet keine Ver&auml;nderung mehr statt, d.h. dass aus $\nabla f(\bar x) =0$ folgt, dass $\bar x$ ein kritischer Punkt von $f$ ist 
 * ist der Gradient nicht der Nullvektor, kann $\bar x$ kein Minimum und kein Maximum sein (weil die Funktion in $-\nabla f(\bar x)$ Richtung abf&auml;llt und in $\nabla f(\bar x)$ Richtung ansteigt)

## Optimierung

Ganz allgemein k&ouml;nnen wir unsere bisherigen Optimierungsprobleme schreiben als
\begin{equation*}
f(x) \to \min_{x\in \mathbb R^{n}},
\end{equation*}
das hei&szlig;t f&uuml;r eine Funktion $f\colon \mathbb R^n \to \mathbb R^{}$ suchen wir das $x \in D(f) \subset \mathbb R^{n}$, f&uuml;r das $f(x)$ minimal wird. 

::: {.definition}
Gegeben sei $f\colon \mathbb R^{n} \to \mathbb R^{}$. Wir nennen $x^* \in D(f)\subset \mathbb R^{n}$ 

 * ein *lokales Minimum* (von $f$) falls es ein $\epsilon >0$ gibt, sodass
\begin{equation*}
f(x^*) \leq f(x)
\end{equation*}
f&uuml;r alle $x \in D(f) \cap B_\epsilon (x^*)$.

 * ein globales Minimum, falls $f(x^*) \leq f(x)$ f&uuml;r alle $x\in D(f)$ ist.

:::

In dieser Definition benutzen wir "$\leq$", da das im Hinblick auf Minimierung (der Wert von $f$ ist wichtig, die Stelle, an der das Minimum angenommen wird, ist weniger entscheidend) praktikabel ist. Wir k&ouml;nnten aber auch von strikten (oder "echten") Minima sprechen und "$<$" verlangen.

Der Schnitt der Umgebung 
$$ B_\epsilon(x^*) := \{x\in \mathbb R^{n}| \|x-x^*\|< \epsilon\} $$
mit dem Definitionsbereich $D(f)$ ist relevant, wenn $x^*$ genau auf dem Rand des Definitionsbereichs liegt.

Nun erstmal ein hinreichendes Kriterium f&uuml;r ein Extremum
[vgl. Thm. 2.2, @NocW06]:

::: {.theorem}
 Sei $D(f) \subseteq \mathbb{R}^{n}$ offen, und sei $f\colon D(f) \rightarrow \mathbb{R}$ stetig partiell differenzierbar. Besitzt $f$ in $\mathbf{x}^{*} \in D(f)$ ein lokales Extremum[^k&ouml;nnte auch ein Maximum sein, was wir aber einfach als ein Minimum von $-f$ einordnen], dann ist $\nabla f(\mathbf{x}^{*})=0$.
:::

::: {.JHSAYS data-latex=''}
Das k&ouml;nnen wir wie in der einfachen Kurvendiskussion verstehen: *Liegt in $x^*$ ein Extremum vor und $x^*$ liegt nicht auf dem Rand, so verschwindet die erste Ableitung.* Um festzustellen, ob ein Minimum, Maximum und vor allem nicht einfach ein Sattelpunkt vorliegt, wurde geschaut ob die zweite Ableitung positiv, negativ oder gleich 0 ist. Bei multivariablen Funktionen wird die zweite Ableitung durch eine Matrix repr&auml;sentiert. Und positiv, negativ oder indefinit wird wird &uuml;ber die Eigenwerte definiert.
:::

::: {.definition name="Hesse-Matrix"}
 Sei $D(f) \subseteq \mathbb{R}^{n}$ offen, und sei $f\colon D(f) \rightarrow \mathbb{R}$ eine 2-mal stetig partiell differenzierbare Funktion. Für $\mathbf{x}^{\star} \in D$ heißt die symmetrische $(n \times n)$-Matrix
$$
H_{f}\left(\mathbf{x}^{\star}\right)=\left(\begin{array}{cccc}
\frac{\partial^{2} f}{\partial x_{1} \partial x_{1}}\left(\mathbf{x}^{\star}\right) & \frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}\left(\mathbf{x}^{\star}\right) & \cdots & \frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}\left(\mathbf{x}^{\star}\right) \\
\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}\left(\mathbf{x}^{\star}\right) & \frac{\partial^{2} f}{\partial x_{2} \partial x_{2}}\left(\mathbf{x}^{\star}\right) & \cdots & \frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}\left(\mathbf{x}^{\star}\right) \\
\vdots & \vdots & \cdots & \vdots \\
\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}\left(\mathbf{x}^{\star}\right) & \frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}\left(\mathbf{x}^{\star}\right) & \cdots & \frac{\partial^{2} f}{\partial x_{n} \partial x_{n}}\left(\mathbf{x}^{\star}\right)
\end{array}\right)
$$
Hesse-Matrix von $f$ in $\mathbf{x}^{\star}$.
:::

Hierbei ist $\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}:=\frac{\partial }{\partial x_{i}}(\frac{\partial f}{\partial x_{j}})$ die wiederholte partielle Differentiation in m&ouml;glicherweise verschiedene Koordinatenrichtungen. Die postulierte Symmetrie der Hesse-Matrix folgt aus der Gleichheit
\begin{equation*}
\frac{\partial }{\partial x_{i}}(\frac{\partial f}{\partial x_{j}})
=
\frac{\partial }{\partial x_{j}}(\frac{\partial f}{\partial x_{i}})
\end{equation*}
deren G&uuml;ltigkeit wir noch in der Mathematik Vorlesung beweisen werden. Als n&auml;chstes ein Hinreichendes Kriterium f&uuml;r Minimalit&auml;t [vgl. Theorem 2.4, @NocW06]:

::: {.theorem name="Hinreichende Optimalit&auml;tsbedingung zweiter Ordnung"}
Sei $D(f) \subset \mathbb{R}^{n}$ offen, und sei $f\colon D(f) \rightarrow \mathbb{R}$ eine 2-mal stetig partiell differenzierbare Funktion. Sei $x^{\star} \in D(f)$ mit $\nabla f ({x}^{\star})={0}$ 
Ist $H_{f}({x}^{\star})$ positiv definit, dann ist $\mathbf{x}^{\star}$ ein lokales Minimum.
:::

Ein paar Bemerkungen dazu

 * Eine symmetrische Matrix $M$ hei&szlig;t *positiv definit*, wenn $x^TMx>0$ ist f&uuml;r alle $x\in \mathbb R^{n}\setminus \{0\}$. Wir schreiben $M \prec 0$.

 * Hinreichende Kriterien daf&uuml;r sind
   * alle Eigenwerte sind positiv oder
   * alle Hauptminoren sind positiv (das sind die Determinante der Matrizen, die aus $M$ durch Streichung der letzten $r$ Zeilen und Spalten entstehen $r=0,1,\dotsc, n-1$.)

 * Ist $-M \prec 0$, dann liegt ein lokales Minimum von $-f$ vor (oder eben ein Maximum von $f$)

 * Gilt $M \preccurlyeq 0$ (also $M$ ist positiv semidefinit, das hei&szlig;t die Eigenwerte sind nur "$\geq 0$" bzw. es gilt nur $x^TMx\geq 0$), dann ist **keine** Aussage m&ouml;glich. 

 * Ist die Matrix bestimmt indefinit, also alle Eigenwerte "$\neq 0$" aber manche positiv und manche negativ, dann liegt ein Sattelpunkt vor (und sicher kein Extremum).

## Gradientenabstiegsverfahren

Aus dem Wissen, dass in einem Minimum $x^*$, der Gradient $\nabla f(x^*)$ verschwindet und dass in einem beliebigen Punkt $x^0$ (der kein Extremum ist), der negative Gradient $-\nabla f(x^0)$ in die Richtung des st&auml;rksten Abstiegs zeigt, ergibt sich der folgende Zusammenhang:

> F&uuml;r einen beliebigen Startpunkt $x^0$ gibt es eine Schrittweite $\alpha^0$, so dass
> $$f(x^1) := f(x^0 - \alpha^0 \nabla f(x^0)) < f(x_0) $$

Das hei&szlig;t in einem Startpunkt k&ouml;nnen wir einen kleinen Schritt in die Richtung des st&auml;rksten Abstiegs gehen und so die Funktion etwas minimieren. Das wiederholte Anwenden dieses Prinzips ergibt das Gradientenabstiegsverfahren,
$$f(x^{n+1}) := f(x^n - \alpha^n \nabla f(x^n)) < f(x_n) $$,
das die Funktion immer weiter minimiert bis entweder der Definitionsbereich verlassen ist oder ein kritischer Punkt mit $\nabla f(x^k)=0$ erreicht wurde. An diesem k&ouml;nnte nun die Hessematrix ausgewertet werden um festzustellen ob tats&auml;chlich ein Minimum vorliegt. 

 * Neben der Berechnung des Gradienten, die f&uuml;r gro&szlig;e Dimensionen sehr aufw&auml;ndig werden kann, ist die Bestimmung der Schrittweite $\alpha^k$ in jedem Schritt entscheidend. 
   * Dazu kann beispielsweise in jeder Iteration die skalare Funktion $\alpha \mapsto f(x^{k} - \alpha \nabla f(x^k))$ minimiert werden (*exact line search*)
   * Da das aber auch aufw&auml;ndig sein kann, wurden zahlreiche Approximationen an diese Zwischenoptimierung entwickelt (*inexact line search*).
   * Vgl. auch den Wikipedia Artikel zum [Gradientenverfahren#Bestimmen_der_Schrittweite](https://de.wikipedia.org/wiki/Gradientenverfahren#Bestimmen_der_Schrittweite)

 * F&uuml;r passend gew&auml;hlte Schrittweiten und wenn $f$ hinreichend glatt ist (der Gradient soll Lipshitz-stetig sein), l&auml;sst sich Konvergenz und sogar Konvergenzrate der Iteration zu einem station&auml;ren Punkt $x^*$ mit $\nabla f(x^*)=0$ bestimmen [vgl. Section 3.2 in @NocW06].

 * Um tats&auml;chlich die Konvergenz zu einem Minimum zu beweisen, wird beispielsweise vorausgesetzt, dass ein striktes lokales Minimum existiert (*lokale Konvexit&auml;t*) und dass die Hesse-Matrix stetig ist (also $f$ zweimal stetig partiell differenzierbar ist) [vgl. Section 3.2 in @NocW06].

 * Wir k&ouml;nnen uns leicht &uuml;berlegen, dass dieser Algorithmus immer zu einem lokalen Minimum konvergieren wird und das entscheidend von der Wahl des Startpunktes abh&auml;ngt.

## Extra: Nichtglatte Optimierung

Ist die Zielfunktion nicht differenzierbar (oder ist die Berechnung des Gradienten zu aufw&auml;ndig) kann auf Optimierungsverfahren zur&uuml;ckgegriffen werden, die keinen Gradienten ben&ouml;tigen. Die bekanntesten sind

 * \emph{Nelder-Mead} oder auch (*nonlinear Simplex*) Verfahren: Berechnet f&uuml;r $n+1$ St&uuml;tzpunkte (ein Simplex im $\mathbb R^{n}$ die Werte von $f$ und ersetzt den *schlechtesten* nach Kriterien die eine (vergleichsweise) schnelle Konvergenz zu einem lokalen Minimum garantieren k&ouml;nnen.
 * Sogenannte *generische* oder *genetische Algorithmen*: nach dem Prinzip der Variation (oder Mutation) und Selektion wird der Suchraum $D(f)$ in zuf&auml;lliger Weise aber mit einer gewissen Systematik abgesucht -- diese Algorithmen konvergieren (in Wahrscheinlichkeit) zu einem globalen Minimum sind aber vergleichsweise langsam bzw. rechenaufw&auml;ndig.

## Extra: Automatisches Differenzieren

Wir sehen, dass in der glatten Optimierung die Berechnung des Gradienten die wichtigste und gleichzeitig aufw&auml;ndigste Komponente ist. Kann der Gradient analytisch berechnet und als Funktion zur Verf&uuml;gung gestellt werden, ist dieses Problem elegant umgangen. In der Praxis sind die Probleme aber oft als eine mehrschichitige Verkn&uuml;pfung von Funktionen in einem Programmcode gegeben. Andererseits sind die kleinste Elemente dieser Funktion typischerweise Elementarfunktionen, die einfach und analytisch differenziert werden k&ouml;nnen.

Die Darstellung einer Funktion, zum Beispiel,
\begin{equation*}
f(x_1, x_2) = \|(x_1, x_2)\| = \sqrt{x_1^2 + x_2^2}
\end{equation*}
die als Koordinatenfunktion f&uuml;r $x_1$ dargestellt werden kann als
$$
\tilde f_1(x_1, z) = g_3(g_2(g_1(x_1); z)), \quad g_3(y) = \sqrt y, \quad g_2(y;z) = y + z^2, \quad g_1(y) = y^2
$$
und mittels Kettenregel die partielle Ableitung als Kombination elementarer Ableitungen berechnet als
\begin{equation*}
\frac{\partial f}{\partial x_1}(x_1,x_2) = g_3(g_2(g_1(x_1); x_2))'\cdot g_2(g_1(x_1); x_2)'\cdot g_1(x_1)'
\end{equation*}

macht sich *Automatisches Differenzieren* zu Nutze und kann zu einem Programmcode 

```{.py}
def ffun(x):
    # ..... alles was f machen soll
    return fx
```

einen Programmcode erzeugen, der `grad_ffun(x)` berechnet. Das ist der sogenannten Vorw&auml;rtsmodus des *Automatischen Differenzierens*.

Der sogenannte *R&uuml;ckw&auml;rtsmodus* ist effizienter und kann leicht ein Programme eingebaut werden und damit zu jeder Funktionsevalution, direkt den Gradienten an dieser Stelle mitliefern. 

::: {.JHSAYS data-latex=''}
Die systematische Verwendung des *Automatischen Differenzierens* ist einer der wesentlichsten Garanten f&uuml;r den Erfolg von *machine learning*, da AD das tr&auml;inieren von grossen Netzwerken &uuml;berhaupt erst m&ouml;glich macht.
:::

## Aufgaben

### Numerische Berechnung des Gradienten (P+T)

Berechnen sie n&auml;herungsweise den Gradienten der Beispielfunktion 
\begin{equation*}
f(x_1, x_2, x_3) = \sin(x_1) + x_3\cos(x_2) + - 2x_2 + x_1^2 + x_2^2 + x_3^2
\end{equation*}
(**Achtung** dieses $f$ ist nur fast das $g$ wie oben)
im Punkt $(x_1, x_2, x_3) = (1, 1, 1)$, 
indem sie die partiellen Ableitungen durch den Differenzenquotienten, z.B.,
\begin{equation*}
\frac{\partial g}{\partial x_2}(1, 1, 1) \approx \frac{g(1, 1+h, 1) - g(1, 1,1)}{h}
\end{equation*}
f&uuml;r $h\in\{10^{-3}, 10^{-6}, 10^{-9}, 10^{-12}\}$ berechnen. Berechnen sie auch die Norm der Differenz zum exakten Wert von $\nabla g(1, 1, 1)$ (s.o.) und interpretieren sie die Ergebnisse.

```{.py}
import numpy as np
 
def gfun(x):
    return np.sin(x[0]) + x[2]*np.cos(x[1]) \
        - 2*x[1] + x[0]**2 + x[1]**2 \
        + x[2]**2

def gradg(x):
    return np.array([np.NaN,
                     -x[2]*np.sin(x[1]) - 2 + 2*x[1],
                     np.NaN]).reshape((3, 1))


# Inkrement
h = 1e-3

# der x-wert und das h-Inkrement in der zweiten Komponente
xzero = np.ones((3, 1))
xzeroh = xzero + np.array([0, h, 0]).reshape((3, 1))

# partieller Differenzenquotient
dgdxtwo = 1/h*(gfun(xzeroh) - gfun(xzero))
# Alle partiellen Ableitungen ergeben den Gradienten
hgrad = np.array([np.NaN, dgdxtwo, np.NaN]).reshape((3, 1))

# Analytisch bestimmter Gradient
gradx = gradg(xzero)

# Die Differenz in der Norm
hdiff = np.linalg.norm((hgrad-gradx)[1])
# bitte alle Kompenenten berechnen
# und dann die Norm ueber den ganzen Vektor nehmen

print(f'h={h:.2e}: diff in gradient {hdiff.flatten()[0]:.2e}')
```

### Optimierung ohne Gradienten (P+T)

Werten sie $g$ an $1000$ zuf&auml;llig gew&auml;hlten Punkten aus und lokalisieren sie das $\bar x \in \mathbb R^{3}$ an welchem $g$ in diesem Sample den kleinsten Wert annimmt.

&Uuml;berlegen Sie sich eine m&ouml;gliche Verbesserung dieses rein auf Zufall basierenden Verfahrens. Wer Inspiration braucht, kann mal nach *Generischen Algorithmen* suchen.

### Gradientenabstiegsverfahren (P)

Implementieren Sie ein Gradientenabstiegsverfahren mit *line search* durch Bisektion und berechnen Sie ein Minimum von $g$ damit.

```{.py}
import numpy as np

from loesung_5_1 import gfun, gradg


def min_by_btls(func, v=None, curx=None,
                azero=10., tau=.8, c=.9, maxiter=200):
    ''' find a smaller value of f in the direction

    f(x+s*v)

    for s in [0, azero]
    by backtracking linesearch

    https://en.wikipedia.org/wiki/Backtracking_line_search
    '''

    m = -v.T.dot(v)
    cstep = azero
    t = -c*m

    cval = func(curx)

    for kkk in range(maxiter):
        newx = curx+cstep*v
        if cval - func(newx) >= cstep*t:
            return newx
        else:
            cstep = tau*cstep
    raise UserWarning('no smaller value could be found')
    return


def gradienten_abstieg_linesearch(func=None, grad=None, inix=None,
                                  maxiter=100, grad_tol=1e-6):
    '''sucht ein Minimum einer Funktion `func` durch
     * Evaluieren des Gradienten `grad`
     * und anpassen von `inix` in Richtung von `-grad`
     * mit einem `line search Verfahren`
    '''

    # initialisierung
    curx = inix
    for kkk in range(maxiter):
        # compute the current gradient
        # finde ein neues `newx` in -Gradienten Richtung
        # check ob der Gradient unter der Toleranz ist
        # anderenfalls:
        curx = newx  # naechster Iterationschritt

    print(f'max iter {maxiter} reached -- size of grad {nrmcgrd:.2e}')
    return newx

# Startwert
xzero = np.ones((3, 1))

minx = gradienten_abstieg_linesearch(func=gfun, grad=gradg, inix=xzero)

print(f'found minimum f(x) = {gfun(minx)}')
print('at x = ', minx)
```

### Built-in Optimierung in Scipy (P)

1. Benutzen sie die [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) Routine um ein Minimum von $g$ zu finden und messen Sie die Zeit die der Algorithmus braucht.

2. Geben Sie dem Algorithmus eine Funktion mit, die den Gradienten berechnet. Berechnen sie ein Minimum und vergleichen Sie die ben&ouml;tigte Zeit. **Hinweis**: die Routine braucht den Gradienten als *flachen* Vektor. Das kann (wie unten im Beispiel) durch eine *wrapper* Funktion erreicht werden.

```{.py}
import numpy as np
from timeit import default_timer

from scipy.optimize import minimize

from loesung_5_1 import gfun, gradg


def gradforopt(x):
    return gradg(x).flatten()


starttime = default_timer()
result = minimize(...., jac=gradforopt)
runtime = default_timer() - starttime

print(f'with gradient: found minimum f(x) = {result.fun}')
print('at x = ', result.x)
print(f'in {runtime:.4f}s time')
```
