<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Optimierung | Einführung in die mathematische Datenanalyse</title>
  <meta name="description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Optimierung | Einführung in die mathematische Datenanalyse" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  <meta name="github-repo" content="highlando/script-emds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Optimierung | Einführung in die mathematische Datenanalyse" />
  
  <meta name="twitter:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  

<meta name="author" content="Jan Heiland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clustering-und-hauptkomponentenanalyse.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">EMDS</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html"><i class="fa fa-check"></i><b>1</b> Was ist Data Science?</a><ul>
<li class="chapter" data-level="1.1" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#wie-passiert-die-datenanalyse"><i class="fa fa-check"></i><b>1.1</b> Wie passiert die Datenanalyse?</a></li>
<li class="chapter" data-level="1.2" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#was-sind-daten"><i class="fa fa-check"></i><b>1.2</b> Was sind Daten?</a></li>
<li class="chapter" data-level="1.3" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#beispiele"><i class="fa fa-check"></i><b>1.3</b> Beispiele</a></li>
<li class="chapter" data-level="1.4" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#python"><i class="fa fa-check"></i><b>1.4</b> Python</a></li>
<li class="chapter" data-level="1.5" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#aufgaben"><i class="fa fa-check"></i><b>1.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html"><i class="fa fa-check"></i><b>2</b> Lineare Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#rauschen-und-fitting"><i class="fa fa-check"></i><b>2.1</b> Rauschen und Fitting</a></li>
<li class="chapter" data-level="2.2" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#ansätze-für-lineare-regression"><i class="fa fa-check"></i><b>2.2</b> Ansätze für lineare Regression</a></li>
<li class="chapter" data-level="2.3" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#sec-linreg-minimierung"><i class="fa fa-check"></i><b>2.3</b> Fehlerfunktional und Minimierung</a></li>
<li class="chapter" data-level="2.4" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#berechnung-der-bestlösung"><i class="fa fa-check"></i><b>2.4</b> Berechnung der Bestlösung</a></li>
<li class="chapter" data-level="2.5" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#beispiel"><i class="fa fa-check"></i><b>2.5</b> Beispiel</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html"><i class="fa fa-check"></i><b>3</b> Matrix-Zerlegungen</a><ul>
<li class="chapter" data-level="3.1" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#qr-zerlegung"><i class="fa fa-check"></i><b>3.1</b> QR Zerlegung</a></li>
<li class="chapter" data-level="3.2" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#singulärwertzerlegung"><i class="fa fa-check"></i><b>3.2</b> Singulärwertzerlegung</a></li>
<li class="chapter" data-level="3.3" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#aufgaben-1"><i class="fa fa-check"></i><b>3.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html"><i class="fa fa-check"></i><b>4</b> Hauptkomponentenanalyse</a><ul>
<li class="chapter" data-level="4.1" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#variationskoeffizienten"><i class="fa fa-check"></i><b>4.1</b> Variationskoeffizienten</a></li>
<li class="chapter" data-level="4.2" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#koordinatenwechsel"><i class="fa fa-check"></i><b>4.2</b> Koordinatenwechsel</a></li>
<li class="chapter" data-level="4.3" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#sec-PCA-maximierung"><i class="fa fa-check"></i><b>4.3</b> Maximierung der Varianz in (Haupt)-Achsenrichtung</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html"><i class="fa fa-check"></i><b>5</b> Hauptkomponentenanalyse Ctd.</a><ul>
<li class="chapter" data-level="5.1" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#der-penguins-datensatz"><i class="fa fa-check"></i><b>5.1</b> Der PENGUINS Datensatz</a></li>
<li class="chapter" data-level="5.2" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#darstellung"><i class="fa fa-check"></i><b>5.2</b> Darstellung</a></li>
<li class="chapter" data-level="5.3" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#korrelationen-und-die-kovarianzmatrix"><i class="fa fa-check"></i><b>5.3</b> Korrelationen und die Kovarianzmatrix</a></li>
<li class="chapter" data-level="5.4" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#hauptachsentransformation"><i class="fa fa-check"></i><b>5.4</b> Hauptachsentransformation</a></li>
<li class="chapter" data-level="5.5" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#rekonstruktion"><i class="fa fa-check"></i><b>5.5</b> Rekonstruktion</a></li>
<li class="chapter" data-level="5.6" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#reduktion-der-daten"><i class="fa fa-check"></i><b>5.6</b> Reduktion der Daten</a></li>
<li class="chapter" data-level="5.7" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#am-beispiel-der-pinguin-daten"><i class="fa fa-check"></i><b>5.7</b> Am Beispiel der Pinguin Daten</a></li>
<li class="chapter" data-level="5.8" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#aufgaben-2"><i class="fa fa-check"></i><b>5.8</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html"><i class="fa fa-check"></i><b>6</b> Clustering und Hauptkomponentenanalyse</a><ul>
<li class="chapter" data-level="6.1" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#clustering-im-allgemeinen"><i class="fa fa-check"></i><b>6.1</b> Clustering im Allgemeinen</a></li>
<li class="chapter" data-level="6.2" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#k-means-clustering"><i class="fa fa-check"></i><b>6.2</b> K-means Clustering</a></li>
<li class="chapter" data-level="6.3" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#clustering-und-hauptkomponentenanalyse-1"><i class="fa fa-check"></i><b>6.3</b> Clustering und Hauptkomponentenanalyse</a></li>
<li class="chapter" data-level="6.4" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#training-und-testing"><i class="fa fa-check"></i><b>6.4</b> Training und Testing</a></li>
<li class="chapter" data-level="6.5" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#am-beispiel-der-pinguine"><i class="fa fa-check"></i><b>6.5</b> Am Beispiel der Pinguine</a></li>
<li class="chapter" data-level="6.6" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#aufgaben-3"><i class="fa fa-check"></i><b>6.6</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="optimierung.html"><a href="optimierung.html"><i class="fa fa-check"></i><b>7</b> Optimierung</a><ul>
<li class="chapter" data-level="7.1" data-path="optimierung.html"><a href="optimierung.html#multivariable-funktionen"><i class="fa fa-check"></i><b>7.1</b> Multivariable Funktionen</a></li>
<li class="chapter" data-level="7.2" data-path="optimierung.html"><a href="optimierung.html#partielle-ableitungen-und-der-gradient"><i class="fa fa-check"></i><b>7.2</b> Partielle Ableitungen und der Gradient</a></li>
<li class="chapter" data-level="7.3" data-path="optimierung.html"><a href="optimierung.html#richtungs-ableitung"><i class="fa fa-check"></i><b>7.3</b> Richtungs-Ableitung</a></li>
<li class="chapter" data-level="7.4" data-path="optimierung.html"><a href="optimierung.html#optimierung-1"><i class="fa fa-check"></i><b>7.4</b> Optimierung</a></li>
<li class="chapter" data-level="7.5" data-path="optimierung.html"><a href="optimierung.html#optimierung-unter-nebenbedingungen"><i class="fa fa-check"></i><b>7.5</b> Optimierung unter Nebenbedingungen</a></li>
<li class="chapter" data-level="7.6" data-path="optimierung.html"><a href="optimierung.html#whats-more"><i class="fa fa-check"></i><b>7.6</b> What’s more</a></li>
<li class="chapter" data-level="7.7" data-path="optimierung.html"><a href="optimierung.html#aufgaben-4"><i class="fa fa-check"></i><b>7.7</b> Aufgaben</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Einführung in die mathematische Datenanalyse</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimierung" class="section level1 hasAnchor">
<h1><span class="header-section-number">7</span> Optimierung<a href="optimierung.html#optimierung" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<!-- \DeclareMathOperator\diva{d} -->
<p>Einige Optimierungsprobleme haben wir schon kennengelernt und zwar</p>
<ul>
<li>bei der linearen Regression (vgl. besonders Kapitel <a href="sec-lineare-regression.html#sec-linreg-minimierung">2.3</a>) bei der die Parameter der Ansatzfunktion <strong>bestmöglich</strong> an die Daten gefittet wurden</li>
<li>bei der Hauptkomponentenanalyse (vgl. besonders Kapitel <a href="hauptkomponentenanalyse.html#sec-PCA-maximierung">4.3</a>) als wir die Hauptrichtungen so ermittelt, dass in ihrer Richtung die Varianz <strong>maximal</strong> wurde.</li>
</ul>
<p>Hier lag in beiden Fällen ein sogenanntes linear-quadratisches Optimierungsproblem vor, für welche die Existenz der Lösungen bewiesen werden kann und die mit Methoden aus der linearen Algebra direkt gelöst werden können.</p>
<p>Sogenannte <em>nichtlineare Optimierungsprobleme</em> liegen vor, wenn die zu erfüllenden Ziele wirklich nichtlineare Gleichungen enthalten. Zum Bespiel sind gängige Elemente von <em>Neuronalen Netzen</em> als
<span class="math display">\[\begin{equation*}
f(x) = \tanh(a\, x + b)
\end{equation*}\]</span>
gegeben, mit Parametern <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> die aus der Minimierung eines <em>mean square error</em> Terms der Form
<span class="math display">\[\begin{equation*}
\frac 1N \sum_{k=1}^N|y_k - \tanh(a\, x_k + b)|^2
\end{equation*}\]</span>
für gegebene Trainingsdaten <span class="math inline">\((x_k, y_k)\)</span> bestimmt werden.</p>
<p>Wir sehen, dass die Optimierung ein ganz wesentlicher Teil der <em>Data Science</em> ist (und ganz unabhängig davon ein eigenes Forschungsgebiet ist und quasi überall in der Praxis mittel– oder unmittelbar benutzt wird).</p>
<p>Nachfolgend ein paar Begriffe aus der Optimierung</p>
<ul>
<li><em>glatte Optimierung</em> – das Modell beinhaltet ausreichend stetige (bzw. differenzierbare) Funktionen. Ein Optimum könnte durch Bestimmung von Ableitungen berechnet werden.</li>
<li><em>nichtglatte Optimierung</em> – das Modell beinhaltet Teile, die nicht differenzierbar sind. Das kann am Problem selbst liegen oder, was bei <em>machine learning</em> im Speziellen und <em>Data Science</em> an sich eine Rolle spielt, dass die Daten verrauscht sind. Jetzt ist eine (klassische) Ableitung nicht mehr möglich aber es existieren diverse Ansätze zur Abhilfe.</li>
<li><em>diskrete Optimierung</em> – in obigen Fällen sind wir implizit davon ausgegangen, dass die Lösung und die Gleichungen auf den reellen oder komplexen Zahlen leben. Denkbar ist aber auch, dass diskrete Lösungen (z.B. Anzahlen oder binäre Zustände) gesucht werden.</li>
<li><em>kombinatorische Optimierung</em> – hier ergeben sich die möglichen Lösungen als Kombination von Möglichkeiten. Oft ist die Anzahl möglicher Lösungen endlich aber sehr groß.</li>
<li><em>restringierte Optimierung</em> – zusätzlich zu einem Optimalitätskriterium sind noch Nebenbedingungen zu erfüllen.</li>
</ul>
<p>Hier werden wir uns erstmal mit glatter Optimierung ohne Nebenbedingungen beschäftigen.</p>
<div id="multivariable-funktionen" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.1</span> Multivariable Funktionen<a href="optimierung.html#multivariable-funktionen" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Um unsere Optimierungsprobleme in allgemeiner Form zu formulieren und auch um allgemeine Lösungsansätze herzuleiten, brauchen wir erstmal ein paar Begriffe aus der Analysis von multivariablen Funktionen.</p>
<p>Es sei also eine Funktion
<span class="math display">\[\begin{equation*}
f\colon \mathbb R^{n}\to \mathbb R^{}, \quad f \colon (x_1,x_2, \dots, x_n) \mapsto f(x_1, x_2, \dotsc, x_n)
\end{equation*}\]</span>
gegeben, die ein <span class="math inline">\(n\)</span>-tupel von Variablen auf einen reellen Wert abbildet. Ein Beispiel wäre
<span class="math display">\[\begin{equation*}
g(x_1, x_2, x_3) = \sin(x_1) + x_3\cos(x_2) + x_1^2 x_2^2 x_3^2 + 2x_2.
\end{equation*}\]</span></p>
<div id="variablen-namen" class="JHSAYS">
<p>Hier verwenden wir jetzt <span class="math inline">\(x_i\)</span> für die <span class="math inline">\(i\)</span>-te Variable der multivariablen Funktion. Bitte nicht verwechseln mit der Bezeichnung für den <span class="math inline">\(i\)</span>-ten Datenpunkt zum Beispiel in Kapitel <a href="sec-lineare-regression.html#sec-lineare-regression">2</a>.</p>
<p>Außerdem wäre es besser wir würden die Funktion mit einem Definitionsbereich <span class="math inline">\(D(f)\)</span> angeben, also schreiben
<span class="math display">\[\begin{equation*}
f\colon D(f) \subset \mathbb R^{n}\to \mathbb R^{}
\end{equation*}\]</span>
da eine Funktion eventuell nicht überall definiert ist (bzw. definiert sein muss). Im Sinne der besseren Lesbarkeit, ersparen wir uns dieses Detail.</p>
</div>
</div>
<div id="partielle-ableitungen-und-der-gradient" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.2</span> Partielle Ableitungen und der Gradient<a href="optimierung.html#partielle-ableitungen-und-der-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Wenn wir alle Variablen bis auf die <span class="math inline">\(i\)</span>-te für einen Augenblick einfrieren (also auf einen konstanten Wert <span class="math inline">\(\bar x_j\)</span> festlegen und nicht verändern), können wir die Teilfunktion
<span class="math display">\[\begin{equation*}
\bar f_{(i)}\colon \mathbb R^{}\to \mathbb R^{}, \quad \bar f_{(i)}\colon x_i \to f(\bar x_1, \dotsc, \bar x_{i-1}, x_i, \bar x_{i+1}, \dotsc, \bar x_n)
\end{equation*}\]</span>
als Funktion mit einer Veränderlichen (und <span class="math inline">\(n-1\)</span> Parametern) betrachten und wie gehabt ableiten. Die erhaltene, sogenannte <em>partielle Ableitung</em> hängt von den Parametern <span class="math inline">\(\bar x_j\)</span> ab und wird mit
<span class="math display">\[\begin{equation*}
\frac{\partial f}{\partial x_i}
\end{equation*}\]</span>
bezeichnet. Alle <span class="math inline">\(n\)</span> möglichen partiellen Ableitungen in einen Vektor geschrieben ergeben den sogenannten <em>Gradienten</em>
<span class="math display">\[\begin{equation*}
\nabla f = 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
\colon \mathbb R^{n} \to \mathbb R^{n},
\end{equation*}\]</span>
der wiederum eine (vektorwertige) Funktion von <span class="math inline">\(n\)</span> Variablen ist.
Für unser obiges Beispiel bekommen wir
<span class="math display">\[\begin{equation*}
\nabla g(x_1, x_2, x_3) =
\begin{bmatrix}
\cos(x_1)+2x_1x_2^2x_3^2\\
-x_3\sin(x_2) + x_1^2x_2x_3^2 - 2 \\
\cos(x_2) + 2x_1^2x_2^2x_3
\end{bmatrix}
\end{equation*}\]</span></p>
</div>
<div id="richtungs-ableitung" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.3</span> Richtungs-Ableitung<a href="optimierung.html#richtungs-ableitung" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Die Betrachtung der Funktionen <span class="math inline">\(\bar f_{(i)}\)</span> ermöglicht die Analysis des Verhaltens der Funktion entlang der <span class="math inline">\(i\)</span>-ten Koordinaten Richtung. Um die Funktion in einer beliebigen Richtung um einen festen Punkt
<span class="math display">\[\begin{equation*}
\bar x = (\bar x_1, \bar x_2, \dotsc, \bar x_n)
\end{equation*}\]</span>
zu untersuchen, kann die Funktion entlang einer Richtung <span class="math inline">\(v \in \mathbb R^{n}\)</span> parametrisiert werden:
<span class="math display">\[\begin{equation*}
\bar f_{(v)} \colon \mathbb R^{} \to \mathbb R^{}, \quad t \mapsto \bar f_{(v)}(t) = f(\bar x + tv)
\end{equation*}\]</span></p>
<div id="richtung-is-partiell" class="JHSAYS">
<p>Für <span class="math inline">\(v=e_i\)</span> bekommen wir direkt, dass <span class="math inline">\(\bar f_{(v)}=\bar f_{(i)}\)</span>.</p>
</div>
<p>Wiederum handelt es sich um eine reelle Funktion in einer Variablen (hier der Parameter <span class="math inline">\(t\)</span>), die wir schulbuchmäßig ableiten könnten. Für diese sogenannte <em>Richtungsableitung</em> gilt der folgende Zusammenhang<a href="optimierung.html#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>
<span class="math display">\[\begin{equation*}
\frac{\mathrm{d} \bar f_{(v)}}{\mathrm{d} t}(0) = \sum_i^n \frac{\partial f}{\partial x_i}(\bar x_i) v_i = \langle \nabla f(\bar x), v \rangle
\end{equation*}\]</span></p>
<div id="richtung-ableitung" class="JHSAYS">
<p>Die Veränderungsrate der Funktion <span class="math inline">\(f\)</span> im Punkt <span class="math inline">\(\bar x\)</span> in der Richtung von <span class="math inline">\(v\)</span> ist gleich dem Skalarprodukt aus Gradient im Punkt <span class="math inline">\(\bar x\)</span> und der gewählten Richtung.</p>
</div>
<p>Sei nun <span class="math inline">\(v\)</span> normiert, also <span class="math inline">\(\|v\|=1\)</span>, dann gilt nach der Winkelformel, dass
<span class="math display">\[\begin{equation*}
\langle \nabla f(\bar x), v \rangle = \|\nabla f(\bar x) \| \cos (\alpha)
\end{equation*}\]</span>
wobei <span class="math inline">\(\alpha\)</span> der von <span class="math inline">\(\nabla f(\bar x)\)</span> und <span class="math inline">\(v\)</span> eingeschlossene Winkel ist. Wir bemerken, dass die Veränderung maximal wird, wenn <span class="math inline">\(\alpha = 0\)</span> oder
<span class="math display">\[\begin{equation*}
v = \frac{1}{ \| \nabla f(\bar x) \| } \nabla f(\bar x)
\end{equation*}\]</span>
als ein Vielfaches des Gradientenvektors gewählt wird.
Andersherum können wir einige ganz wesentliche Aspekte folgern:</p>
<ul>
<li>der Gradient selbst in die Richtung des stärksten Anstiegs</li>
<li>der negative Gradient <span class="math inline">\(-\nabla f(\bar x)\)</span> zeigt in die Richtung des stärksten Abfalls</li>
<li>ist der Gradient der Nullvektor, dann findet keine Veränderung mehr statt, d.h. dass aus <span class="math inline">\(\nabla f(\bar x) =0\)</span> folgt, dass <span class="math inline">\(\bar x\)</span> ein kritischer Punkt von <span class="math inline">\(f\)</span> ist</li>
<li>ist der Gradient nicht der Nullvektor, kann <span class="math inline">\(\bar x\)</span> kein Minimum und kein Maximum sein (weil die Funktion in <span class="math inline">\(-\nabla f(\bar x)\)</span> Richtung abfällt und in <span class="math inline">\(\nabla f(\bar x)\)</span> Richtung ansteigt)</li>
</ul>
</div>
<div id="optimierung-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.4</span> Optimierung<a href="optimierung.html#optimierung-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ganz allgemein können wir unsere bisherigen Optimierungsprobleme schreiben als
<span class="math display">\[\begin{equation*}
f(x) \to \min_{x\in \mathbb R^{n}}
\end{equation*}\]</span></p>
<ul>
<li>allgemeine Darstellung
<ul>
<li>multivariable Funktion</li>
</ul></li>
<li>Gradient/Hesse Matrix</li>
<li>Gradientenabstiegsverfahren
<ul>
<li>klassisch</li>
<li>gedaempft</li>
<li>stochastisch</li>
</ul></li>
</ul>
</div>
<div id="optimierung-unter-nebenbedingungen" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.5</span> Optimierung unter Nebenbedingungen<a href="optimierung.html#optimierung-unter-nebenbedingungen" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="whats-more" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.6</span> What’s more<a href="optimierung.html#whats-more" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Automatisches Differenzieren</li>
<li>Multikriterielle Optimierung</li>
</ul>
</div>
<div id="aufgaben-4" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.7</span> Aufgaben<a href="optimierung.html#aufgaben-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="numerische-berechnung-des-gradienten" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.7.1</span> Numerische Berechnung des Gradienten<a href="optimierung.html#numerische-berechnung-des-gradienten" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Berechnen sie näherungsweise den Gradienten unserer Beispielfunktion
<span class="math display">\[\begin{equation*}
g(x_1, x_2, x_3) = \sin(x_1) + x_3\cos(x_2) + x_1^2 x_2^2 x_3^2 - 2x_2
\end{equation*}\]</span>
im Punkt <span class="math inline">\((x_1, x_2, x_3) = (1, 1, 1)\)</span>,
indem sie die partiellen Ableitungen durch den Differenzenquotienten, z.B.,
<span class="math display">\[\begin{equation*}
\frac{\partial g}{\partial x_2}(1, 1, 1) \approx \frac{g(1, 1+h, 1) - g(1, 1,1)}{h}
\end{equation*}\]</span>
für <span class="math inline">\(h\in\{10^{-3}, 10^{-6}, 10^{-9}, 10^{-12}\}\)</span> berechnen. Berechnen sie auch die Norm der Differenz zum exakten Wert von <span class="math inline">\(\nabla g(1, 1, 1)\)</span> (s.o.) und interpretieren sie die Ergebnisse.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode py"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" title="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb18-2" title="2"></a>
<a class="sourceLine" id="cb18-3" title="3"></a>
<a class="sourceLine" id="cb18-4" title="4"><span class="kw">def</span> gfun(x):</a>
<a class="sourceLine" id="cb18-5" title="5">    <span class="cf">return</span> np.sin(x[<span class="dv">0</span>]) <span class="op">+</span> x[<span class="dv">2</span>]<span class="op">*</span>np.cos(x[<span class="dv">1</span>]) <span class="op">\</span></a>
<a class="sourceLine" id="cb18-6" title="6">        <span class="op">+</span> x[<span class="dv">0</span>]<span class="op">*</span>x[<span class="dv">0</span>]<span class="op">*</span>x[<span class="dv">1</span>]<span class="op">*</span>x[<span class="dv">1</span>]<span class="op">*</span>x[<span class="dv">2</span>]<span class="op">*</span>x[<span class="dv">2</span>] <span class="op">\</span></a>
<a class="sourceLine" id="cb18-7" title="7">        <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>x[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb18-8" title="8"></a>
<a class="sourceLine" id="cb18-9" title="9"></a>
<a class="sourceLine" id="cb18-10" title="10"><span class="kw">def</span> gradg(x):</a>
<a class="sourceLine" id="cb18-11" title="11">    <span class="cf">return</span> np.array([np.NaN,  <span class="co"># bitte hier vervollstaendigen</span></a>
<a class="sourceLine" id="cb18-12" title="12">                     <span class="op">-</span>x[<span class="dv">2</span>]<span class="op">*</span>np.sin(x[<span class="dv">1</span>])<span class="op">+</span>x[<span class="dv">0</span>]<span class="op">*</span>x[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">*</span>x[<span class="dv">2</span>]<span class="op">*</span>x[<span class="dv">2</span>]<span class="op">-</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb18-13" title="13">                     np.NaN]).reshape((<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb18-14" title="14"></a>
<a class="sourceLine" id="cb18-15" title="15"></a>
<a class="sourceLine" id="cb18-16" title="16"><span class="co"># Inkrement</span></a>
<a class="sourceLine" id="cb18-17" title="17">h <span class="op">=</span> <span class="fl">1e-3</span></a>
<a class="sourceLine" id="cb18-18" title="18"></a>
<a class="sourceLine" id="cb18-19" title="19"><span class="co"># der x-wert und das h-Inkrement in der zweiten Komponente</span></a>
<a class="sourceLine" id="cb18-20" title="20">xzero <span class="op">=</span> np.ones((<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb18-21" title="21">xzeroh <span class="op">=</span> xzero <span class="op">+</span> np.array([<span class="dv">0</span>, h, <span class="dv">0</span>]).reshape((<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb18-22" title="22"></a>
<a class="sourceLine" id="cb18-23" title="23"><span class="co"># partieller Differenzenquotient</span></a>
<a class="sourceLine" id="cb18-24" title="24">dgdxtwo <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>h<span class="op">*</span>(gfun(xzeroh) <span class="op">-</span> gfun(xzero))</a>
<a class="sourceLine" id="cb18-25" title="25"><span class="co"># Alle partiellen Ableitungen ergeben den Gradienten</span></a>
<a class="sourceLine" id="cb18-26" title="26">hgrad <span class="op">=</span> np.array([np.NaN, dgdxtwo, np.NaN]).reshape((<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb18-27" title="27"></a>
<a class="sourceLine" id="cb18-28" title="28"><span class="co"># Analytisch bestimmter Gradient</span></a>
<a class="sourceLine" id="cb18-29" title="29">gradx <span class="op">=</span> gradg(xzero)</a>
<a class="sourceLine" id="cb18-30" title="30"></a>
<a class="sourceLine" id="cb18-31" title="31"><span class="co"># Die Differenz in der Norm</span></a>
<a class="sourceLine" id="cb18-32" title="32">hdiff <span class="op">=</span> np.linalg.norm((hgrad<span class="op">-</span>gradx)[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb18-33" title="33"><span class="co"># bitte alle Kompenenten berechnen</span></a>
<a class="sourceLine" id="cb18-34" title="34"><span class="co"># und dann die Norm ueber den ganzen Vektor nehmen</span></a>
<a class="sourceLine" id="cb18-35" title="35"></a>
<a class="sourceLine" id="cb18-36" title="36"><span class="bu">print</span>(<span class="ss">f&#39;h=</span><span class="sc">{h:.2e}</span><span class="ss">: diff in gradient </span><span class="sc">{</span>hdiff<span class="sc">.</span>flatten()[<span class="dv">0</span>]<span class="sc">:.2e}</span><span class="ss">&#39;</span>)</a></code></pre></div>
</div>
<div id="optimierung-ohne-gradienten-p" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.7.2</span> Optimierung ohne Gradienten (P)<a href="optimierung.html#optimierung-ohne-gradienten-p" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Werten sie <span class="math inline">\(g\)</span> an <span class="math inline">\(1000\)</span> zufällig gewählten Punkten aus und lokalisieren sie das <span class="math inline">\(\bar x \in \mathbb R^{3}\)</span> an welchem <span class="math inline">\(g\)</span> in diesem Sample den kleinsten Wert annimmt.</p>
<p>Überlegen Sie sich eine mögliche Verbesserung dieses rein auf Zufall basierenden Verfahrens.</p>
</div>
<div id="gradientenabstiegsverfahren-p" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.7.3</span> Gradientenabstiegsverfahren (P)<a href="optimierung.html#gradientenabstiegsverfahren-p" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Implementieren Sie ein Gradientenabstiegsverfahren mit <em>line search</em> durch Bisektion und berechnen Sie ein Minimum von <span class="math inline">\(g\)</span> damit.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode py"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" title="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb19-2" title="2"></a>
<a class="sourceLine" id="cb19-3" title="3"><span class="im">from</span> loesung_5_1 <span class="im">import</span> gfun, gradg</a>
<a class="sourceLine" id="cb19-4" title="4"></a>
<a class="sourceLine" id="cb19-5" title="5"></a>
<a class="sourceLine" id="cb19-6" title="6"><span class="kw">def</span> min_by_bisec(func, v<span class="op">=</span><span class="va">None</span>, curx<span class="op">=</span><span class="va">None</span>, stepsize<span class="op">=</span><span class="fl">1.</span>, maxiter<span class="op">=</span><span class="dv">15</span>):</a>
<a class="sourceLine" id="cb19-7" title="7">    <span class="co">&#39;&#39;&#39; find a smaller value of f in the direction</span></a>
<a class="sourceLine" id="cb19-8" title="8"></a>
<a class="sourceLine" id="cb19-9" title="9"><span class="co">    f(x+s*v)</span></a>
<a class="sourceLine" id="cb19-10" title="10"></a>
<a class="sourceLine" id="cb19-11" title="11"><span class="co">    for s in [0, stepsize]</span></a>
<a class="sourceLine" id="cb19-12" title="12"></a>
<a class="sourceLine" id="cb19-13" title="13"><span class="co">    by bisection</span></a>
<a class="sourceLine" id="cb19-14" title="14"><span class="co">    &#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb19-15" title="15">    cstep <span class="op">=</span> stepsize</a>
<a class="sourceLine" id="cb19-16" title="16"></a>
<a class="sourceLine" id="cb19-17" title="17">    cval <span class="op">=</span> func(curx)</a>
<a class="sourceLine" id="cb19-18" title="18">    nval <span class="op">=</span> func(curx<span class="op">+</span>cstep<span class="op">*</span>v)</a>
<a class="sourceLine" id="cb19-19" title="19"></a>
<a class="sourceLine" id="cb19-20" title="20">    <span class="cf">for</span> kkk <span class="kw">in</span> maxiter:</a>
<a class="sourceLine" id="cb19-21" title="21">        <span class="cf">if</span> nval <span class="op">&gt;</span> cval:</a>
<a class="sourceLine" id="cb19-22" title="22">            cstep <span class="op">=</span> <span class="fl">.5</span><span class="op">*</span>cstep</a>
<a class="sourceLine" id="cb19-23" title="23">            newx <span class="op">=</span> curx<span class="op">+</span>cstep<span class="op">*</span>v</a>
<a class="sourceLine" id="cb19-24" title="24">            nval <span class="op">=</span> func(newx)</a>
<a class="sourceLine" id="cb19-25" title="25">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb19-26" title="26">            <span class="cf">return</span> newx</a>
<a class="sourceLine" id="cb19-27" title="27">    <span class="cf">raise</span> <span class="pp">UserWarning</span>(<span class="st">&#39;no smaller value could be found&#39;</span>)</a>
<a class="sourceLine" id="cb19-28" title="28">    <span class="cf">return</span></a>
<a class="sourceLine" id="cb19-29" title="29"></a>
<a class="sourceLine" id="cb19-30" title="30"></a>
<a class="sourceLine" id="cb19-31" title="31"></a>
<a class="sourceLine" id="cb19-32" title="32"><span class="kw">def</span> gradienten_abstieg_linesearch(func<span class="op">=</span><span class="va">None</span>, grad<span class="op">=</span><span class="va">None</span>, inix<span class="op">=</span><span class="va">None</span>,</a>
<a class="sourceLine" id="cb19-33" title="33">                                  maxiter<span class="op">=</span><span class="dv">10</span>):</a>
<a class="sourceLine" id="cb19-34" title="34">    <span class="co">&#39;&#39;&#39;sucht ein Minimum einer Funktion `func` durch</span></a>
<a class="sourceLine" id="cb19-35" title="35"><span class="co">     * Evaluieren des Gradienten `grad`</span></a>
<a class="sourceLine" id="cb19-36" title="36"><span class="co">     * und anpassen von `inix` in Richtung von `-grad`</span></a>
<a class="sourceLine" id="cb19-37" title="37"><span class="co">     * mit einem `line search Verfahren`</span></a>
<a class="sourceLine" id="cb19-38" title="38"><span class="co">    &#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb19-39" title="39"></a>
<a class="sourceLine" id="cb19-40" title="40">    curx <span class="op">=</span> inix</a>
<a class="sourceLine" id="cb19-41" title="41">    cgrad <span class="op">=</span> grad(curx)</a>
<a class="sourceLine" id="cb19-42" title="42">    nxtx <span class="op">=</span> min_by_bisec(func, v<span class="op">=-</span>cgrad, curx<span class="op">=</span>curx)</a>
<a class="sourceLine" id="cb19-43" title="43"></a>
<a class="sourceLine" id="cb19-44" title="44">    <span class="cf">return</span> nxtx</a>
<a class="sourceLine" id="cb19-45" title="45"></a>
<a class="sourceLine" id="cb19-46" title="46">xzero <span class="op">=</span> np.ones((<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb19-47" title="47"></a>
<a class="sourceLine" id="cb19-48" title="48">minx <span class="op">=</span> gradienten_abstieg_linesearch(func<span class="op">=</span>gfun, grad<span class="op">=</span>gradg, inix<span class="op">=</span>xzero)</a></code></pre></div>
</div>
<div id="built-in-optimierung-in-scipy-p" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.7.4</span> Built-in Optimierung in Scipy (P)<a href="optimierung.html#built-in-optimierung-in-scipy-p" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>Benutzen sie die <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize"><code>scipy.optimize.minimize</code></a> Routine um ein Minimum von <span class="math inline">\(g\)</span> zu finden und messen Sie die Zeit die der Algorithmus braucht.</p></li>
<li><p>Geben Sie dem Algorithmus eine Funktion mit, die den Gradienten berechnet. Berechnen sie ein Minimum und vergleichen Sie die benötigte Zeit.</p></li>
</ol>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Auf Matrixnormen kommen wir noch in der Vorlesung zu sprechen.<a href="matrix-zerlegungen.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p> <a href="https://owncloud.gwdg.de/index.php/s/sAjEy9B8kIbzoYj">Download bitte hier</a> – Achtung das sind 370MB<a href="matrix-zerlegungen.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>allerdings mit 2 unvollständigen Datenpunkten, die ich entfernt habe für unseere Beispiele<a href="hauptkomponentenanalyse-ctd..html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Im Originaldatensatz ist das Gewicht in Gramm angegeben, um die Daten innerhalb einer 10er Skala zu haben, habe ich das Gewicht auf in kg umgerechnet<a href="hauptkomponentenanalyse-ctd..html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>wir dürfen aber nicht vergessen, dass Daten typischerweise nur eine Stichprobe von Beobachtungen eines Phänomens sind. Die Unabhängigkeit in den <em>features</em> gilt also nur für die gesammelten Daten aber in der Regel nicht für das Phänomen. Für normalverteilte Prozesse liefern die daten-basiert ermittelten Hauptrichtungen jedoch auch die Hauptrichtungen des zugrundeliegenden Phänomens<a href="hauptkomponentenanalyse-ctd..html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>wenn wir Inhalt mit Varianz gleich setzen<a href="clustering-und-hauptkomponentenanalyse.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Im <em>machine learning</em> wird gerne von <em>generalization</em> gesprochen<a href="clustering-und-hauptkomponentenanalyse.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>Das folgt aus der Kettenregel für multivariable Funktionen, die wir in der Vorlesung <em>Mathematik für Data Science 2</em> noch beweisen werden<a href="optimierung.html#fnref8" class="footnote-back">↩</a></p></li>
</ol>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>Das folgt aus der Kettenregel für multivariable Funktionen, die wir in der Vorlesung <em>Mathematik für Data Science 2</em> noch beweisen werden<a href="optimierung.html#fnref8" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering-und-hauptkomponentenanalyse.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["EMDS.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
