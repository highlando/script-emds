# Lineare Regression

Ein wesentlicher Aspekt von *Data Science* ist die Analyse oder das
Verstehen von Daten. Allgemein gesagt, es wird versucht, aus den Daten
heraus Aussagen über Trends oder Eigenschaften des Phänomens zu treffen,
mit welchem die Daten im Zusammenhang stehen.

Wir kommen nochmal auf das Beispiel aus der Einf&uuml;hrungswoche zur&uuml;ck, werfen eine bereits gesch&auml;rften Blick darauf und gehen das mit verbesserten mathematischen Methoden an.

Gegeben seien die Fallzahlen aus der CoVID Pandemie 2020 f&uuml;r Bayern f&uuml;r den Oktober 2020.

::: footnotesize
::: {#tab:covid-cases}
  Tag        1      2      3      4      5      6      7      8      9      10     11
  ---------- ------ ------ ------ ------ ------ ------ ------ ------ ------ ------ ------
  Fälle      352    347    308    151    360    498    664    686    740    418    320

  : Anzahl der SARS-CoV-2 Neuinfektionen in Bayern im Oktober 2020.
:::

  Tag        12     13     14      15      16     17     18     19      20      21
  ---------- ------ ------ ------- ------- ------ ------ ------ ------- ------- -------
  Fälle      681    691    1154    1284    127    984    573    1078    1462    2239

  Tag        22      23      24      25      26      27      28      29      30      31
  ---------- ------- ------- ------- ------- ------- ------- ------- ------- ------- -------
  Fälle      2236    2119    1663    1413    2283    2717    3113    2972    3136    2615
                                                                                     
:::

![Fallzahlen von Sars-CoV-2 in Bayern im Oktober
2020](bilder/02-cases.png){#fig:cases width=".8\\textwidth"}

Wieder stellen wir uns die Frage ob wir **in den Daten einen funktionalen
Zusammenhang** feststellen können. Also ob wir die Datenpaare

> (Tag $x$, Infektionen am Tag $x$)

die wir als 

> ($x_i$, $y_i$)


über eine Funktion $f$ und die Paare

> ($x$, $f(x)$)

beschreiben (im Sinne von gut darstellen oder approximieren) können.

## Rauschen und Fitting

Beim obigen Beispiel (und ganz generell bei Daten) ist davon auszugehen, dass die Daten **verrauscht** sind, also einem Trend folgen oder in einem funktionalen Zusammenhang stehen aber zuf&auml;llige Abweichungen oder Fehler enthalten.

Unter diesem Gesichtspunkt ist eine Funktion, die 

> $f(x_i)=y_i$

nicht zielführend. (Wir wollen Trends und gr&ouml;&szlig;ere Zusammenhänge erkennen und nicht kleine Fehler nachzeichnen.) Das zu strenge Anpassen an m&ouml;glicherweise verrauschte Daten wird **overfitting** genannt.

Vielmehr werden wir nach einer Funktion $f$ suchen, die die Daten n&auml;herungsweise nachstellt: 

> $f(x_i)\approx y_i$

Hierbei passen jetzt auch Funktionen, die vielleicht einfach zu handhaben sind aber die Daten kaum noch repr&auml;sentieren. Jan spricht von **underfitting**.

Eine gute Approximation besteht im Kompromiss von *nah an den Daten* aber mit wenig *overfitting*.

## Ans&auml;tze f&uuml;r lineare Regression

Um eine solche Funktion $f$ zu finden, trifft Jan als erstes ein paar
Modellannahmen. Modellannahmen legen fest, wie das $f$ im Allgemeinen
aussehen soll und versuchen dabei

1.  die Bestimmung von $f$ zu ermöglichen

2.  zu garantieren, dass $f$ auch die gewollten Aussagen liefert

3.  und sicherzustellen, dass $f$ zum Problem passt.

Jan bemerke, dass die ersten beiden Annahmen im Spannungsverhältnis zur
dritten stehen.

**Lineare Regression** besteht darin, dass die Funktion als Linearkombination

\begin{equation*}
f(x) = \sum_{i=1}^N w_i b_i(x)
\end{equation*}

von Basisfunktionen geschrieben wird und dann die *Koeffizienten* $w_i$ so bestimmt werden, dass $f$ die Daten bestmöglich ann&auml;hert. 

> Jan bemerke, dass *bestmöglich* wieder *overfitting* bedeuten kann aber auch, bei schlechter Wahl der Basis, wenig aussagekr&auml;ftig sein kann. Der gute Kompromiss liegt also jetzt in der Wahl der passenden Basisfunktionen und deren Anzahl. (Mehr Basisfunktionen bedeutet m&ouml;glicherweise bessere Approximation aber auch die Gefahr von *overfitting*.)

Typische Wahlen f&uuml;r die Basis $\{b_1, b_2, \dotsc, b_N\}$ sind

 * Polynome: $\{1, x, x^2, \dotsc, x^{N-1}\}$ -- f&uuml;r $N=2$ ist der Ansatz *eine Gerade*
 * Trigonometrische Funktionen: $\{1, \cos(x), \sin(x), \cos(2x), \sin(2x), \dotsc\}$
 * Splines -- Polynome, die abschnittsweise definiert werden
 * Wavelets -- Verallgemeinerungen von trigonometrischen Funktionen

## Fehlerfunktional und Berechnung der Approximierenden

Wir setzen nun also an
\begin{equation*}
f_w(x) = \sum_{i=1}^Nw_i b_i (x)
\end{equation*}
und wollen damit $y_i \approx f_w(x_i)$ *bestmöglich* erreichen (indem wir die Koeffizienten $(w_1, \dotsc, w_N)$ *optimal* w&auml;hlen. Bestmöglich und optimal spezifizieren wir &uuml;ber den Mittelwert der Abweichungen in der Approximation &uuml;ber alle Datenpunkte
\begin{equation*}
\frac{1}{N}
\end{equation*}



In unserem Falle, wollen wir annehmen, dass $f$ eine lineare Funktion
(vielleicht auch noch als *Gerade* bekannt)
$$f(x) = \theta_1 + \theta_2 x$$ ist. Damit können wir sagen, dass

1.  $f$ einigermassen einfach zu bestimmen ist (wir brauchen nur zwei
    Parameter $\theta_1$ und $\theta_2$)

2.  wenn wir an Trends interessiert sind, sind lineare Funktionen eine
    gute Wahl

3.  allerdings sind Prozesse meistens nichtlinear.

Vor allem haben wir sicher oft gehört, dass die Virusausbreitung gerne
exponentiell verläuft, also zum Beispiel durch eine Funktion
$$g(x) = k_1 2^{k_2x}$$ beschrieben werden sollte. Hier ist $k_1$ ein
Skalierungsfaktor und $k_2$ ein Parameter, der die Wachstumsrate
bestimmt.

In der Tat scheint die Annäherung der Datenpunkte aus Abbildung durch
eine Gerade nicht zielführend.

> Allerdings sind exponentielle Zusammenhänge auf einer logarithmischen
> Skala linear.

Deshalb schauen wir uns die Daten über den Logarithmus (zur Basis $2$)
der Infektionszahlen an

> (Tag $x$, $\log_2$(Infektionen am Tag $x$))

Im Bild der Daten () kann Jan erkennen, dass eine Gerade vielleicht
(abgesehen von einigen Werten) ganz gut reinpassen könnte.

## Lineare Regression

Die Methode der *linearen Regression* besteht aus dem Anpassen einer
linearen Funktion (einer *Geraden* im 2-dimensionalen Raum) an eine
Datenwolke. Aus den unendlich vielen Möglichkeiten die Funktion zu
definieren, wird die gewählt, die im Mittel den kleinsten Fehler
zwischen den Datenpunkten und der Funktionsbeschreibung erzeugt.

Für unser Beispiel der CoVID-19 Zahlen, können wir *ad hoc* die
Parameter $\theta_1$ und $\theta_2$ bestimmen. Wenn es soweit ist,
werden wir die zugrundeliegenden mathematischen Konzepte und allgemein
gültige Formeln zur Lösung kennenlernen.

Konkret gehen wir davon aus, dass wir $N$ Datenpunkte
$$(x_j,h_j), \quad j=1,2,3,\dotsc,N$$ haben wobei $x_j$ die Variable ist
und $h_j$ das Merkmal dazu. In unserem Beispiel ist $N=31$ (so viele
Tage hat der Oktober 2020), $x_{10}=10$ wäre der 10. Oktober und
$h_{10}$ der Logarithmus der Anzahl der am 10. Oktober registrierten
Fälle.

Und durch diese Datenpunkte wollen wir nun bestmöglich eine Gerade der
Form $$h(x)=\theta_1 + \theta_2 x$$ legen. Bestmöglich bedeutet, dass
der Fehler über alle Datenpunkte gemittelt möglichst klein sein soll.
Dafür definieren wir zunächst den Fehler im Datenpunkt $x_j$ als
$$e_j:= \frac 12 (h(x_j)-h_j)^2 = \frac 12 (\theta_1+\theta_2x_j -f_j)^2$$
wobei das Quadrat (u.a.) sicherstellt, dass alle Fehler positiv sind und
das "$\frac 12$" einfach ranmultipliziert wird um nachher beim Ableiten
einen Faktor zu sparen.

Und der gesammelte Fehler ist dann gegeben durch die Fehlerfunktion $e$
als die Summer aller Fehler:
$$e=e_1+e_2+\dotsc+e_N = \sum_{j=1}^N e_j = \frac 12\sum_{j=1}^N  (\theta_1+\theta_2x_j -f_j)^2$$

Jan kann erkennen, dass die Funktion $e$ für verschiedene
$(\theta_1, \theta_2)$ verschiedene Werte annimmt und dass ein minimaler
Wert von $e$ einen minimalen Fehler in der Approximation der Daten durch
$h(x)=\theta_1+\theta_2 x$ bedeutet.

Und wie finden wir die Minimalstelle von $e$ beziehungsweise die
optimalen $(\theta_1,\theta_2)$? Fast wie in der Schule:

1.  die Funktion $e$ ableiten,

2.  eine Nullstelle der Ableitung finden.

Dafür müssen wir erstmal verstehen, dass die Funktion eine Funktion von
2 Variablen ist (und dass das ein grosses Thema der Vorlesung werden
wird).

Fürs erste sei gesagt, dass Jan die Variablen $(\theta_1, \theta_2)$
separat betrachten kann und danach ableiten und null setzen. Also nach
der Summen-- und der Kettenregel: $$\begin{aligned}
\frac{d}{d\theta_1} e(\theta_1, \theta_2) &= \frac 12 \sum_{j=1}^N  \frac{d}{d\theta_1} (\theta_1+\theta_2x_j -f_j)^2 = \frac 12 \sum_{j=1}^N 2(\theta_1+\theta_2x_j -f_j) \\
&= \sum_{j=1}^N (\theta_1+\theta_2x_j -f_j)\end{aligned}$$ und
$$\begin{aligned}
\frac{d}{d\theta_2} e(\theta_1, \theta_2) &= \frac 12 \sum_{j=1}^N  \frac{d}{d\theta_2} (\theta_1+\theta_2x_j -f_j)^2 = \frac 12 \sum_{j=1}^N 2(\theta_1+\theta_2x_j -f_j)x_j \\
&= \sum_{j=1}^N (\theta_1+\theta_2x_j -f_j)x_j\end{aligned}$$ abgeleitet
sodass das Nullsetzen $$\begin{aligned}
0&= \sum_{j=1}^N (\theta_1+\theta_2x_j -f_j) \\
0&= \sum_{j=1}^N (\theta_1+\theta_2x_j -f_j)x_j\end{aligned}$$ uns zwei
Gleichungen ergibt, die (in aller Regel) die zwei unbekannten Parameter
$\theta_1$ und $\theta_2$ (eindeutig) bestimmen.

Mit den konkreten Zahlen von oben gerechnet (und auf 2 Nachkommastellen
gekürzt) identifizieren wir $$\theta_1^*=7.88, \quad \theta_2^*=0.12$$
als die optimale Wahl und damit $$h^*(x) = 7.88 + 0.12x$$ als die beste
Gerade; Jan vergleiche Abbildung
[3](#fig:logtwo-cases-fitted){reference-type="ref"
reference="fig:logtwo-cases-fitted"}.

Natürlich können wir mit $$y = 2^{\log_2(y)}$$ auch das *logarithmieren*
wieder rückgängig und auch die Ausgleichsgerade auf die lineare (nicht
*logarithmische*) Datenskalierung transformieren
$$g(x) = 2^{\theta_1 + \theta_2 x} = 2^{\theta_1}\cdot 2^{\theta_2 x}$$
was uns mit den Parametern von oben die optimale Ausgleichskurve als
$$g^*(x) = 2^{7.88 + 0.12x} = 2^{7.88}\cdot 2^{0.12 x}=236.16\cdot 2^{0.12 x}$$
ergibt (dargestellt in Abbildung
[4](#fig:cases-fitted){reference-type="ref"
reference="fig:cases-fitted"}).

![SARS-CoV-2 Fälle in Bayern im Oktober 2020 und die mittels linearer
Regression auf der logarithmischen Skala bestimmte
Ausgleichskurve](bilder/02-cases-fitted.png){#fig:cases-fitted
width=".8\\textwidth"}

## Mathematische Konzepte in der Linearen Regression {#sec:owo:mathematische-konzepte}

In diesem Beispiel haben wir schon einige Konzepte und Methoden benutzt,
die wir im Laufe der Vorlesung in allen Details kennenlernen werden.

Neben der grundlegenden Technik der *linearen Regression* zur
Datenanalyse werden wir uns eingehend mit

-   Elementarfunktionen (wie hier der Logarithmus und die
    Exponentialfunktion, aber auch trigonometrische Funktionen und
    weitere),

-   Differentiation und Integration von Funktionen und

-   dem Lösen linearer Gleichungssysteme

beschäftigen.
