<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Optimierung unter Nebenbedingungen | Einführung in die mathematische Datenanalyse</title>
  <meta name="description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Optimierung unter Nebenbedingungen | Einführung in die mathematische Datenanalyse" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  <meta name="github-repo" content="highlando/script-emds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Optimierung unter Nebenbedingungen | Einführung in die mathematische Datenanalyse" />
  
  <meta name="twitter:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  

<meta name="author" content="Jan Heiland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="optimierung.html"/>
<link rel="next" href="stochastisches-gradientenverfahren-und-maschinelles-lernen.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">EMDS</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html"><i class="fa fa-check"></i><b>1</b> Was ist Data Science?</a><ul>
<li class="chapter" data-level="1.1" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#wie-passiert-die-datenanalyse"><i class="fa fa-check"></i><b>1.1</b> Wie passiert die Datenanalyse?</a></li>
<li class="chapter" data-level="1.2" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#was-sind-daten"><i class="fa fa-check"></i><b>1.2</b> Was sind Daten?</a></li>
<li class="chapter" data-level="1.3" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#beispiele"><i class="fa fa-check"></i><b>1.3</b> Beispiele</a></li>
<li class="chapter" data-level="1.4" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#python"><i class="fa fa-check"></i><b>1.4</b> Python</a></li>
<li class="chapter" data-level="1.5" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#aufgaben"><i class="fa fa-check"></i><b>1.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html"><i class="fa fa-check"></i><b>2</b> Lineare Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#rauschen-und-fitting"><i class="fa fa-check"></i><b>2.1</b> Rauschen und Fitting</a></li>
<li class="chapter" data-level="2.2" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#ansätze-für-lineare-regression"><i class="fa fa-check"></i><b>2.2</b> Ansätze für lineare Regression</a></li>
<li class="chapter" data-level="2.3" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#sec-linreg-minimierung"><i class="fa fa-check"></i><b>2.3</b> Fehlerfunktional und Minimierung</a></li>
<li class="chapter" data-level="2.4" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#berechnung-der-bestlösung"><i class="fa fa-check"></i><b>2.4</b> Berechnung der Bestlösung</a></li>
<li class="chapter" data-level="2.5" data-path="sec-lineare-regression.html"><a href="sec-lineare-regression.html#beispiel"><i class="fa fa-check"></i><b>2.5</b> Beispiel</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html"><i class="fa fa-check"></i><b>3</b> Matrix-Zerlegungen</a><ul>
<li class="chapter" data-level="3.1" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#qr-zerlegung"><i class="fa fa-check"></i><b>3.1</b> QR Zerlegung</a></li>
<li class="chapter" data-level="3.2" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#singulärwertzerlegung"><i class="fa fa-check"></i><b>3.2</b> Singulärwertzerlegung</a></li>
<li class="chapter" data-level="3.3" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#aufgaben-1"><i class="fa fa-check"></i><b>3.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html"><i class="fa fa-check"></i><b>4</b> Hauptkomponentenanalyse</a><ul>
<li class="chapter" data-level="4.1" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#variationskoeffizienten"><i class="fa fa-check"></i><b>4.1</b> Variationskoeffizienten</a></li>
<li class="chapter" data-level="4.2" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#koordinatenwechsel"><i class="fa fa-check"></i><b>4.2</b> Koordinatenwechsel</a></li>
<li class="chapter" data-level="4.3" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#sec-pca-maximierung"><i class="fa fa-check"></i><b>4.3</b> Maximierung der Varianz in (Haupt)-Achsenrichtung</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html"><i class="fa fa-check"></i><b>5</b> Hauptkomponentenanalyse Ctd.</a><ul>
<li class="chapter" data-level="5.1" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#der-penguins-datensatz"><i class="fa fa-check"></i><b>5.1</b> Der PENGUINS Datensatz</a></li>
<li class="chapter" data-level="5.2" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#darstellung"><i class="fa fa-check"></i><b>5.2</b> Darstellung</a></li>
<li class="chapter" data-level="5.3" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#korrelationen-und-die-kovarianzmatrix"><i class="fa fa-check"></i><b>5.3</b> Korrelationen und die Kovarianzmatrix</a></li>
<li class="chapter" data-level="5.4" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#hauptachsentransformation"><i class="fa fa-check"></i><b>5.4</b> Hauptachsentransformation</a></li>
<li class="chapter" data-level="5.5" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#rekonstruktion"><i class="fa fa-check"></i><b>5.5</b> Rekonstruktion</a></li>
<li class="chapter" data-level="5.6" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#reduktion-der-daten"><i class="fa fa-check"></i><b>5.6</b> Reduktion der Daten</a></li>
<li class="chapter" data-level="5.7" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#am-beispiel-der-pinguin-daten"><i class="fa fa-check"></i><b>5.7</b> Am Beispiel der Pinguin Daten</a></li>
<li class="chapter" data-level="5.8" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#aufgaben-2"><i class="fa fa-check"></i><b>5.8</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html"><i class="fa fa-check"></i><b>6</b> Clustering und Hauptkomponentenanalyse</a><ul>
<li class="chapter" data-level="6.1" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#clustering-im-allgemeinen"><i class="fa fa-check"></i><b>6.1</b> Clustering im Allgemeinen</a></li>
<li class="chapter" data-level="6.2" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#k-means-clustering"><i class="fa fa-check"></i><b>6.2</b> K-means Clustering</a></li>
<li class="chapter" data-level="6.3" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#clustering-und-hauptkomponentenanalyse-1"><i class="fa fa-check"></i><b>6.3</b> Clustering und Hauptkomponentenanalyse</a></li>
<li class="chapter" data-level="6.4" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#training-und-testing"><i class="fa fa-check"></i><b>6.4</b> Training und Testing</a></li>
<li class="chapter" data-level="6.5" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#am-beispiel-der-pinguine"><i class="fa fa-check"></i><b>6.5</b> Am Beispiel der Pinguine</a></li>
<li class="chapter" data-level="6.6" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#aufgaben-3"><i class="fa fa-check"></i><b>6.6</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="optimierung.html"><a href="optimierung.html"><i class="fa fa-check"></i><b>7</b> Optimierung</a><ul>
<li class="chapter" data-level="7.1" data-path="optimierung.html"><a href="optimierung.html#multivariable-funktionen"><i class="fa fa-check"></i><b>7.1</b> Multivariable Funktionen</a></li>
<li class="chapter" data-level="7.2" data-path="optimierung.html"><a href="optimierung.html#partielle-ableitungen-und-der-gradient"><i class="fa fa-check"></i><b>7.2</b> Partielle Ableitungen und der Gradient</a></li>
<li class="chapter" data-level="7.3" data-path="optimierung.html"><a href="optimierung.html#richtungs-ableitung"><i class="fa fa-check"></i><b>7.3</b> Richtungs-Ableitung</a></li>
<li class="chapter" data-level="7.4" data-path="optimierung.html"><a href="optimierung.html#optimierung-1"><i class="fa fa-check"></i><b>7.4</b> Optimierung</a></li>
<li class="chapter" data-level="7.5" data-path="optimierung.html"><a href="optimierung.html#gradientenabstiegsverfahren"><i class="fa fa-check"></i><b>7.5</b> Gradientenabstiegsverfahren</a></li>
<li class="chapter" data-level="7.6" data-path="optimierung.html"><a href="optimierung.html#extra-nichtglatte-optimierung"><i class="fa fa-check"></i><b>7.6</b> Extra: Nichtglatte Optimierung</a></li>
<li class="chapter" data-level="7.7" data-path="optimierung.html"><a href="optimierung.html#extra-automatisches-differenzieren"><i class="fa fa-check"></i><b>7.7</b> Extra: Automatisches Differenzieren</a></li>
<li class="chapter" data-level="7.8" data-path="optimierung.html"><a href="optimierung.html#aufgaben-4"><i class="fa fa-check"></i><b>7.8</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimierung-unter-nebenbedingungen.html"><a href="optimierung-unter-nebenbedingungen.html"><i class="fa fa-check"></i><b>8</b> Optimierung unter Nebenbedingungen</a><ul>
<li class="chapter" data-level="8.1" data-path="optimierung-unter-nebenbedingungen.html"><a href="optimierung-unter-nebenbedingungen.html#richtungen-und-nebenbedingungen"><i class="fa fa-check"></i><b>8.1</b> Richtungen und Nebenbedingungen</a></li>
<li class="chapter" data-level="8.2" data-path="optimierung-unter-nebenbedingungen.html"><a href="optimierung-unter-nebenbedingungen.html#restringierte-optimierungsprobleme-und-der-gradient"><i class="fa fa-check"></i><b>8.2</b> Restringierte Optimierungsprobleme und der Gradient</a></li>
<li class="chapter" data-level="8.3" data-path="optimierung-unter-nebenbedingungen.html"><a href="optimierung-unter-nebenbedingungen.html#linear-quadratische-probleme"><i class="fa fa-check"></i><b>8.3</b> Linear Quadratische Probleme</a></li>
<li class="chapter" data-level="8.4" data-path="optimierung-unter-nebenbedingungen.html"><a href="optimierung-unter-nebenbedingungen.html#sequential-quadratic-programming"><i class="fa fa-check"></i><b>8.4</b> Sequential Quadratic Programming</a></li>
<li class="chapter" data-level="8.5" data-path="optimierung-unter-nebenbedingungen.html"><a href="optimierung-unter-nebenbedingungen.html#aufgaben-5"><i class="fa fa-check"></i><b>8.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="stochastisches-gradientenverfahren-und-maschinelles-lernen.html"><a href="stochastisches-gradientenverfahren-und-maschinelles-lernen.html"><i class="fa fa-check"></i><b>9</b> Stochastisches Gradientenverfahren und Maschinelles Lernen</a><ul>
<li class="chapter" data-level="9.1" data-path="stochastisches-gradientenverfahren-und-maschinelles-lernen.html"><a href="stochastisches-gradientenverfahren-und-maschinelles-lernen.html#hintergrund"><i class="fa fa-check"></i><b>9.1</b> Hintergrund</a></li>
<li class="chapter" data-level="9.2" data-path="stochastisches-gradientenverfahren-und-maschinelles-lernen.html"><a href="stochastisches-gradientenverfahren-und-maschinelles-lernen.html#iterative_method"><i class="fa fa-check"></i><b>9.2</b> Stochastisches Abstiegsverfahren</a></li>
<li class="chapter" data-level="9.3" data-path="stochastisches-gradientenverfahren-und-maschinelles-lernen.html"><a href="stochastisches-gradientenverfahren-und-maschinelles-lernen.html#aufgabe"><i class="fa fa-check"></i><b>9.3</b> Aufgabe</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Einführung in die mathematische Datenanalyse</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimierung-unter-nebenbedingungen" class="section level1 hasAnchor">
<h1><span class="header-section-number">8</span> Optimierung unter Nebenbedingungen<a href="optimierung-unter-nebenbedingungen.html#optimierung-unter-nebenbedingungen" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Oftmals werden an die gesuchte Lösung <span class="math inline">\(x^* \in \mathbb R^{n}\)</span> eines Optimierungsproblems a-priori bestimmte Anforderungen gestellt, wie:</p>
<ul>
<li><span class="math inline">\(x_1 + x_2 \geq 0\)</span> (die Lösung soll in einem bestimmten Abschnitt liegen)</li>
<li><span class="math inline">\(x_1^2 + x_2^2 - c^2 = 0\)</span> (die ersten beiden Komponenten der Lösung sollen auf einer Kreisbahn liegen)</li>
</ul>
<p>Allgemein lässt sich so ein Optimierungsproblem schreiben als</p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 8.1  (Optimierungsproblem mit Nebenbedingungen) </strong></span><span class="math display">\[\begin{equation*}
f(x) \to \min_{x\in \mathbb R^{n}}, \quad \text{s.t.} \quad g_i(x)\geq 0, \quad h_j(x) = 0,
\end{equation*}\]</span></p>
<ul>
<li>mit der <em>Zielfunktion</em> <span class="math inline">\(f\colon \mathbb R^{n} \to \mathbb R^{}\)</span></li>
<li><p>und zusätzlichen Nebenbedingungen (<em>s.t.</em> – <em>subject to</em>), die in</p>
<ul>
<li><em>Ungleichungsform</em> über Funktionen <span class="math inline">\(g_i\colon \mathbb R^{n} \to \mathbb R\)</span>, mit <span class="math inline">\(i\)</span> aus der Indexmenge <span class="math inline">\(\mathcal I \subset \mathbb N\)</span> oder</li>
<li><em>Gleichungsform</em> über Funktionen <span class="math inline">\(h_j\colon \mathbb R^{n} \to \mathbb R\)</span>, mit <span class="math inline">\(j\)</span> aus der Indexmenge <span class="math inline">\(\mathcal J \subset \mathbb N\)</span>
vorliegen können.</li>
</ul></li>
</ul>
<p>heißt <em>restringiertes Optimierungsproblem</em> oder <em>Optimierungsproblem mit Nebenbedingungen</em></p>
<p>Liegen keine Gleichheits– oder keine Ungleichungsnebenbedingungen vor, setzen wir <span class="math inline">\(\mathcal I = \emptyset\)</span> oder <span class="math inline">\(\mathcal J = \emptyset\)</span>.</p>
<p>Ein Optimierungsproblem ohne Nebenbedingungen heißt auch <em>unrestringiertes</em> Problem.</p>
</div>
<p>Jetzt stellt sich zur Frage ob ein <span class="math inline">\(x^*\)</span> <em>optimal</em> ist, noch die Frage, ob es auch <em>zulässig</em> (engl.: <em>feasible</em>) ist. Dazu lässt sich formal der Zulässigkeitsbereich (engl.: <em>feasible set</em>) definieren
<span class="math display">\[\begin{equation*}
\Omega = \{x\in \mathbb R^{n} ~|~g_i(x)\geq 0, \quad h_j(x)=0, \quad i\in \mathcal I,\, j\in \mathcal J\}
\end{equation*}\]</span>
und das Optimierungsproblem als
<span class="math display">\[\begin{equation*}
f(x) \to \min_{x\in \Omega}
\end{equation*}\]</span>
schreiben.</p>
<div id="constaints-domofdef" class="JHSAYS">
<p>Die Definition <a href="optimierung.html#def:loc-glob-minimum">7.1</a> einer Minimalstelle gilt unmittelbar weiter, wenn nun auch der Zulässigkeitsbereich noch beachtet wird.</p>
</div>
<p>Die Unterscheidung von <span class="math inline">\(D(f)\)</span> und <span class="math inline">\(\Omega\)</span> in der Betrachtung des Optimierungsproblems und potentieller Minimalstellen hat diverse vor allem praktische Gründe:</p>
<ul>
<li>der Definitionsbereich von <span class="math inline">\(f\)</span> ist eine vorgegebene Eigenschaft der Funktion während <span class="math inline">\(\Omega\)</span> vielleicht variiert werden soll</li>
<li>liegen nichttriviale Gleichheitsbedingungen vor, ist beispielsweise kein Punkt von <span class="math inline">\(\Omega\)</span> mehr ein innerer Punkt (im <span class="math inline">\(\mathbb R^{n}\)</span>) und der Gradient von <span class="math inline">\(f\)</span> wäre erstmal nicht definiert</li>
<li>die Modellierung ist oftmals einfacher, wenn Nebenbedingungen einfach zum Problem hinzugefügt werden können</li>
<li>ebenso die Feststellung ob ein gegebener Punkt im Zulässigkeitsbereich liegt</li>
</ul>
<p>Die Analysis restringierter Optimierungsprobleme und entsprechend die Umsetzung von Algorithmen zur Lösung wird durch die Nebenbedingungen ungleich schwieriger, da</p>
<ul>
<li>in der Theorie nicht mehr einfach differenziert werden kann (insbesondere bei Gleichheitsbedingungen oder wenn der kritische Punkt am Rand der Ungleichungsbedingungen liegt) und</li>
<li>in der Umsetzung auf dem Computer darauf geachtet werden muss, dass der Zulässigkeitsbereich nicht verlassen wird.</li>
</ul>
<div id="richtungen-und-nebenbedingungen" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.1</span> Richtungen und Nebenbedingungen<a href="optimierung-unter-nebenbedingungen.html#richtungen-und-nebenbedingungen" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bei <em>restringierten Optimierungsproblemen</em> ist die Richtung in der analysiert wird entscheidend. Hierbei geht es darum ob in einer Richtung <span class="math inline">\(v\)</span></p>
<ol style="list-style-type: decimal">
<li>die Zielfunktion eventuell kleiner wird</li>
<li>der Zulässigkeitsbereich eventuell verlassen wird (bspw. charakterisiert durch <span class="math inline">\(g(x) = 0\)</span> oder <span class="math inline">\(h(x)\geq0\)</span>)</li>
</ol>
<p>Da es stets nur um die unmittelbare Umgebung eines Punktes geht, und wir aus der Diskussion der Ableitung (in 1D) wissen, dass eine differenzierbare Funktion lokal gut durch ihre Tangente angenähert werden kann<a href="referenzen.html#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>, können wir beide Betrachtungen in der <em>lineaeren Approximation</em> anstellen.</p>
<p>Wir schauen also ob für eine Richtung <span class="math inline">\(v\)</span> gilt, ob nicht</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(f(x^*) &gt; f(x^*+hv) \approx f(x^*) + \nabla f(x^*)^Tv\)</span> bzw.</li>
<li><span class="math inline">\(0 \neq g(x^*+hv) \approx \nabla g(x^*)^Tv\)</span> oder <span class="math inline">\(0&gt;h(x^*+hv) \approx \nabla h(x^*)^Tv\)</span>.</li>
</ol>
</div>
<div id="restringierte-optimierungsprobleme-und-der-gradient" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.2</span> Restringierte Optimierungsprobleme und der Gradient<a href="optimierung-unter-nebenbedingungen.html#restringierte-optimierungsprobleme-und-der-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Zur Illustration und zur Einführung der Konzepte betrachten wir Probleme, die entweder nur Gleichungsnebenbedingungen oder nur Ungleichungsnebenbedingungen haben.</p>
<p>Wir beginnen mit Gleichungsnebenbedingungen und das auch nur im zweidimensionalen Fall:
<span class="math display">\[\begin{equation*}
f(x,y) \to \min, \quad \text{s.t. }g(x,y)=c.
\end{equation*}\]</span>
Aus der schematischen Darstellung in Abbildung <a href="optimierung-unter-nebenbedingungen.html#fig:eqconstraint-opti">8.1</a> können wir ablesen, dass in einem potentiellen Extremum, die Gradienten von <span class="math inline">\(f\)</span> und <span class="math inline">\(g\)</span> in die gleiche Richtung zeigen, es also eine Skalar <span class="math inline">\(\lambda\)</span> geben muss, sodass in einem kritischen Punkt <span class="math inline">\((x^*, y^*)\)</span> gilt, dass
<span class="math display">\[\begin{equation*}
\nabla f(x^*, y^*) + \lambda \nabla g(x^*, y^*) =0
\end{equation*}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:eqconstraint-opti"></span>
<img src="bilder/08-LagrangeMultipliers2D.png" alt="Schematische Darstellung eines Optimierungsproblems mit Gleichungsnebenbedingungen in 2D." width="60%" />
<p class="caption">
Figure 8.1: Schematische Darstellung eines Optimierungsproblems mit Gleichungsnebenbedingungen in 2D.
</p>
</div>
<p>Wir könnten also versuchen, eine Lösung <span class="math inline">\((x,y,\lambda)\)</span> für das Gleichungssystem
<span class="math display" id="eq:kkt-sc">\[\begin{equation}
\begin{split}
\nabla f(x, y) + \lambda \nabla g(x, y) &amp;=0 \\
g(x,y) &amp;= c
\end{split}
\tag{8.1}
\end{equation}\]</span>
zu finden.</p>
<div id="lagrange-KKT" class="JHSAYS">
<p>Dieses Gleichungssystem ist ein Spezialfall der <em>Karush-Kuhn-Tucker</em> Bedingungen, die ein notwendiges Kriterium für einen optimalen Punkt darstellen. Das involvierte <span class="math inline">\(\lambda\)</span> wird <em>Lagrange Multiplikator</em> genannt.</p>
</div>
<p>Dass an einem potentiellen Minimum gelten muss, dass <span class="math inline">\(\nabla f(x^*) = -\lambda \nabla g(x^*)\)</span> haben wir anhand einer Zeichnung im 2D Fall festgestellt.</p>
<p>Für <span class="math inline">\(x\in \mathbb R^{n}\)</span> und einer Gleichungsnebenbedingung <span class="math inline">\(g\)</span>, ist die Argumentation wie folgt.</p>
<div class="lemma">
<p><span id="lem:not-para-not-min" class="lemma"><strong>Lemma 8.1  </strong></span>Sei <span class="math inline">\(x^*\in \mathbb R^{n}\)</span> mit <span class="math inline">\(g(x^*)=0\)</span> so, dass <span class="math inline">\(\nabla f\)</span> und <span class="math inline">\(\nabla g\)</span> <strong>nicht</strong> parallel sind. Dann gilt für die Richtung
<span class="math display">\[\begin{equation*}
v := -\biggl (I-\frac{1}{\|\nabla g(x^*)\|^2}\nabla g(x^*)g(x^*)^T \biggr )\nabla f(x^*)
\end{equation*}\]</span>
dass <span class="math inline">\(\nabla g(x^*)^Tv=0\)</span> und <span class="math inline">\(\nabla f(x^*)^Tv&lt;0\)</span> ist.</p>
</div>
<p>Lemma <a href="optimierung-unter-nebenbedingungen.html#lem:not-para-not-min">8.1</a> sagt, dass es im skizzierten Fall also eine Richtung gibt, in der (in erster Ableitung) die Funktion <span class="math inline">\(f\)</span> minimiert werden kann ohne den Zulässigkeitsbereich zu verlassen.</p>
</div>
<div id="linear-quadratische-probleme" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.3</span> Linear Quadratische Probleme<a href="optimierung-unter-nebenbedingungen.html#linear-quadratische-probleme" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ist die Zielfunktion als quadratische Funktion
<span class="math display">\[\begin{equation*}
f(x) = x^T Q x + c^Tx 
\end{equation*}\]</span>
gegeben mit <span class="math inline">\(Q\in \mathbb R^{n\times n}\)</span> und <span class="math inline">\(c\in \mathbb R^{n}\)</span> und sind die Nebenbedingungen linear gegeben als
<span class="math display">\[\begin{equation*}
Ax \geq b
\end{equation*}\]</span>
mit <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>, <span class="math inline">\(b\in \mathbb R^{m}\)</span> und mit <span class="math inline">\(Ax \geq b\)</span> bedeuten soll, dass <strong>alle Einträge</strong> des Vektors <span class="math inline">\(Ax-b\)</span> kleiner oder gleich <span class="math inline">\(0\)</span> sind, dann sprechen wir von einem (linearen) quadratischen Optimierungsproblem.</p>
<p>Hier müssen Gleichheitsbedingungen nicht explizit angeführt werden, da sie durch Ungleichungsbedingungen ausgedrückt werden können (vgl. <span class="math inline">\(x=0\)</span> gdw. <span class="math inline">\(x\geq0\)</span> und <span class="math inline">\(-x\geq 0\)</span>).</p>
<p>Sind allerdings alle Bedingungen letztlich Gleichheitsbedingungen, liegt also das Problem als
<span class="math display">\[\begin{equation*}
x^T Q x + c^Tx  \to \min_{x\in \mathbb R^{n}} \quad \text{s.t. }Ax=b
\end{equation*}\]</span>
vor, dann sind die notwendigen Optimalitätsbedingungen durch
<span class="math display">\[\begin{equation*}
\begin{bmatrix}
Q+Q^T &amp; A^T \\
A &amp; 0
\end{bmatrix}
=
\begin{bmatrix}
-c \\ b
\end{bmatrix}.
\end{equation*}\]</span></p>
<p>Aus der Betrachtung dieser Optimalitätsbedingungen können wir folgern, dass, wenn <span class="math inline">\(Q\)</span> symmetrisch ist und positiv definit,
sowie alle Zeilen von <span class="math inline">\(A\)</span> linear unabhängig sind, dass dann das Optimierungsproblem eine eindeutige Lösung hat, die durch die Optimalitätsbedingungen eindeutig charakterisiert ist.</p>
</div>
<div id="sequential-quadratic-programming" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.4</span> Sequential Quadratic Programming<a href="optimierung-unter-nebenbedingungen.html#sequential-quadratic-programming" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Aus der Beobachtung, dass wir LQP Probleme direkt mit <span class="math inline">\(Q\)</span> symmetrisch positiv definit direkt lösen können und dass wir für kleine Abweichungen <span class="math inline">\(\xi\)</span> von einem gebenen Punkt <span class="math inline">\(x\)</span> multivariable Funktionen über quadratische (für unser <span class="math inline">\(f\)</span>) und lineare Approximationen (für unsere <span class="math inline">\(g_i\)</span>) gut annähern können:
<span class="math display">\[\begin{equation*}
f(x^*) = f(x+\xi) = f(x) + \nabla f(x)^T \xi + \xi^TH_f(x) \xi + \mathcal o(\| \xi \| ^2 )
\end{equation*}\]</span>
und
<span class="math display">\[\begin{equation*}
g_i(x^*) = g_i(x+\xi) = g_i(\xi) + \nabla g_i(\xi)^T h + \mathcal o(\| \xi \|), \quad i \in \mathcal I,
\end{equation*}\]</span>
ergibt sich das folgende iterative Verfahren genannt <em>Sequential Linear Programming</em> (SQP):</p>
<p>Beginnend von einem Näherungswert <span class="math inline">\(x^k\)</span></p>
<ol style="list-style-type: decimal">
<li>Berechne <span class="math inline">\(d^k\)</span> aus der Minimierung von<a href="referenzen.html#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>
<span class="math display">\[\begin{align*}
\tilde f(x^k+d):= \nabla f(x^k)^T d + d^TH_f(x^k) d \to \min_{d\in \mathbb R^{n}}, \\\quad \text{unter der Nebenbedingung} \\ \tilde g_i(x^k+d):=g(x^k)+\nabla g_i(x^k)d = 0 \quad i \in \mathcal I
  \end{align*}\]</span></li>
<li>ein Update <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</li>
</ol>
<p>Es ist also in jeder Iteration ein linear quadratisches Optimierungsproblem
<span class="math display">\[\begin{equation*}
x^T Q_k x + c_k^Tx  \to \min_{x\in \mathbb R^{n}} \quad \text{s.t. }A_kx=b_k
\end{equation*}\]</span>
zu lösen, wobei (in der obigen Notation mit <span class="math inline">\(Q\)</span>, <span class="math inline">\(c\)</span>, <span class="math inline">\(A\)</span> und <span class="math inline">\(b\)</span>) gilt dass
<span class="math display">\[\begin{equation*}
Q_k=H_f(x^k), \quad c_k=\nabla f(x_k)
\end{equation*}\]</span>
und
<span class="math display">\[\begin{equation*}
A_k = 
\begin{bmatrix}
\nabla g_i(x_k)^T
\end{bmatrix}_{i \in \mathcal I} \in \mathbb R^{|\mathcal I| \times n},
\quad 
b_k = -
\begin{bmatrix}
g_i(x_k) 
\end{bmatrix}_{i \in \mathcal I}\in \mathbb R^{|\mathcal I|}.
\end{equation*}\]</span></p>
<div id="rem-conv-hesse" class="JHSAYS">
<p>Ist die Funktion <span class="math inline">\(f\)</span> konvex und zweimal stetig differenzierbar (in einer Umgebung von <span class="math inline">\(x_k\)</span>), dann ist die Hesse-matrix <span class="math inline">\(H_f(x_k)\)</span> positiv definit und das LQP hat eine eindeutige Lösung <span class="math inline">\(x_{k+1}\)</span>.</p>
</div>
</div>
<div id="aufgaben-5" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.5</span> Aufgaben<a href="optimierung-unter-nebenbedingungen.html#aufgaben-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="kkt-für-lqp-t" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.5.1</span> KKT für LQP (T)<a href="optimierung-unter-nebenbedingungen.html#kkt-für-lqp-t" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Verifizieren Sie, dass die notwendigen Optimalitätsbedingungen für das LQP Problem mit Gleichungsnebenbedingungen (mit <span class="math inline">\(m=1\)</span>) genau der Optimalitäsbedingung aus Gleichung <a href="optimierung-unter-nebenbedingungen.html#eq:kkt-sc">(8.1)</a> entspricht. <strong>Hinweis</strong>: Mit der Definition der <em>totalen Ableitung</em> und der Relation zum Gradienten für reellwertige multivariable Funktionen (vgl. die Vorlesung <em>Mathe für DS2</em> vom 6. Juli) ist eine Darstellung der Gradienten <span class="math inline">\(\nabla f(x)\)</span> und <span class="math inline">\(\nabla g(x)\)</span> direkt herleitbar.</p>
</div>
<div id="nicht-parallele-gradienten-t" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.5.2</span> Nicht parallele Gradienten (T)<a href="optimierung-unter-nebenbedingungen.html#nicht-parallele-gradienten-t" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Verifizieren sie die Aussage aus Lemma <a href="optimierung-unter-nebenbedingungen.html#lem:not-para-not-min">8.1</a>.</p>
</div>
<div id="hauptkomponente-aus-sqp-tp" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.5.3</span> Hauptkomponente aus SQP (T+P)<a href="optimierung-unter-nebenbedingungen.html#hauptkomponente-aus-sqp-tp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Skizzieren sie das Optimierungsproblems, das die <strong>letzte</strong> Hauptkomponentenrichtung definiert (also die Richtung mit minimaler Varianz) eines zentrierten Datensatzes definiert (analog dazu, wie in Abschnitt <a href="hauptkomponentenanalyse.html#sec-pca-maximierung">4.3</a> eingeführt). <strong>Hinweis</strong>: Alles wird vielleicht etwas einfacher, wenn sie die Nebenbedingung <span class="math inline">\(\|x\|=1\)</span> durch <span class="math inline">\(\|x\|^2 = 1\)</span> bzw. $x^Tx = 1 $ ersetzen. (T)</p>
<p>Skizzieren einen SQP-Schritt zur Lösung dieses Problems. (T)</p>
<p>Implementieren sie die SQP Iteration zur Berechnung der letzten Hauptkomponentenrichtung für die Pinguin Daten. Berechnen Sie die Abweichung zur <em>exakten</em> (mit der SVD berechneten) Lösung und geben sie ihn in jedem Schritt beispielsweise den Winkel zwischen dem Iteranten und der exakten Loesung berechnen. <strong>Hinweis</strong>: Nur die Norm der Differenz könnte irritieren, weil das Vorzeichen der Richtungen nicht festgelegt ist.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode py"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" title="1"><span class="im">import</span> json</a>
<a class="sourceLine" id="cb22-2" title="2"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb22-3" title="3"></a>
<a class="sourceLine" id="cb22-4" title="4"><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;penguin-data.json&#39;</span>, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> f:</a>
<a class="sourceLine" id="cb22-5" title="5">    datadict <span class="op">=</span> json.load(f)</a>
<a class="sourceLine" id="cb22-6" title="6"></a>
<a class="sourceLine" id="cb22-7" title="7">data <span class="op">=</span> np.array(datadict[<span class="st">&#39;data&#39;</span>])</a>
<a class="sourceLine" id="cb22-8" title="8"></a>
<a class="sourceLine" id="cb22-9" title="9"><span class="co"># center the data</span></a>
<a class="sourceLine" id="cb22-10" title="10">X <span class="op">=</span> data <span class="op">-</span> data.mean(axis<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb22-11" title="11">XtX <span class="op">=</span> X.T <span class="op">@</span> X  <span class="co"># for later use</span></a>
<a class="sourceLine" id="cb22-12" title="12"></a>
<a class="sourceLine" id="cb22-13" title="13">U, S, Vh <span class="op">=</span> np.linalg.svd(X)</a>
<a class="sourceLine" id="cb22-14" title="14"></a>
<a class="sourceLine" id="cb22-15" title="15">pcfour <span class="op">=</span> Vh[<span class="op">-</span><span class="dv">1</span>:, :].T  <span class="co"># die &quot;letzte&quot; Hauptrichtung</span></a>
<a class="sourceLine" id="cb22-16" title="16">nrmpcf <span class="op">=</span> np.linalg.norm(pcfour)  <span class="co"># brauchen wir um den Winkel zu berechnen</span></a>
<a class="sourceLine" id="cb22-17" title="17"></a>
<a class="sourceLine" id="cb22-18" title="18">xk <span class="op">=</span> np.zeros((<span class="dv">4</span>, <span class="dv">1</span>))  <span class="co"># Startvektor</span></a>
<a class="sourceLine" id="cb22-19" title="19">xk[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb22-20" title="20">arccosalpha <span class="op">=</span> xk.T <span class="op">@</span> pcfour <span class="op">/</span> (np.linalg.norm(xk)<span class="op">*</span>nrmpcf)</a>
<a class="sourceLine" id="cb22-21" title="21"><span class="bu">print</span>(<span class="ss">f&#39;Iteration: </span><span class="sc">{</span><span class="dv">0</span><span class="sc">}</span><span class="ss"> -- `arccos(sol, xk)` </span><span class="sc">{</span>arccosalpha<span class="sc">}</span><span class="ss">&#39;</span>)</a>
<a class="sourceLine" id="cb22-22" title="22"></a>
<a class="sourceLine" id="cb22-23" title="23"></a>
<a class="sourceLine" id="cb22-24" title="24"><span class="kw">def</span> solve_sadpoint_sys_x(Q<span class="op">=</span><span class="va">None</span>, A<span class="op">=</span><span class="va">None</span>, fone<span class="op">=</span><span class="va">None</span>, ftwo<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb22-25" title="25">    <span class="co">&#39;&#39;&#39; Loesung des Sattelpunktprobles</span></a>
<a class="sourceLine" id="cb22-26" title="26"></a>
<a class="sourceLine" id="cb22-27" title="27"><span class="co">    | Q  A.T | . | x |    | f1 |</span></a>
<a class="sourceLine" id="cb22-28" title="28"><span class="co">    | A   0  |   | l |  = | f2 |</span></a>
<a class="sourceLine" id="cb22-29" title="29"></a>
<a class="sourceLine" id="cb22-30" title="30"><span class="co">    Notes:</span></a>
<a class="sourceLine" id="cb22-31" title="31"><span class="co">    -----</span></a>
<a class="sourceLine" id="cb22-32" title="32"></a>
<a class="sourceLine" id="cb22-33" title="33"><span class="co">    Es wird nur `x` zurueckgegeben</span></a>
<a class="sourceLine" id="cb22-34" title="34"></a>
<a class="sourceLine" id="cb22-35" title="35"><span class="co">    &#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb22-36" title="36">    m <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb22-37" title="37">    lineone <span class="op">=</span> [Q, A.T]</a>
<a class="sourceLine" id="cb22-38" title="38">    linetwo <span class="op">=</span> [A, np.zeros((m, m))]</a>
<a class="sourceLine" id="cb22-39" title="39">    sdptmat <span class="op">=</span> np.vstack([np.hstack(lineone),</a>
<a class="sourceLine" id="cb22-40" title="40">                         np.hstack(linetwo)])</a>
<a class="sourceLine" id="cb22-41" title="41">    rhs <span class="op">=</span> np.vstack([fone, ftwo])</a>
<a class="sourceLine" id="cb22-42" title="42">    xl <span class="op">=</span> np.linalg.solve(sdptmat, rhs)</a>
<a class="sourceLine" id="cb22-43" title="43">    <span class="cf">return</span> xl[:<span class="op">-</span>m]  <span class="co"># return only the `x-part`</span></a>
<a class="sourceLine" id="cb22-44" title="44"></a>
<a class="sourceLine" id="cb22-45" title="45"></a>
<a class="sourceLine" id="cb22-46" title="46"><span class="cf">for</span> kkk <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>):</a>
<a class="sourceLine" id="cb22-47" title="47">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb22-48" title="48">    <span class="co"># die SQP Iteration</span></a>
<a class="sourceLine" id="cb22-49" title="49">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb22-50" title="50">    xk <span class="op">=</span> xk <span class="op">+</span> solve_sadpoint_sys_x()</a>
<a class="sourceLine" id="cb22-51" title="51"></a>
<a class="sourceLine" id="cb22-52" title="52">    arccosalpha <span class="op">=</span> xk.T <span class="op">@</span> pcfour <span class="op">/</span> (np.linalg.norm(xk)<span class="op">*</span>nrmpcf)</a>
<a class="sourceLine" id="cb22-53" title="53">    <span class="bu">print</span>(<span class="ss">f&#39;Iteration: </span><span class="sc">{</span>kkk<span class="sc">}</span><span class="ss"> -- `arccos(sol, xk)` </span><span class="sc">{</span>arccosalpha<span class="sc">}</span><span class="ss">&#39;</span>)</a></code></pre></div>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p><em>gut</em> im Sinne von der Fehler <span class="math inline">\(f(x+h)-f(x)-f&#39;(x)h\)</span> geht schneller zur <span class="math inline">\(0\)</span> als <span class="math inline">\(h\)</span><a href="optimierung-unter-nebenbedingungen.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Die korrekte quadratische Approximation von <span class="math inline">\(f\)</span> würde noch den Term <span class="math inline">\(f(x^k)\)</span> enthalten. Dieser kann aber einfach weggelassen werden bei der Suche der Minimalstelle.<a href="optimierung-unter-nebenbedingungen.html#fnref11" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimierung.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="stochastisches-gradientenverfahren-und-maschinelles-lernen.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["EMDS.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
