<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Lineare Regression | Einführung in die mathematische Datenanalyse</title>
  <meta name="description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Lineare Regression | Einführung in die mathematische Datenanalyse" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  <meta name="github-repo" content="highlando/script-emds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Lineare Regression | Einführung in die mathematische Datenanalyse" />
  
  <meta name="twitter:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2022" />
  

<meta name="author" content="Jan Heiland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="was-ist-data-science.html"/>
<link rel="next" href="matrix-zerlegungen.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">EMDS</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html"><i class="fa fa-check"></i><b>1</b> Was ist Data Science?</a><ul>
<li class="chapter" data-level="1.1" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#wie-passiert-die-datenanalyse"><i class="fa fa-check"></i><b>1.1</b> Wie passiert die Datenanalyse?</a></li>
<li class="chapter" data-level="1.2" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#was-sind-daten"><i class="fa fa-check"></i><b>1.2</b> Was sind Daten?</a></li>
<li class="chapter" data-level="1.3" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#beispiele"><i class="fa fa-check"></i><b>1.3</b> Beispiele</a></li>
<li class="chapter" data-level="1.4" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#python"><i class="fa fa-check"></i><b>1.4</b> Python</a></li>
<li class="chapter" data-level="1.5" data-path="was-ist-data-science.html"><a href="was-ist-data-science.html#aufgaben"><i class="fa fa-check"></i><b>1.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lineare-regression.html"><a href="lineare-regression.html"><i class="fa fa-check"></i><b>2</b> Lineare Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="lineare-regression.html"><a href="lineare-regression.html#rauschen-und-fitting"><i class="fa fa-check"></i><b>2.1</b> Rauschen und Fitting</a></li>
<li class="chapter" data-level="2.2" data-path="lineare-regression.html"><a href="lineare-regression.html#ansätze-für-lineare-regression"><i class="fa fa-check"></i><b>2.2</b> Ansätze für lineare Regression</a></li>
<li class="chapter" data-level="2.3" data-path="lineare-regression.html"><a href="lineare-regression.html#fehlerfunktional-und-minimierung"><i class="fa fa-check"></i><b>2.3</b> Fehlerfunktional und Minimierung</a></li>
<li class="chapter" data-level="2.4" data-path="lineare-regression.html"><a href="lineare-regression.html#berechnung-der-bestlösung"><i class="fa fa-check"></i><b>2.4</b> Berechnung der Bestlösung</a></li>
<li class="chapter" data-level="2.5" data-path="lineare-regression.html"><a href="lineare-regression.html#beispiel"><i class="fa fa-check"></i><b>2.5</b> Beispiel</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html"><i class="fa fa-check"></i><b>3</b> Matrix-Zerlegungen</a><ul>
<li class="chapter" data-level="3.1" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#qr-zerlegung"><i class="fa fa-check"></i><b>3.1</b> QR Zerlegung</a></li>
<li class="chapter" data-level="3.2" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#singulärwertzerlegung"><i class="fa fa-check"></i><b>3.2</b> Singulärwertzerlegung</a></li>
<li class="chapter" data-level="3.3" data-path="matrix-zerlegungen.html"><a href="matrix-zerlegungen.html#aufgaben-1"><i class="fa fa-check"></i><b>3.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html"><i class="fa fa-check"></i><b>4</b> Hauptkomponentenanalyse</a><ul>
<li class="chapter" data-level="4.1" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#variationskoeffizienten"><i class="fa fa-check"></i><b>4.1</b> Variationskoeffizienten</a></li>
<li class="chapter" data-level="4.2" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#koordinatenwechsel"><i class="fa fa-check"></i><b>4.2</b> Koordinatenwechsel</a></li>
<li class="chapter" data-level="4.3" data-path="hauptkomponentenanalyse.html"><a href="hauptkomponentenanalyse.html#maximierung-der-varianz-in-haupt-achsenrichtung"><i class="fa fa-check"></i><b>4.3</b> Maximierung der Varianz in (Haupt)-Achsenrichtung</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html"><i class="fa fa-check"></i><b>5</b> Hauptkomponentenanalyse Ctd.</a><ul>
<li class="chapter" data-level="5.1" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#der-penguins-datensatz"><i class="fa fa-check"></i><b>5.1</b> Der PENGUINS Datensatz</a></li>
<li class="chapter" data-level="5.2" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#darstellung"><i class="fa fa-check"></i><b>5.2</b> Darstellung</a></li>
<li class="chapter" data-level="5.3" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#korrelationen-und-die-kovarianzmatrix"><i class="fa fa-check"></i><b>5.3</b> Korrelationen und die Kovarianzmatrix</a></li>
<li class="chapter" data-level="5.4" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#hauptachsentransformation"><i class="fa fa-check"></i><b>5.4</b> Hauptachsentransformation</a></li>
<li class="chapter" data-level="5.5" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#rekonstruktion"><i class="fa fa-check"></i><b>5.5</b> Rekonstruktion</a></li>
<li class="chapter" data-level="5.6" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#reduktion-der-daten"><i class="fa fa-check"></i><b>5.6</b> Reduktion der Daten</a></li>
<li class="chapter" data-level="5.7" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#am-beispiel-der-pinguin-daten"><i class="fa fa-check"></i><b>5.7</b> Am Beispiel der Pinguin Daten</a></li>
<li class="chapter" data-level="5.8" data-path="hauptkomponentenanalyse-ctd..html"><a href="hauptkomponentenanalyse-ctd..html#aufgaben-2"><i class="fa fa-check"></i><b>5.8</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html"><i class="fa fa-check"></i><b>6</b> Clustering und Hauptkomponentenanalyse</a><ul>
<li class="chapter" data-level="6.1" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#clustering-im-allgemeinen"><i class="fa fa-check"></i><b>6.1</b> Clustering im Allgemeinen</a></li>
<li class="chapter" data-level="6.2" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#k-means-clustering"><i class="fa fa-check"></i><b>6.2</b> K-means Clustering</a></li>
<li class="chapter" data-level="6.3" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#clustering-und-hauptkomponentenanalyse-1"><i class="fa fa-check"></i><b>6.3</b> Clustering und Hauptkomponentenanalyse</a></li>
<li class="chapter" data-level="6.4" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#training-und-testing"><i class="fa fa-check"></i><b>6.4</b> Training und Testing</a></li>
<li class="chapter" data-level="6.5" data-path="clustering-und-hauptkomponentenanalyse.html"><a href="clustering-und-hauptkomponentenanalyse.html#am-beispiel-der-pinguine"><i class="fa fa-check"></i><b>6.5</b> Am Beispiel der Pinguine</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Einführung in die mathematische Datenanalyse</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lineare-regression" class="section level1 hasAnchor">
<h1><span class="header-section-number">2</span> Lineare Regression<a href="lineare-regression.html#lineare-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Auch bekannt als</p>
<ul>
<li><em>lineare Ausgleichsrechnung</em> oder</li>
<li><em>Methode der kleinsten Quadrate</em>.</li>
</ul>
<p>Ein wesentlicher Aspekt von <em>Data Science</em> ist die Analyse oder das
Verstehen von Daten. Allgemein gesagt, es wird versucht, aus den Daten
heraus Aussagen über Trends oder Eigenschaften des Phänomens zu treffen,
mit welchem die Daten im Zusammenhang stehen.</p>
<p>Wir kommen nochmal auf das Beispiel aus der Einführungswoche zurück, werfen eine bereits geschärften Blick darauf und gehen das mit verbesserten mathematischen Methoden an.</p>
<p>Gegeben seien die Fallzahlen aus der CoVID Pandemie 2020 für Bayern für den Oktober 2020.</p>






































<table>
<thead>
<tr class="header">
<th align="left">Tag</th>
<th align="left">12</th>
<th align="left">13</th>
<th align="left">14</th>
<th align="left">15</th>
<th align="left">16</th>
<th align="left">17</th>
<th align="left">18</th>
<th align="left">19</th>
<th align="left">20</th>
<th align="left">21</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Fälle</td>
<td align="left">681</td>
<td align="left">691</td>
<td align="left">1154</td>
<td align="left">1284</td>
<td align="left">127</td>
<td align="left">984</td>
<td align="left">573</td>
<td align="left">1078</td>
<td align="left">1462</td>
<td align="left">2239</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="left">Tag</th>
<th align="left">22</th>
<th align="left">23</th>
<th align="left">24</th>
<th align="left">25</th>
<th align="left">26</th>
<th align="left">27</th>
<th align="left">28</th>
<th align="left">29</th>
<th align="left">30</th>
<th align="left">31</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Fälle</td>
<td align="left">2236</td>
<td align="left">2119</td>
<td align="left">1663</td>
<td align="left">1413</td>
<td align="left">2283</td>
<td align="left">2717</td>
<td align="left">3113</td>
<td align="left">2972</td>
<td align="left">3136</td>
<td align="left">2615</td>
</tr>
</tbody>
</table>
</div>
<div class="figure">
<img src="bilder/02-cases.png" alt="Fallzahlen von Sars-CoV-2 in Bayern im Oktober 2020" id="fig:cases" />
<p class="caption">Fallzahlen von Sars-CoV-2 in Bayern im Oktober
2020</p>
</div>
<p>Wieder stellen wir uns die Frage ob wir <strong>in den Daten einen funktionalen
Zusammenhang</strong> feststellen können. Also ob wir die Datenpaare</p>
<blockquote>
<p>(Tag <span class="math inline">\(x\)</span>, Infektionen am Tag <span class="math inline">\(x\)</span>)</p>
</blockquote>
<p>die wir als</p>
<blockquote>
<p>(<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>)</p>
</blockquote>
<p>über eine Funktion <span class="math inline">\(f\)</span> und die Paare</p>
<blockquote>
<p>(<span class="math inline">\(x\)</span>, <span class="math inline">\(f(x)\)</span>)</p>
</blockquote>
<p>beschreiben (im Sinne von gut darstellen oder approximieren) können.</p>
<div id="rauschen-und-fitting" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.1</span> Rauschen und Fitting<a href="lineare-regression.html#rauschen-und-fitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Beim obigen Beispiel (und ganz generell bei Daten) ist davon auszugehen, dass die Daten <strong>verrauscht</strong> sind, also einem Trend folgen oder in einem funktionalen Zusammenhang stehen aber zufällige Abweichungen oder Fehler enthalten.</p>
<p>Unter diesem Gesichtspunkt ist eine Funktion, die</p>
<blockquote>
<p><span class="math inline">\(f(x_i)=y_i\)</span></p>
</blockquote>
<p>erzwingt nicht zielführend. (Wir wollen Trends und größere Zusammenhänge erkennen und nicht kleine Fehler nachzeichnen.) Das zu strenge Anpassen an möglicherweise verrauschte Daten wird <strong>overfitting</strong> genannt.</p>
<p>Vielmehr werden wir nach einer Funktion <span class="math inline">\(f\)</span> suchen, die die Daten näherungsweise nachstellt:</p>
<blockquote>
<p><span class="math inline">\(f(x_i)\approx y_i\)</span></p>
</blockquote>
<p>Hierbei passen jetzt allerdings auch Funktionen, die vielleicht einfach zu handhaben sind aber die Daten kaum noch repräsentieren. Jan spricht von <strong>underfitting</strong>.</p>
<p>Eine gute Approximation besteht im Kompromiss von <em>nah an den Daten</em> aber mit wenig <em>overfitting</em>.</p>
</div>
<div id="ansätze-für-lineare-regression" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.2</span> Ansätze für lineare Regression<a href="lineare-regression.html#ansätze-für-lineare-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Um eine solche Funktion <span class="math inline">\(f\)</span> zu finden, trifft Jan als erstes ein paar
Modellannahmen. Modellannahmen legen fest, wie das <span class="math inline">\(f\)</span> im Allgemeinen
aussehen soll und versuchen dabei</p>
<ol style="list-style-type: decimal">
<li><p>die Bestimmung von <span class="math inline">\(f\)</span> zu ermöglichen</p></li>
<li><p>zu garantieren, dass <span class="math inline">\(f\)</span> auch die gewollten Aussagen liefert</p></li>
<li><p>und sicherzustellen, dass <span class="math inline">\(f\)</span> zum Problem passt.</p></li>
</ol>
<p>Jan bemerke, dass die ersten beiden Annahmen im Spannungsverhältnis zur
dritten stehen.</p>
<p><strong>Lineare Regression</strong> besteht darin, dass die Funktion als Linearkombination</p>
<p><span class="math display">\[\begin{equation*}
f_w(x) = \sum_{j=1}^n w_j b_j(x)
\end{equation*}\]</span></p>
<p>von Basisfunktionen geschrieben wird und dann die <em>Koeffizienten</em> <span class="math inline">\(w_i\)</span> so bestimmt werden, dass <span class="math inline">\(f\)</span> die Daten bestmöglich annähert.</p>
<blockquote>
<p>Jan bemerke, dass <em>bestmöglich</em> wieder <em>overfitting</em> bedeuten kann aber auch, bei schlechter Wahl der Basis, wenig aussagekräftig sein kann. Der gute Kompromiss liegt also jetzt in der Wahl der passenden Basisfunktionen und deren Anzahl. (Mehr Basisfunktionen bedeutet möglicherweise bessere Approximation aber auch die Gefahr von <em>overfitting</em>.)</p>
</blockquote>
<p>Typische Wahlen für die Basis <span class="math inline">\(\{b_1, b_2, \dotsc, b_n\}\)</span> sind</p>
<ul>
<li>Polynome: <span class="math inline">\(\{1, x, x^2, \dotsc, x^{n-1}\}\)</span> – für <span class="math inline">\(n=2\)</span> ist der Ansatz <em>eine Gerade</em></li>
<li>Trigonometrische Funktionen: <span class="math inline">\(\{1, \cos(x), \sin(x), \cos(2x), \sin(2x), \dotsc\}\)</span></li>
<li>Splines – Polynome, die abschnittsweise definiert werden</li>
<li>Wavelets – Verallgemeinerungen von trigonometrischen Funktionen</li>
</ul>
</div>
<div id="fehlerfunktional-und-minimierung" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.3</span> Fehlerfunktional und Minimierung<a href="lineare-regression.html#fehlerfunktional-und-minimierung" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Wir setzen nun also an
<span class="math display">\[\begin{equation*}
f_w(x) = \sum_{j=1}^nw_j b_j (x)
\end{equation*}\]</span>
und wollen damit <span class="math inline">\(y_i \approx f_w(x_i)\)</span> <em>bestmöglich</em> erreichen (indem wir die Koeffizienten <span class="math inline">\((w_1, \dotsc, w_n)\)</span> <em>optimal</em> wählen. Bestmöglich und optimal spezifizieren wir über den Mittelwert der quadratischen Abweichungen in der Approximation über alle Datenpunkte
<span class="math display">\[\begin{equation*}
\frac{1}{N}\sum_{i=1}^N (y_i - f_w(x_i))^2
\end{equation*}\]</span></p>
<p>Ein paar Bemerkungen</p>
<ul>
<li>jetzt müssen wir die <span class="math inline">\(w_i\)</span>’s bestimmen so dass dieser Fehlerterm minimal wird</li>
<li>das optimale <span class="math inline">\(w\)</span> ist unabhängig von einer Skalierung des Fehlerterms</li>
<li>des wegen schreiben wir gerne einfach <span class="math inline">\(\frac 12 \sum_{i=1}^N (y_i - f_w(x_i))^2\)</span> als das Zielfunktional, das es zu minimieren gilt.</li>
</ul>
<p>Wie finden wir jetzt die <span class="math inline">\(w_i\)</span>’s? Zunächst gilt, dass
<span class="math display">\[\begin{equation*}
f_w(x_i) = \sum_{i=j}^n w_j b_j(x_i) = 
\begin{bmatrix}
b_1(x_i) &amp; b_2(x_i) &amp; \dots &amp; b_n(x_i)
\end{bmatrix}
\begin{bmatrix}
w_1 \\ w_2 \\ \vdots \\ w_n
\end{bmatrix}
\end{equation*}\]</span></p>
<p>und wenn wir alle <span class="math inline">\(f_w(x_i)\)</span>, <span class="math inline">\(i=1,\dotsc,N\)</span> übereinander in einen Vektor schreiben, dass</p>
<p><span class="math display">\[\begin{equation*}
f_w(\mathbf x) := 
\begin{bmatrix}
f_w(x_1) \\ \vdots \\ f_w(x_N)
\end{bmatrix}
=
\begin{bmatrix}
b_1(x_1) &amp; \dots &amp; b_n(x_1) \\
\vdots &amp; \ddots &amp; \vdots \\
b_1(x_N) &amp; \dots &amp; b_n(x_N)
\end{bmatrix}
\begin{bmatrix}
w_1 \\ \vdots \\ w_n
\end{bmatrix}
=: \Phi(\mathbf x) w
\end{equation*}\]</span></p>
<p>Damit, mit <span class="math inline">\(\mathbf y\)</span> als den Vektor aller <span class="math inline">\(y_i\)</span>’s, und mit der Definition der Vektornorm, können wir unser Minimierungsproblem schreiben als
<span class="math display">\[\begin{equation*}
\frac 12  \sum_{i=1}^N (y_i - f_w(x_i))^2 = \frac 12 \| \mathbf y - \Phi (\mathbf x) w\|^2 \to \min.
\end{equation*}\]</span></p>
<p>Wir bemerken, dass</p>
<ul>
<li>das Fehlerfunktional immer größer und bestenfalls gleich 0 ist</li>
<li>falls das lineare Gleichungssystem <span class="math inline">\(\Phi (\mathbf x)w = \mathbf y\)</span> eine Lösung <span class="math inline">\(w\)</span> hat, ist das auch eine Lösung unserer Minimierung</li>
<li>im typischen Falle aber ist allerdings <span class="math inline">\(N\gg n\)</span> und das System überbestimmt (<span class="math inline">\(n=N\)</span> würde ein overfitting bedeuten…) sodass wir keine Lösung des linearen Gleichungssystems erwarten können.</li>
<li>Das Minimierungsproblems selbst hat allerdings immer eine Lösung.</li>
</ul>
</div>
<div id="berechnung-der-bestlösung" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.4</span> Berechnung der Bestlösung<a href="lineare-regression.html#berechnung-der-bestlösung" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Wir suchen also ein Minimum der Funktion (mit <span class="math inline">\(\Phi\)</span>, <span class="math inline">\(\mathbf x\)</span>, <span class="math inline">\(\mathbf y\)</span> gegeben)
<span class="math display">\[\begin{equation*}
\begin{split}
w \mapsto \frac 12 \|\mathbf y - \Phi(\mathbf x)w \|^2 &amp;= \frac 12 (\mathbf y - \Phi(\mathbf x)w)^T(\mathbf y - \Phi(\mathbf x)w)  \\ 
&amp;= \frac 12 [\mathbf y^T\mathbf y - \mathbf y^T \Phi(\mathbf x)w - w^T \Phi(\mathbf x)^T\mathbf y  + w^T \Phi(\mathbf x)^T\Phi(\mathbf x)w] \\
&amp;= \frac 12 [\mathbf y^T\mathbf y -2 w^T \Phi(\mathbf x)^T\mathbf y  + w^T \Phi(\mathbf x)^T\Phi(\mathbf x)w] 
\end{split}
\end{equation*}\]</span>
wobei wir die Definition der Norm <span class="math inline">\(\|v\|^2 = v^Tv\)</span> und die Eigenschaft, dass für die skalare Größe <span class="math inline">\(w^T \Phi(\mathbf x)^T\mathbf y = [w^T \Phi(\mathbf x)^T\mathbf y]^T = \mathbf y^T \Phi(\mathbf x)w\)</span> gilt, ausgenutzt haben.</p>
<p>Wären <span class="math inline">\(w\)</span> und <span class="math inline">\(\mathbf y\)</span> keine Vektoren sondern einfach reelle Zahlen, wäre das hier eine Parabelgleichung <span class="math inline">\(aw^2 + bw + c\)</span> mit <span class="math inline">\(a&gt;0\)</span>, die immer eine Minimalstelle hat.</p>
<p>Tatsächlich gilt hier alles ganz analog. Insbesondere ist <span class="math inline">\(\Phi(\mathbf x)^T\Phi(\mathbf x)\)</span> in der Regel “größer 0” (was heißt das wohl bei quadratischen Matrizen?). Und mittels “Nullsetzen” der ersten Ableitung können wir das Minimum bestimmen. In diesem Fall ist die erste Ableitung (nach <span class="math inline">\(w\)</span>)
<span class="math display">\[\begin{equation*}
\nabla_w (\frac 12 \|\mathbf y - \Phi(\mathbf x) \|^2) = \Phi(\mathbf x)^T\Phi(\mathbf x)w - \Phi(\mathbf x)^T\mathbf y,
\end{equation*}\]</span>
(den <em>Gradienten</em> <span class="math inline">\(\nabla_w\)</span> als Ableitung von Funktionen mit mehreren Veränderlichen werden wir noch genauer behandeln)
was uns als Lösung, die Lösung des linearen Gleichungssystems
<span class="math display">\[\begin{equation*}
\Phi(\mathbf x)^T\Phi(\mathbf x)w = \Phi(\mathbf x)^T\mathbf y
\end{equation*}\]</span>
definiert.</p>
<p>Letzte Frage: Wann hat dieses Gleichungssystems eine eindeutige Lösung? Mit <span class="math inline">\(N&gt;n\)</span> (also <span class="math inline">\(\Phi(\mathbf x)\)</span> hat mehr Zeilen als Spalten) gelten die Äquivalenzen:</p>
<ul>
<li><span class="math inline">\(\Phi(\mathbf x)^T\Phi(\mathbf x)w = \Phi(\mathbf x)^T\mathbf y\)</span> hat eine eindeutige Lösung</li>
<li>die Matrix <span class="math inline">\(\Phi(\mathbf x)^T\Phi(\mathbf x)\)</span> ist regulär</li>
<li>die Spalten von <span class="math inline">\(\Phi(\mathbf x)\)</span> sind linear unabhängig</li>
<li>die Vektoren <span class="math inline">\(b_i(\mathbf x)\)</span> sind linear unabhängig.</li>
</ul>
<p>Praktischerweise tritt genau diese Situation im Allgemeinen ein.</p>
<ul>
<li><span class="math inline">\(N&gt;n\)</span> (mehr Datenpunkte als Parameter)</li>
<li><span class="math inline">\(b_i\)</span>’s werden als <em>linear unabhängig</em> (im Sinne ihres Funktionenraums) gewählt, was die lineare unabhängigket der <span class="math inline">\(b_i(\mathbf x)\)</span> impliziert.</li>
</ul>
</div>
<div id="beispiel" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.5</span> Beispiel<a href="lineare-regression.html#beispiel" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Unsere Covid-Zahlen “mit einer Geraden angenähert”:</p>
<ul>
<li><span class="math inline">\(f_w(x) = w_1 + w_2 x\)</span> – das heißt <span class="math inline">\(n=2\)</span> und Basisfunktionen <span class="math inline">\(b_1(x)\equiv 1\)</span> und <span class="math inline">\(b_2(x) = x\)</span></li>
<li><span class="math inline">\(\mathbf x = (1,2,3, \dots, 31)\)</span> – die Tage im Februar, das heißt <span class="math inline">\(N=31\)</span></li>
<li><span class="math inline">\(\mathbf y = (352, 347, \dots, 2615)\)</span> – die Fallzahlen</li>
</ul>
<p>Wir bekommen</p>
<p><span class="math display">\[\begin{equation*}
\Phi(\mathbf x) = 
\begin{bmatrix}
1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ \vdots &amp; \vdots \\ 1 &amp; 31
\end{bmatrix}
\end{equation*}\]</span>
(die Spalten sind linear unabhängig) und müssen “nur” das 2x2 System
<span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\ 1 &amp; 2 &amp; 3 &amp; \dots &amp; 31
\end{bmatrix}
\begin{bmatrix}
1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ \vdots \\ 1 &amp; 31
\end{bmatrix}
\begin{bmatrix}
w_1 \\ w_2
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\ 1 &amp; 2 &amp; 3 &amp; \dots &amp; 31
\end{bmatrix} 
\begin{bmatrix}
352 \\  347 \\ 308 \\ \vdots \\ 2615
\end{bmatrix} 
\]</span>
lösen um die Approximation <span class="math inline">\(f_w\)</span> zu bestimmen.</p>
<p>Und noch als letzte Bemerkung. Egal wie die Basisfunktionen <span class="math inline">\(b_i\)</span> gewählt werden, die Parameterabhängigkeit von <span class="math inline">\(w\)</span> ist immer linear. Deswegen der Name <strong>lineare Ausgleichsrechnung</strong>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="was-ist-data-science.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrix-zerlegungen.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["EMDS.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
