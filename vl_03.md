# Matrix-Zerlegungen

Die L&ouml;sung $w$ des Problems der *linearen Ausgleichsrechnung* war entweder als L&ouml;sung eines Optimierungsproblems
\begin{equation*}
\min_{w} \| Aw - y \|^2
\end{equation*}
oder als L&ouml;sung des linearen Gleichungssystems
\begin{equation*}
A^TAw=y
\end{equation*}
gegeben. Hierbei steht nun $A\in \mathbb R^{N\times n}$ f&uuml;r die Matrix $\Phi(\mathbf x)$ der Daten und Basisfunktionen. Wir hatten uns &uuml;berlegt, dass in den meisten F&auml;llen

 * die Matrix mehr Zeilen als Spalten hat ($N>n$) und
 * die Spalten linear unabh&auml;ngig sind.

## QR Zerlegung

Wir betrachten nochmal das Optimierungsproblem $\min_{w} \| Aw - y \|^2$. G&auml;be es eine L&ouml;sung des Systems $Aw=y$, w&auml;re das sofort eine L&ouml;sung des Optimierungsproblems. Da $A$ aber mehr Zeilen als Spalten hat, ist das System $Aw=y$ &uuml;berbestimmt und eine L&ouml;sung in der Regel nicht gegeben.

Die &Uuml;berlegung ist nun, die Gleichung $Aw=y$ *so gut wie m&ouml;glich* zu erf&uuml;llen, indem wir die relevanten Gleichungen indentifizieren und wenigstens diese l&ouml;sen. Ein systematischer (und wie wir sp&auml;ter sehen werden auch zum Optimierungsproblem passender) Zugang bietet die QR Zerlegung.

::: {.theorem name="QR Zerlegung" #qr}
Sei $A\mathbb R^{m\times n}$, $m>n$. Dann existiert eine orthonormale Matrix $Q\in \mathbb R^{m\times m}$ und eine obere Dreiecksmatrix $\hat R\in \mathbb R^{n\times n}$ derart dass
\begin{equation*}
A = QR =: Q 
\begin{bmatrix}
\hat R \\ 0
\end{bmatrix}.
\end{equation*}
Hat $A$ vollen (Spalten)Rang, dann ist $\hat R$ invertierbar.
:::

Hier hei&szlig;t *orthonormale Matrix* $Q$, dass die Spalten von $Q$ paarweise orthogonal sind. Insbesondere gilt 
\begin{equation*}
Q^T Q = I.
\end{equation*}

F&uuml;r unser zu l&ouml;sendes Problem ergibt sich dadurch die Umformung
\begin{equation*}
Aw = y \quad \Leftrightarrow \quad QRw=y \quad \Leftrightarrow \quad Q^TQRw=Q^T y 
 \quad \Leftrightarrow \quad  Rw = Q^Ty
\end{equation*}
oder auch
\begin{equation*}
\begin{bmatrix}
\hat R \\ 0
\end{bmatrix}w 
=
Q^Ty
=
\begin{bmatrix}
Q_1^T \\ Q_2^T
\end{bmatrix}
y
\end{equation*}
(wobei wir die $Q_1\in \mathbb R^{m\times n}$ die Matrix der ersten $n$ Spalten von $Q$ ist)
und als *Kompromiss* der Vorschlag, das Teilsystem
\begin{equation*}
\hat R w = Q_1^Ty
\end{equation*}
nach $w$ zu l&ouml;sen und in Kauf zu nehmen, dass der Rest, n&auml;mlich das $Q_2^Ty$, nicht notwendigerweise gleich null ist.

Wir halten zun&auml;chst mal fest, dass

* Obwohl $Q$ eine regul&auml;re Matrix ist, bedarf der &Uuml;bergang von $Aw=y$ zu $Q^TAw=Q^Ty$ einer genaueren Analyse. 
* Wir bemerken, dass f&uuml;r eine hypothetische komplette L&ouml;sung $Aw-y$, diese Transformation keine Rolle spielt.
* F&uuml;r die Kompromissl&ouml;sung jedoch schon, weil beispielsweise verschiedene Konstruktionen eines invertierbaren Teils, verschiedene Residuen bedeuten und somit Optimalit&auml;t im Sinne von $\min_w \|Aw-y\|^2$ nicht garantiert ist.

Allerdings, wie Sie als &Uuml;bungsaufgabe nachweisen werden, l&ouml;st dieser Ansatz tats&auml;chlich das Optimierungsproblem.

## Singul&auml;rwertzerlegung 

Eine weitere Matrix Zerlegung, die eng mit der L&ouml;sung von Optimierungsproblemen oder &uuml;berbestimmten Gleichungssystemen zusammenh&auml;ngt ist die *Singul&auml;rwertzerlegung* (SVD -- von english: *Singular Value Decomposition*).

::: {.theorem name="Singul&auml;rwertzerlegung" #svd}
Sei $A\in \mathbb C^{m\times n}$, $m\geq n$. Dann existieren orthogonale Matrizen $U \in \mathbb C^{m\times m}$ und $V\in \mathbb C^{n\times n}$ und eine Matrix $\Sigma \in \mathbb R^{m\times n}$ der Form
\begin{equation*}
\Sigma = 
\begin{bmatrix}
\sigma_1 & 0 & \dots & 0\\
0 & \sigma_2 &\ddots & \vdots\\
0 & \ddots & \ddots &0\\
  0 & \dots&0 & \sigma_n \\
  0 & 0 & \dots & 0 \\
  \vdots & \ddots &  & \vdots\\
  0 & 0 & \dots & 0
\end{bmatrix}
\end{equation*}
mit reellen sogenannten *Singul&auml;rwerten*
\begin{equation*}
\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_n \geq 0
\end{equation*}
sodass gilt
\begin{equation*}
A = U \Sigma V^*
\end{equation*}
wobei gilt $V^* = \overline{V^T}$ (transponiert und komplex konjugiert).
:::

Ein paar Bemerkungen. 

 * Ist $A$ reell, k&ouml;nnen auch $U$ und $V$ reell gew&auml;hlt werden.
 * Die Annahme $m \geq n$ war nur n&ouml;tig um f&uuml;r die Matrix $\Sigma$ keine Fallunterscheidung zu machen. (F&uuml;r $m\leq n$ "steht der Nullblock rechts von den Singul&auml;rwerten"). Insbesondere gilt $A^* = V\Sigma U^*$ ist eine SVD von $A^*$.
 * Eine Illustration der Zerlegung ist in Abbildung \@ref(SVD) zu sehen.

Wir machen einige &Uuml;berlegungen im Hinblick auf gro&szlig;e Matrizen. Sei dazu $m>n$, $A\in \mathbb C^{m\times n}$ und $A=U\Sigma V^*$ eine SVD wie in Theorem \@ref{thm:SVD}. Sei nun
\begin{equation*}
U = \begin{bmatrix}
U_1 & U_2
\end{bmatrix}
% = \begin{bmatrix} V_1^* & V_2^*
\end{equation*}
partitioniert sodass $U_1$ die ersten $n$ Spalten von $U$ enth&auml;lt.

Dann gilt (nach der Matrix-Multiplikations Regel *Zeile mal Spalte* die Teile $U_2$ und $V_2$ immer mit dem Nullblock in $\Sigma$ multipliziert werden) dass
\begin{equation*}
A = U\Sigma V = 
\begin{bmatrix}
U_1 & U_2
\end{bmatrix}
\begin{bmatrix}
\hat \Sigma \\ 0
\end{bmatrix}
V^*
% \begin{bmatrix} V_1^* \\ V_2^* \end{bmatrix}
=
U_1 
\hat \Sigma
V^*
% \begin{bmatrix} V_1^* \\ V_2^* \end{bmatrix}
\end{equation*}
Es gen&uuml;gt also nur die ersten $m$ Spalten von $U$ zu berechnen. Das ist die sogenannte **slim SVD**.

Hat, dar&uuml;berhinaus, die Matrix $A$ keinen vollen Rang, also $\operatorname{Rg}(A) = r < n$, dann:

 * ist $\sigma_i=0$, f&uuml;r alle $i=r+1, \dotsc, n$, (wir erinnern uns, dass die Singul&auml;rwerte nach Gr&ouml;&szlig;e sortiert sind)
 * die Matrix $\hat \Sigma$ hat $n-r$ Nullzeilen
 * f&uuml;r die Zerlegung sind nur die ersten $r$ Spalten von $U$ und $V$ relevant -- die sogenannte **Kompakte SVD**.

In der Datenapproximation ist au&szlig;erdem die **truncated SVD** von Interesse. Dazu sei $\hat r<r$ ein beliebig gew&auml;hlter Index. Dann werden alle Singul&auml;rwerte,  $\sigma_i=0$, f&uuml;r alle $i=\hat r+1, \dotsc, n$, abgeschnitten -- das hei&szlig;t null gesetzt und die entsprechende *kompakte SVD* berechnet.

Hier gilt nun nicht mehr die Gleichheit in der Zerlegung. Vielmehr gilt 
\begin{equation*}
A \approx A_{\hat r}
\end{equation*}
wobei $A_{\hat r}$ aus der *truncated SVD* von $A$ erzeugt wurde. Allerdings ist diese Approximation von $A$ durch optimal in dem Sinne, dass es keine Matrix vom Rang $\hat r \geq r=\operatorname{Rg}(A)$ gibt, die $A$ (in der *induzierten* euklidischen Norm^[Auf Matrixnormen kommen wir noch in der Vorlesung zu sprechen.]) besser approximiert. Es gilt
\begin{equation*}
\min_{B\in \mathbb C^{m\times n}, \operatorname{Rg}(B)=\hat r} \|A-B\|_2 = \|A-A_{\hat r}\|_2 = \sigma_{\hat r + 1};
\end{equation*}
vgl. Bollhoefer/Mehrmann Satz 14.15.

Zum Abschluss noch der Zusammenhang zum Optimierungsproblem. Ist $A=U\Sigma V^*$ "SV-zerlegt", dann gilt
\begin{equation*}
A^*Aw = V\Sigma^*U^*U\Sigma V^*w = V\hat \Sigma^2 V^* 
\end{equation*}
und damit
\begin{equation*}
A^*Aw = A^*y \quad \Leftrightarrow \quad V\hat \Sigma^2 V^*  = V\Sigma^*U^*y \quad \Leftrightarrow \quad w = V(\Sigma^+)^*U^*y
\end{equation*}
wobei
\begin{equation*}
\Sigma^+ = \begin{bmatrix}
\hat \Sigma^{-1} \\ 0_{m-n \times n}
\end{bmatrix}
\end{equation*}
aus $\Sigma \hat \Sigma^{-1}\hat \Sigma^{-1}$ herr&uuml;hrt.

**Bemerkung**: $\Sigma^+$ kann auch definiert werden, wenn $\hat \Sigma$ nicht invertierbar ist (weil manche Diagonaleintr&auml;ge null sind). Dann wird $\hat \Sigma^+$ betrachtet, bei welcher nur die $\sigma_i>0$ invertiert werden und die anderen $\sigma_i=0$ belassen werden. Das definiert eine sogenannte *verallgemeinerte Inverse* und l&ouml;st auch das Optimierungsproblem falls $A$ keinen vollen Rang hat.


![Illustration der SVD. Bitte beachten der $*$ bedeutet hier transponiert und komplex konjugiert. By Cmglee - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=67853297](bilder/03-412px-Singular_value_decomposition_visualisation.svg.png){width=50% #SVD}

## Aufgaben

(T) hei&szlig;t theoretische Aufgabe, (P) hei&szlig;t programmieren.

### Norm und Orthogonale Transformation (T)

Sei $Q\in \mathbb R^{n\times n}$ eine orthogonale Matrix und sei $y\in \mathbb R^{n}$. Zeigen Sie, dass
\begin{equation*}
\|y\|^2 = \|Qy \|^2
\end{equation*}
gilt. 

### Kleinste Quadrate und Mittelwert

Zeigen sie, dass der *kleinste Quadrate* Ansatz zur Approximation einer Datenwolke 
\begin{equation*}
(x_i, y_i), \quad i=1,2,\dotsc,N,
\end{equation*}
mittels einer konstanten Funktion $f(x)=w_1$ auf $w_1$ als den Mittelwert von $y_i$ f&uuml;hrt.


### QR Zerlegung und Kleinstes Quadrate Problem (T) 

Sei $A\in \mathbb R^{m,n}$, $m>n$, $A$ hat vollen Rank und sei
\begin{equation*}
\begin{bmatrix}
Q_1 & Q_2
\end{bmatrix}
\begin{bmatrix}
\hat R \\ 0
\end{bmatrix} = A
\end{equation*}
eine QR-Zerlegung von $A$. Zeigen sie, dass die L&ouml;sung von
\begin{equation*}
\hat R w = Q_1^T y
\end{equation*}
ein kritischer Punkt (d.h. der Gradient $\nabla_w$ verschwindet) von
\begin{equation*}
w \mapsto \| Aw - y \|^2
\end{equation*}
ist. **Hinweis**: Die Formel f&uuml;r den Gradienten wurde in der Vorlesung 02 hergeleitet.

### Eigenwerte Symmetrischer Matrizen (T)

Zeigen Sie, dass Eigenwerte symmetrischer reeller Matrizen $A\in \mathbb R^{n\times n}$ immer reell sind.

### Singul&auml;rwertzerlegung und Eigenwerte (T)

Zeigen Sie, dass die quadrierten Singul&auml;rwerte einer Matrix $A\in \mathbb R^{m\times n}$, $m>n$, genau die Eigenwerte der Matrix $A^TA$ sind und in welcher Beziehung sie mit den Eigenwerten von $AA^T$ stehen. **Hinweis**: hier ist "$m>n$" wichtig.

### Python -- Laden und Speichern von Arrays

### Lineare Regression f&uuml;r Covid Daten (P)

### Truncated SVD

SVs plotten
Formel verifizieren


